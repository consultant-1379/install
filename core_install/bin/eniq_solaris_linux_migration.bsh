#!/bin/bash
# ********************************************************************
# Ericsson Radio Systems AB                                     SCRIPT
# ********************************************************************
#
# (c) Ericsson Radio Systems AB 2021 - All rights reserved.
#
# The copyright to the computer program(s) herein is the property
# of Ericsson Radio Systems AB, Sweden. The programs may be used
# and/or copied only with the written permission from Ericsson Radio
# Systems AB or in accordance with the terms and conditions stipulated
# in the agreement/contract under which the program(s) have been
# supplied.
#
# ********************************************************************
#
# Name    : eniq_solaris_linux_migration.bsh
# Date    : 15/07/2022
# Revision: \main\48
# Purpose : 1.This script will support pre-migration from Solaris 11 OS to
#           RHEL
#           2.This script will support recovery from RHEL to Solaris 11 OS
# Usage   : eniq_solaris_linux_migration.bsh -a <action> [-R] [-l <logfile>]
#
# ********************************************************************
#
#   Command Section
#
# ********************************************************************
AWK=/usr/bin/awk
BASENAME=/usr/bin/basename
BASH=/usr/bin/bash
BC=/usr/bin/bc
CAT=/usr/bin/cat
CHOWN=/usr/bin/chown
CLEAR=/usr/bin/clear
CUT=/usr/bin/cut
CP=/usr/bin/cp
CPIO=/usr/bin/cpio
DATE=/usr/bin/date
DF=/usr/bin/df
DIFF=/usr/bin/diff
DIRNAME=/usr/bin/dirname
DLADM=/usr/sbin/dladm
DUMPADM=/usr/sbin/dumpadm
DU=/usr/bin/du
ECHO=/usr/bin/echo
EGREP=/usr/bin/egrep
EXPECT=/usr/local/bin/expect
EXPR=/usr/bin/expr
FIND=/usr/bin/find
GEGREP=/usr/sfw/bin/gegrep
GETENT=/usr/bin/getent
GREP=/usr/bin/grep
HEAD=/usr/bin/head
ID=/usr/bin/id
IPADM=/usr/sbin/ipadm
LS=/usr/bin/ls
METASTAT=/usr/sbin/metastat
MKDIR=/usr/bin/mkdir
MOUNT=/usr/sbin/mount
MYHOSTNAME=/usr/bin/hostname
NAWK=/usr/bin/nawk
OPENSSL=/usr/bin/openssl
PERL=/usr/bin/perl
PING=/usr/sbin/ping
PRINTF=/usr/bin/printf
PWD=/usr/bin/pwd
RM=/usr/bin/rm
SED=/usr/bin/sed
SORT=/usr/bin/sort
SLEEP=/usr/bin/sleep
SSH=/usr/bin/ssh
SU=/usr/bin/su
SVCADM=/usr/sbin/svcadm
SVCS=/usr/bin/svcs
SWAP=/usr/sbin/swap
TAIL=/usr/bin/tail
TEE=/usr/bin/tee
TOUCH=/usr/bin/touch
TR=/usr/bin/tr
XARGS=/usr/bin/xargs
UMOUNT=/usr/sbin/umount
UNAME=/usr/bin/uname
ZFS=/usr/sbin/zfs
ZPOOL=/usr/sbin/zpool

# ********************************************************************
#
#       Configuration Section
#
# ********************************************************************

# Default user
DEFAULT_USER=root

if [ ! "${BACKUP}" ]; then
    # Cmd to exec a shell and drop user to it in case of an error
    EXEC_SHELL_CMD="exec /bin/bash -o emacs"
fi

# Name of the ini Files
BACKUP_DATA_INI=backup_migration_data.ini
BLK_STOR_INI=block_storage.ini
ENIQ_INI=niq.ini
IPMP_INI=ipmp.ini
STORAGE_INI=storage.ini
SUNOS_INI=SunOS.ini
SYM_LINKS_INI=sym_links.ini

# Service SMFs
AUTO_LU_SMF_ID="svc:/lu/auto_lu:default"
DDC_SMF_ID="svc:/ericsson/eric_monitor/ddc:default"
HOSTSYNC_SMF_ID="hostsync"
SENTINEL_SMF_ID="svc:/licensing/sentinel:default"


# NAS SMF
NAS_MILESTONE_SMF_ID="svc:/milestone/NAS-online:default"
NASd_SMF_ID="svc:/storage/NASd:default"

# Cron SMF
CRON_SMF_ID="svc:/system/cron:default"

# Zpool Information
ROOT_POOL="rpool"

# Set REPLACEMENT default value
REPLACEMENT="NO"

# Setting no confirmation from user
NO_CONFIRM="YES"

# ********************************************************************
#
#   Functions
#
# ********************************************************************
### Function: abort_script ###
#
#   This will be called if the script is aborted by an error
#   which is encountered during runtime
#
# Arguments:
#       $1 - Error message from part of program (Not always used)
# Return Values:
#       none
abort_script()
{
_err_time_=`$DATE '+%Y-%b-%d_%H.%M.%S'`

if [ "$1" ]; then
    _err_msg_="${_err_time_} - $1"
else
    _err_msg_="${_err_time_} - ERROR : Script aborted.......\n"
fi

if [ "${LOGFILE}" ]; then
    $ECHO "\nERROR : ${_err_msg_}\n" | $TEE -a ${LOGFILE}
else
    $ECHO "\nERROR : ${_err_msg_}\n"
fi

if [[ ${BACKUP} ]] && [[ ${USER_STAGE} == "get_migration_data" ]];then
    if [ -f "${MIGRATION_CONF}"_bkp ];then
        $MV "${MIGRATION_CONF}"_bkp "${MIGRATION_CONF}"
    fi
fi

$RM -rf ${TEM_DIR}

if [ "$2" ]; then
    if [ ! "${CONTINUE}" ]; then 
        ${2}
    fi
    exit 1
else
   exit 1
fi
}

### Function: allow_root_access ###
#
# Updates to allow root to telnet/ftp access
#
# Arguments:
#   none
# Return Values:
#   none
allow_root_access()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling allow_root_access stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s allow_root_access ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: backup_contents ###
#
#   Backup root data
#
# Arguments:
#   $1 - FS type of which data needs to be backed up
#   $2 - ini file
#   $3 - Exclude list
# Return Values:
#   none
backup_contents()
{
if [ $# -lt 2 ]; then
    _err_msg_="Mandatory arguments not passed"
    abort_script ${_err_msg_}
fi

_content_name_=$1
_ini_file_=$2

if [ "$3" ]; then
    _exclude_name_=$3
fi


# Creating specified directory under mounted migration backup directory
_backup_dir_=${MIGRATION_BACKUP_DIR}/${HNAME}/${_content_name_}

# Remove if present and recreate
if [ -d ${_backup_dir_} ]; then
    $RM -rf ${_backup_dir_} 2>> /dev/null
fi
$MKDIR -p ${_backup_dir_}
if [ ! -d ${_backup_dir_} ]; then
    _err_msg_="Could not create directory ${_backup_dir_}"
    abort_script "${_err_msg_}"
fi

log_msg -s "\n\nDumping the ${_content_name_} filesystem to ${_backup_dir_}" -l ${LOGFILE}

# Exclude list for content
EXCLUDE_CMD=""
if [ "${_exclude_name_}" ]; then
    _exclude_list_=""
    _excluded_content_=`iniget ${_exclude_name_} -f ${_ini_file_}`
    for _item_ in ${_excluded_content_}; do
        if [ ! "${_exclude_list_}" ]; then
            _exclude_list_="^${_item_}"
        else
            _exclude_list_="${_exclude_list_}|^${_item_}"
        fi
    done
    EXCLUDE_CMD="${_exclude_list_}"
fi

# Getting list of directories and file which needs to be backed up from backup_data.ini file
_backup_data_=`iniget ${_content_name_} -f ${_ini_file_}`
_rd1_backup_dir_=${MIGRATION_BACKUP_DIR}/${RD1_HNAME}/${_content_name_}
_rd2_backup_dir_=${MIGRATION_BACKUP_DIR}/${RD2_HNAME}/${_content_name_}
for _file_ in `$ECHO ${_backup_data_}`;do
    if [ -d ${_file_} ]; then
        if [ "${EXCLUDE_CMD}" ]; then
            $FIND ${_file_} -xdev -depth -print | $EGREP -v "${EXCLUDE_CMD}" | $CPIO -pdum ${_backup_dir_} >> /dev/null 2>&1
        else
            $FIND ${_file_} -xdev -depth -print | $CPIO -pdum ${_backup_dir_} >> /dev/null 2>&1
        fi
        if [ $? -ne 0 ]; then
            log_msg -s "\n\nDumping the ${_content_name_} filesystem to ${_backup_dir_} failed" -l ${LOGFILE}
            _err_msg_="Could not backup the ${_content_name_} filesystem to ${_backup_dir_}"
            abort_script "${_err_msg_}"
        fi
    elif [ -f ${_file_} ]; then 
        $CP -p ${_file_} ${_backup_dir_} >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            _err_msg_="Could not backup ${_file_} file to ${_backup_dir_}"
            abort_script "${_err_msg_}"
        fi

        if [ "${CO_SERVER}" == "YES" -a "${_file_}" == "/eniq/installation/config/fls_conf" ]; then
             log_msg -s "\nCopying ${_file_} file to ${_rd1_backup_dir_}/${_file_}" -l ${LOGFILE}
             $CP -p ${_file_} ${_rd1_backup_dir_}/${_file_} >> /dev/null 2>&1
             
             log_msg -s "\nCopying ${_file_} file to ${_rd2_backup_dir_}/${_file_}" -l ${LOGFILE}
             $CP -p ${_file_} ${_rd2_backup_dir_}/${_file_} >> /dev/null 2>&1
        fi

    else
        log_msg -s "\n${_file_} file or directory does not exists" -l ${LOGFILE}
    fi
done
}

### Function: backup_iqheader ###
#
# Backup iq_header_info file
#
# Arguments:
#   none
# Return Values:
#   none
backup_iqheader()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip if Rack server
if [ "${STORAGE_TYPE}" != "raw" ]; then
    insert_header_footer foot "Rack Migration - Skipping - ${NEXT_STAGE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip if engine
if [ "${CO_SERVER}" != "YES" -a "${RD_SERVER}" != "YES" ]; then
    insert_header_footer foot "Skipping the stage for ${CURR_SERVER_TYPE} - ${NEXT_STAGE} " ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip if premigration on CO is not selected in case of multiblade replacement
if [ "${CO_SERVER}" ]; then
    if [[ ! "${BACKUP}" ]] && [[ ! -f ${MIGRATE_CO} ]]; then
        insert_header_footer foot "Skipping replacement stage on ${CURR_SERVER_TYPE} - ${NEXT_STAGE}" ${LOGFILE}
        set_next_stage `$EXPR ${ARRAY_ELEM}+1`
        return 0
    fi
fi

# Check NASd is online
check_and_manage_smf ${NASd_SMF_ID} enable

IQ_HEADER_INFO=${ENIQ_CONF_DIR}/iq_header_info

# Save iq header data
save_iq_header_info

if [ ! -s ${IQ_HEADER_INFO} ]; then
    _err_msg_="Unable to save ${IQ_HEADER_INFO} file."
    abort_script "${_err_msg_}"
fi

log_msg -s "\nSuccessfully taken iq_header_info backup in ${IQ_HEADER_INFO} file." -l ${LOGFILE}

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}


### Function: backup_nas_data ###
#
#   Backup NAS data
#
# Arguments:
#   none
# Return Values:
#   none
backup_nas_data()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skipping this stage on non-Coordinator server
if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "Skipping the stage for ${CURR_SERVER_TYPE} server - ${NEXT_STAGE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Taking the inputs from "backup_migration_data.ini" file to backup the contents to NAS 
backup_contents NAS ${MIGRATION_TEMPL_DIR}/${BACKUP_DATA_INI}

log_msg -q -s "Taken backup of NAS data\n" -l ${LOGFILE}
insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: backup_root_data ###
#
#   Backup root data
#
# Arguments:
#   none
# Return Values:
#   none
backup_root_data()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Backup the data
backup_contents ROOT ${MIGRATION_TEMPL_DIR}/${BACKUP_DATA_INI} ROOT_EXCLUDE

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: backup_zfs_data ###
#
#   Backup ZFS data
#
# Arguments:
#   none
# Return Values:
#   none
backup_zfs_data()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Enable NASd if storage is raw
if [ "${STORAGE_TYPE}" == "raw" ]; then
    check_and_manage_smf ${NASd_SMF_ID} enable
fi

# Stop the ENIQ services, if exist, before taking backup of ZFS
$SVCS -a | $GREP eniq >> /dev/null 2>&1
if [ $? -eq 0 ];then
    stop_eniq_services
fi

# Stop SMF services
log_msg -s "\nStopping SMF services." -l ${LOGFILE}
stop_smf_services

# Disabling OSS mounts
disable_oss_mounts

# Taking the inputs from "backup_migration_data.ini" file to backup the contents to NAS 
backup_contents ZFS ${MIGRATION_TEMPL_DIR}/${BACKUP_DATA_INI}

log_msg -q -s "\nTaken backup of ZFS data\n" -l ${LOGFILE}
insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}



### Function: change_mount_owners ###
#
# Change ownership of ALL mounts
#
# Arguments:
#   none
# Return Values:
#   none
change_mount_owners()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling change_mount_owners stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s change_mount_owners ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: check_absolute_path ###
#
# Determine absolute path to software
#
# Arguments:
#   none
# Return Values:
#   none
check_absolute_path()
{
_dir_=`$DIRNAME $0`
SCRIPTHOME=`cd $_dir_ 2>/dev/null && pwd || $ECHO $_dir_`

if [ "${ACTION_TYPE}" == "premigration" ];then
    if [[ ! "${BACKUP}" ]] && [[ ! "$SCRIPTHOME" =~ ^/var/tmp.* ]];then
        _err_msg_="Script should be placed and run from a directory under /var/tmp/ location."
        abort_script "${_err_msg_}"
    fi
fi
}


### Function: check_and_manage_smf ###
#
#   Check SMF service status and manage
#
# Arguments:
#   $1 : SMF name
#   $2 : Action
# Return Values:
#   none
check_and_manage_smf()
{
# Check argument count
if [ $# -ne 2 ];then
    _err_msg_="Incorrect number of argument passed to check_and_manage_smf."
    abort_script "${_err_msg_}"
fi

_smf_str_=$1
_svc_action_=$2

# Check status of SMF
$SVCS ${_smf_str_} >> /dev/null
if [ $? -ne 0 ];then
    _err_msg_="Could not find SMF ${_smf_str_}"
    abort_script "${_err_msg_}"
fi
_smf_status_=`$SVCS -H ${_smf_str_} | $AWK '{print $1}'`

_smf_next_state_=""
case $_svc_action_ in
  enable) _smf_next_state_="online"
     ;;
  disable) _smf_next_state_="disabled"
     ;;
  *) $ECHO "Invalid action"
     exit 1
     ;;
esac

if [ "${_smf_status_}" != "${_smf_next_state_}" ];then
    if [ "${_smf_status_}" == "maintenance" ];then
        $SVCADM $CLEAR ${_smf_str_} >> /dev/null 2>&1
    fi
    $SVCADM ${_svc_action_} -s ${_smf_str_} >> /dev/null 2>&1
    if [ $? -ne 0 ];then
        _err_msg_="Could not $_svc_action_ `$BASENAME $_smf_str_ :default` service."
        abort_script "${_err_msg_}"
    fi
fi

# Wait for NAS milestone service if enabling NASd service 
if [ "${_smf_str_}" == "${NASd_SMF_ID}" -a "${_svc_action_}" == "enable" ];then
    _count_=0
    while [ 1 ];
    do
        _milestone_state_=`$SVCS -H -o state ${NAS_MILESTONE_SMF_ID}`
        if [ "${_milestone_state_}" == "online" ];then
            log_msg -q -s "${NAS_MILESTONE_SMF_ID} service is online." -l ${LOGFILE}
            break
        fi
        if [ $_count_ -eq 0 ]; then
            $ECHO "\nEnabling NAS milestone service. It can take upto 2 to 5 minutes. Please wait...\n"
        fi
        $SLEEP 30
        let _count_=_count_+1
        if [ $_count_ -eq 5 ];then
            _err_msg_="NAS milestone ${NAS_MILESTONE_SMF_ID} SMF not online. Check ${ENIQ_LOG_DIR}/NASd/NASd.log"
            abort_script ${_err_msg_}
        fi
    done
fi
}

### Function: check_for_file ###
#
# To check whether file or directory exist or not and to test the basic file operations.
#
# Arguments:
#       $1 : File operations
#       $2 : File qualified path
# Return Values:
#         none
check_for_file()
{
if [ ! $1 $2 ]; then
    _err_msg_="$2 does not exist"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi
}

### Function: check_id ###
#
#   Check that the effective id of the user is correct
#   If not print error msg and exit.
#
# Arguments:
#       $1 : User ID name
# Return Values:
#       none
check_id()
{
_check_id_=`$ID | $AWK -F\( '{print $2}' | $AWK -F\) '{print $1}'`
if [ "$_check_id_" != "$1" ]; then
    _err_msg_="You must be $1 to execute this script."
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi
}


### Function: check_mandatory_files ###
#
#   Checks mandatory files in backup Directory in ZPOOL
#
# Arguments:
#   none
# Return Values:
#   none
check_mandatory_files()
{
_mandatory_files_="etc/hosts etc/netmasks etc/defaultrouter"

for _file_ in ${_mandatory_files_}
do
    log_msg -s "Checking the mandatory file ${_file_} in the backup directory ${MIGRATION_BACKUP_DIR}" -l ${LOGFILE}
    if [ ! -f ${MIGRATION_BACKUP_DIR}/${HNAME}/ROOT/${_file_} ]; then
        _err_msg_="Couldn't backup the mandatory file ${_file_} to the ZFS pool. Take manual backup for ${_file_} to directory ${MIGRATION_BACKUP_DIR} and re-run the stage"
        abort_script "${_err_msg_}"
    fi
done
}

### Function: check_params ###
#
# Check Input Params
#
# Arguments:
#    none
# Return Values:
#    none
check_params()
{
# Check that we got the required action type
if [ ! "${ACTION_TYPE}" ]; then
    $ECHO "\nERROR: Required parameters not passed."
    usage_msg
    exit 1
fi

if [ "${ACTION_TYPE}" != "premigration" -a "${ACTION_TYPE}" != "recovery" -a "${ACTION_TYPE}" != "cleanup" ]; then
    $ECHO "\nERROR: Not a valid action type"
    usage_msg
    exit 1
fi

case $ACTION_TYPE in
  premigration) STOP_STAGE="cleanup_premigration"
                ACTIVITY="Backup procedure for Linux OS Migration"
                _base_sw_param_="MIG_BASE_SW_LOC"
                _om_sw_param_="MIG_OM_SW_LOC"
     ;;
  recovery) STOP_STAGE="recovery_cleanup"
            ACTIVITY="procedure to recover Linux OS"
            _base_sw_param_="REC_BASE_SW_LOC"
            _om_sw_param_="REC_OM_SW_LOC"
     ;;
  cleanup) ACTIVITY="procedure to clean temporary files/directories"
     ;;
  \?) $ECHO "Invalid action type"
     usage_msg
     exit 1
     ;;
esac

}

### Function: cleanup_premigration ###
#
#   Cleanup premigration data
#
# Arguments:
#   none
# Return Values:
#   none
cleanup_premigration()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

log_msg -s "Cleaning up temporary directories used for premigration." -l ${LOGFILE}
$RM -rf ${TEM_DIR} >> /dev/null 2>&1

# If CO is not premigrated, remove stage file
if [ ! -f ${MIGRATE_CO} ]; then
    log_msg -s "Cleaning up temporary files used for replacement." -l ${LOGFILE}
    $RM -rf ${STAGEFILE} >> /dev/null 2>&1
    $RM -rf ${_selected_servers_} >> /dev/null 2>&1
fi

log_msg -s "Cleaning up flag ${MIGRATE_CO} used for premigration." -l ${LOGFILE}
$RM -rf ${MIGRATE_CO} >> /dev/null 2>&1

log_msg -s "Cleaning up flag ${MIGR_PROGRESS} used for premigration." -l ${LOGFILE}
$RM -rf ${MIGR_PROGRESS} >> /dev/null 2>&1

if [ ! ${BACKUP} ]; then
    $TOUCH ${MIGR_SUCCESS}
fi

# Backing up log file on NAS
log_msg -s "Dumping premigration logfile on NAS." -l ${LOGFILE}
$CP -p ${LOGFILE} ${MIGRATION_BACKUP_DIR}/${HNAME}/ROOT/${VAR_TMP_UPGRADE} >> /dev/null 2>&1

# Unmount portbackup
log_msg -s "\nUnmounting ${MIGRATION_BACKUP_DIR}" -l ${LOGFILE}
${UMOUNT} -f ${MIGRATION_BACKUP_DIR} >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not unmount ${MIGRATION_BACKUP_DIR}"
    abort_script "${_err_msg_}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
}

### Function: clear_data ###
#
#   Clear migration/recovery backup 
#   Files/Directories and Flags from server
#
# Arguments:
#   none
# Return Values:
#   none
clear_data()
{
log_msg -s "\nChecking for unnecessary files/directories to be cleaned up.\n" -l ${LOGFILE}

_flag_list_=`$LS -1 ${VAR_DIR}/tmp/linux_*_success 2>/dev/null`
if [ "${ACTION_TYPE}" != "cleanup" ]; then
    _flag_list_="${_flag_list_} `$LS -1 ${VAR_DIR}/tmp/linux_*_progress 2>/dev/null`"
fi

# Remove flag files
if [ "${_flag_list_}" ]; then
    log_msg -s "\nRemoving flags used for migration." -l ${LOGFILE}
    for _flag_ in ${_flag_list_}; do
        $RM -rf ${_flag_} >> /dev/null 2>&1
        if [ ! -f "${_flag_}" ]; then
            log_msg -s "Successfully removed ${_flag_} file." -l ${LOGFILE}
        fi
    done
fi

# Directories to be removed
_remove_list_="${CONFIG_BACKUP_DIR} ${MIG_SW_BACKUP_DIR}"
for _dirname_ in ${_remove_list_}; do
    if [ -d ${_dirname_} ]; then
        if [ "${ACTION_TYPE}" == "cleanup" ]; then
            log_msg -s "\nRemoving directory ${_dirname_} used for migration." -l ${LOGFILE}
            $RM -rf ${_dirname_} >> /dev/null 2>&1
            if [ ! -d "${_dirname_}" ]; then
                log_msg -s "Successfully removed ${_dirname_} directory." -l ${LOGFILE}
            fi
        fi
    fi
done

# Cleaning up configuration file
if [ -f ${MIGRATION_CONF} ]; then
    log_msg -s "\nRemoving migration config file used for migration." -l ${LOGFILE}
    $RM -rf ${MIGRATION_CONF} >> /dev/null 2>&1
    if [ ! -f "${MIGRATION_CONF}" ]; then
        log_msg -s "Successfully removed ${MIGRATION_CONF} file." -l ${LOGFILE}
    fi
fi

# Remove ipmp.ini backup file
if [ -f "${ENIQ_CONF_DIR}/${IPMP_INI}.recovery" ];then
    log_msg -s "\nRemoving ${ENIQ_CONF_DIR}/${IPMP_INI}.recovery used for migration." -l ${LOGFILE}
    $RM -rf ${ENIQ_CONF_DIR}/${IPMP_INI}.recovery >> /dev/null 2>&1
    if [ ! -f "${ENIQ_CONF_DIR}/${IPMP_INI}.recovery" ]; then
        log_msg -s "Successfully removed ${ENIQ_CONF_DIR}/${IPMP_INI}.recovery file." -l ${LOGFILE}
    else
        log_msg -s "Remove ${ENIQ_CONF_DIR}/${IPMP_INI}.recovery file manually." -l ${LOGFILE}
    fi
fi

# Destroy zpool backup dir 
_mig_bkup_dir_="${BACKUP_DIR}"_migrated
$ZFS list -H -o name | $GREP ${_mig_bkup_dir_} >> /dev/null 2>&1
if [ $? -eq 0 ];then
    cd /
    log_msg -s "\n${_mig_bkup_dir_} exists. Destroying." -l ${LOGFILE}
    $ZFS destroy -rf ${_mig_bkup_dir_}
    if [ $? -ne 0 ];then
        _err_msg_="Could not destroy ${_mig_bkup_dir_}."
        abort_script "${_err_msg_}"
    fi
    log_msg -s "Successfully removed zpool directory ${_mig_bkup_dir_}\n" -l ${LOGFILE}
fi


# Remove portbackup directory
if [ -d "${MIGRATION_BACKUP_DIR}" ];then
    log_msg -s "\nRemoving "${MIGRATION_BACKUP_DIR}" directory." -l ${LOGFILE}
    $UMOUNT -f ${MIGRATION_BACKUP_DIR} >> /dev/null 2>&1
    $RM -rf "${MIGRATION_BACKUP_DIR}" >> /dev/null 2>&1
    if [ -d "${MIGRATION_BACKUP_DIR}" ];then 
        _err_msg_="Failed to remove ${MIGRATION_BACKUP_DIR} file." -l ${LOGFILE}
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    else
        log_msg -s "Successfully removed "${MIGRATION_BACKUP_DIR}" "-l ${LOGFILE}
    fi
fi
}

### Function: configure_backup_vlan ###
#
# To configure the Backup VLAN interface
#
# Arguments:
#        none
# Return Values:
#         none
configure_backup_vlan()
{
# Check if backup vlan info exists in migration conf
$CAT ${MIGRATION_CONF} | $GREP "^BKUP_VLAN_EXIST_${HNAME}" >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    log_msg -s "Migration config file doesn't contain Backup VLAN information. Skipping step." -l ${LOGFILE} 
    return 0
fi

# Check if backup vlan configuration required
_backup_vlan_status_=`read_value BKUP_VLAN_EXIST_${HNAME} ${MIGRATION_CONF}` || abort_script "${_backup_vlan_status_}"
if [ "${_backup_vlan_status_}" == "NO" ]; then
    log_msg -s "Backup VLAN configuration not required." -l ${LOGFILE}
    return 0
fi

# Get interface information from conf file
_backup_vlan_name_=`read_value BKUP_VLAN_INTF_NAME_${HNAME} ${MIGRATION_CONF}` || abort_script "${_backup_vlan_name_}"
_backup_vlan_ip_=`read_value BKUP_VLAN_INTF_IP_${HNAME} ${MIGRATION_CONF}` || abort_script "${_backup_vlan_ip_}"
_backup_vlan_netmask_=`read_value BKUP_VLAN_INTF_NETMASK_${HNAME} ${MIGRATION_CONF}` || abort_script "${_backup_vlan_netmask_}"

# Create the interface 
create_interface ${_backup_vlan_name_} ${_backup_vlan_ip_} ${_backup_vlan_netmask_}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to create Backup VLAN on $HNAME."
    abort_script "${_err_msg_}"
fi
}

### Function: confirm_user_input ###
#
# Confirm with user that values entered are correct
#
# Arguments:
#       none
# Return Values:
#       none
confirm_user_input()
{
while :; do
    $CLEAR

    $CAT ${1}
    read USER_CONF

    # If the User hit nothing
    if [ ! "${USER_CONF}" ]; then
        continue
    fi

    # Did the user input (Y/y)
    if [ "${USER_CONF}" == "Y" -o "${USER_CONF}" == "y" -o "${USER_CONF}" == "N" -o "${USER_CONF}" == "n" ]; then
        break
    fi
done
}

### Function: copy_config_files ###
#
#   Copy configuration files
#
# Arguments:
#   none
# Return Values:
#   none
copy_config_files()
{
log_msg -s "Copying configuration files." -l ${LOGFILE}

_source_dir_=/eniq_zfs_storage_pools/${BACKUP_DIR}

# Copy etc files
_file_list_="etc/hosts etc/netmasks"

# Files required in Solaris 10
if [ "${SOLARIS_10}" ]; then
    _file_list_="${_file_list_} etc/defaultrouter"
fi

# Additional files if "raw"
if [ "${STORAGE_TYPE}" == "raw" ]; then
    _file_list_="${_file_list_} agentID.txt opt/Unisphere/bin/agent.config etc/Unisphere/agent.config"
fi

for _file_ in ${_file_list_}
do
    log_msg -s "Copying ${_source_dir_}/${_file_} to /${_file_}" -l ${LOGFILE}
    if [ -f ${_source_dir_}/${_file_} ];then
        $CP ${_source_dir_}/${_file_} /${_file_} >> /dev/null 2>&1
        if [ $? -ne 0 ];then
            _err_msg_="Could not copy ${_source_dir_}/${_file_} to /${_file_}"
            abort_script "${_err_msg_}"
        fi
    fi
done
}


### Function: copy_migr_conf_to_nas ###
#
# Copy migration software to /eniq/backup/migration_sw
#
# Arguments:
#   none
# Return Values:
#   none
copy_migr_conf_to_nas()
{
# Copy only migration conf to backup directory if non-CO

if [ ! "${CO_SERVER}" ]; then
    check_and_copy ${MIGRATION_CONF} ${MIG_SW_BACKUP_DIR}/
    log_msg -q -s "Successfully copied ${MIGRATION_CONF} to ${MIG_SW_BACKUP_DIR}" -l ${LOGFILE}
    return 0
fi

# Create new migration config backup directory
if [ -d ${MIG_SW_BACKUP_DIR} ];then
    if [ "${BACKUP}" ]; then
        if [ -f ${MIG_SW_BACKUP_DIR}/${MIGRATION_CONF_FILE} ]; then
            log_msg -q -s "Migration Conf file ${MIGRATION_CONF} already present removing..." -l ${LOGFILE}
            $RM -rf ${MIG_SW_BACKUP_DIR}/${MIGRATION_CONF_FILE}
        fi
    else
        log_msg -q -s "Migration Conf directory ${MIG_SW_BACKUP_DIR} already present removing and recreating..." -l ${LOGFILE}
        $RM -rf ${MIG_SW_BACKUP_DIR}
    fi
fi

$MKDIR -p ${MIG_SW_BACKUP_DIR}

# Copy migration config files if CO 
# lib file
check_and_copy ${MIGRATION_LIB}/common_migration_functions.lib ${MIG_SW_BACKUP_DIR}/

# config file
check_and_copy ${MIGRATION_CONF} ${MIG_SW_BACKUP_DIR}/

log_msg -q -s "Successfully copied required config files to ${MIG_SW_BACKUP_DIR}" -l ${LOGFILE}
}

### Function: create_nas_users ###
#
# Create NAS users
#
# Arguments:
#       none
# Return Values:
#       none
create_nas_users()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_nas_users stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_nas_users ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

log_msg -s "\nSetting up SSH connection for NAS." -l ${LOGFILE}

# Check required files and scripts exist
local _ssh_input_file_="${ENIQ_CONF_DIR}/ssh_input_file"
check_for_file -s ${_ssh_input_file_} 

local _setup_ssh_FileStore_script_="${ERICSSON_BIN_DIR}/setup_ssh_FileStore.sh"
check_for_file -s ${_setup_ssh_FileStore_script_} 

log_msg -s "\nExecuting command: $BASH ${ERICSSON_BIN_DIR}/setup_ssh_FileStore.sh ${ENIQ_CONF_DIR}/ssh_input_file" -l ${LOGFILE}
$BASH ${ERICSSON_BIN_DIR}/setup_ssh_FileStore.sh ${ENIQ_CONF_DIR}/ssh_input_file
if [ $? -ne 0 ];then
     _err_msg_="Could not setup SSH connectivity. Please check logfile ${ERICSSON_STOR_DIR}/log/setup_ssh_FileStore.log for further details."
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: copy_new_sw ###
#
# Copy core install software to /eniq/installation
#
# Arguments:
#   none
# Return Values:
#   none
copy_new_sw()
{
ENIQ_CORE_DIR=${ENIQ_CORE_INST_DIR}

# Templates Directory
ENIQ_TEMPL_DIR="${ENIQ_CORE_DIR}/templates/${INSTALL_TYPE}"

local _file_to_copy_loc_=${MIGRATION_HOME}/core_install
local _file_list_=`$LS ${_file_to_copy_loc_} | $EGREP -vi "etc|version|lib"`
local _san_device_=""
if [ ! "${_file_list_}" ]; then
    _err_msg_="Could not build a list of directories to update"
    abort_script "$_err_msg_"
fi

# Take backup of the older release storage.ini and SunOS.ini templates
_ini_file_list_="storage_ini SunOS.ini"
_tmpl_backup_dir_=${CONFIG_BACKUP_DIR}/ini_tmpl_backup
if [ -d ${_tmpl_backup_dir_} ]; then
    log_msg -s "Removing exisiting directory ${_tmpl_backup_dir_}." -l ${LOGFILE}
    $RM -rf ${_tmpl_backup_dir_} >>/dev/null 2>&1
fi
$MKDIR -p ${_tmpl_backup_dir_}
if [ $? -ne 0 ]; then
    _err_msg_="Could not make directory ${_tmpl_backup_dir_}"
    abort_script "$_err_msg_"
fi

for _ini_file_ in ${_ini_file_list_}
do
    log_msg -s "Taking backup of existing ${_ini_file_} templates from ${ENIQ_TEMPL_DIR} to ${_tmpl_backup_dir_}." -l ${LOGFILE}
    $CP -p ${ENIQ_TEMPL_DIR}/${_ini_file_}* ${_tmpl_backup_dir_}
    if [ $? -ne 0 ]; then
        _err_msg_="Could not copy ${_ini_file_} templates to ${_tmpl_backup_dir_}"
        abort_script "${_err_msg_}"
    fi
done

log_msg -s "\nRemoving current core install SW" -l ${LOGFILE}
for _dir_ in `$ECHO ${_file_list_}`; do
    $RM -rf ${ENIQ_CORE_DIR}/${_dir_}
    if [ $? -ne 0 ]; then
        _err_msg_="Could not remove ${ENIQ_CORE_DIR}/${_dir_}"
        abort_script "$_err_msg_"
    fi
done

log_msg -s "Copying new core SW from ${_file_to_copy_loc_} to ${ENIQ_CORE_DIR}" -l ${LOGFILE}
for _dir_ in `$ECHO ${_file_list_}`; do
    cd ${_file_to_copy_loc_}/${_dir_}
    $MKDIR -p ${ENIQ_CORE_DIR}/${_dir_}
    if [ $? -ne 0 ]; then
        _err_msg_="Could not make directory ${ENIQ_CORE_DIR}/${_dir_}"
        abort_script "$_err_msg_"
    fi
    log_msg -q -s "Copying from ${_file_to_copy_loc_}/${_dir_} to ${ENIQ_CORE_DIR}/${_dir_}" -l ${LOGFILE}
    $FIND . -depth -print | $CPIO -pdmu ${ENIQ_CORE_DIR}/${_dir_} >> /dev/null 2>&1
    if [ $? -ne 0 ]; then
        _err_msg_="Could not copy new SW from ${_file_to_copy_loc_}/${_dir_} to ${ENIQ_CORE_DIR}/${_dir_}"
        abort_script "$_err_msg_"
    fi
done

log_msg -s "Copying new scripts ${_file_to_copy_loc_}/etc to ${ENIQ_CORE_DIR}/etc" -l ${LOGFILE}
for _etc_file_ in `$LS ${_file_to_copy_loc_}/etc`; do
    # We need not to copy stage file of premigration from /var/tmp location
    if [ "${_etc_file_}" == "eniq_linux_migr_stage" ]; then
        continue
    fi
    log_msg -q -s "Copying ${_file_to_copy_loc_}/etc/${_etc_file_} to ${ENIQ_CORE_DIR}/etc" -l ${LOGFILE}
    $CP ${_file_to_copy_loc_}/etc/${_etc_file_} ${ENIQ_CORE_DIR}/etc
    if [ $? -ne 0 ]; then
        _err_msg_="Could not copy ${_file_to_copy_loc_}/etc/${_etc_file_} to ${ENIQ_CORE_DIR}/etc"
        abort_script "$_err_msg_"
    fi
done

for _ini_file_ in ${_ini_file_list_}
do
    log_msg -s "Copying back the ${_ini_file_} templates from ${_tmpl_backup_dir_} to ${ENIQ_TEMPL_DIR}" -l ${LOGFILE}
    $CP -rp ${_tmpl_backup_dir_}/${_ini_file_}* ${ENIQ_TEMPL_DIR}
    if [ $? -ne 0 ]; then
        _err_msg_="Could not copy ${_ini_file_} templates from ${_tmpl_backup_dir_} to ${ENIQ_TEMPL_DIR}"
        abort_script "${_err_msg_}"
    fi
done

return 0
}

### Function: create_admin_dir ###
#
# Creates the admin directory
#
# Arguments:
#   none
# Return Values:
#   none
create_admin_dir()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip stage on non-Coordinator blades
if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for ${CURR_SERVER_TYPE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Calling create_admin_dir stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -n -s create_admin_dir -u -l ${LOGFILE}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_directories ###
#
# Creates all required Directories
#
# Arguments:
#   none
# Return Values:
#   none
create_directories()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_directories stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_directories ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_groups ###
#
# Creates all required Groups
#
# Arguments:
#   none
# Return Values:
#   none
create_groups()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_groups stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_groups ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_interface ###
#
# To configure interface  
#
# Arguments:
#        $1 : Interface Name
#        $2 : Interface IP
#        $3 : Interface Netmask IP
# Return Values:
#         none
create_interface()
{
# Check if required parameters are passed
if [ $# -ne 3 ]; then
    _err_msg_="Inadequate information passed to configure interface."
    abort_script "${_err_msg_}"
fi

_intf_name_=$1
_intf_ip_=$2
_intf_netmask_=$3

# Calculate subnet if required
_intf_subnet_=""
if [ ! "${SOLARIS_10}" ]; then
    _intf_subnet_=`get_network_from_netmask ${_intf_netmask_}`
fi

_add_intf_=0

# Get list of interfaces available
_intf_list_file_=${TEM_DIR}/available_interface_list
log_msg -s "\nCreating list of interfaces available on current server." -l ${LOGFILE}
if [ ! "${SOLARIS_10}" ];then
    $DLADM show-link -p -o LINK | $SORT -u > ${_intf_list_file_}
else
    $DLADM show-dev | $AWK '{print $1}' > ${_intf_list_file_}
fi

if [ ! -s ${_intf_list_file_} ];then
    _err_msg_="Could not fetch available interfaces."
    log_msg -s "${_err_msg_}" -l ${LOGFILE}
    return 1
fi

# Check if interface name valid
$GREP ${_intf_name_} ${_intf_list_file_} >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    if [ ! "${SOLARIS_10}" ];then
        $DLADM show-phys | $GREP ${_intf_name_} >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            _err_msg_="Invalid interface name ${_intf_name_} provided."
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi
        _intf_name_=`$DLADM show-phys | $GREP ${_intf_name_} | $AWK '{print $1}'`
    else
        _err_msg_="Invalid interface name ${_intf_name_} provided during premigration."
        log_msg -s "${_err_msg_}" -l ${LOGFILE}
        return 1
    fi
fi

# Check if the interface is already configured
_assigned_ip_=""
if [ ! "${SOLARIS_10}" ]; then
    $IPADM show-if -p -o IFNAME ${_intf_name_} >> /dev/null 2>&1
    if [ $? -eq 0 ]; then
        _assigned_ip_=`$IPADM show-addr -p -o ADDR ${_intf_name_}/v4 | $CUT -f1 -d'/'`
    fi
else
    $LS /etc/hostname.${_intf_name_} >> /dev/null 2>&1
    if [ $? -eq 0 ]; then
        _assigned_ip_=`$CAT /etc/hostname.${_intf_name_} | $AWK '{print $1}'`
    fi
fi

if [ "${_assigned_ip_}" == "${_intf_ip_}" ]; then
    log_msg -s "\nInterface ${_intf_name_} is already configured on server." -l ${LOGFILE}
    return 0
else
    _add_intf_=1
fi

# Configure the interface
if [ $_add_intf_ -eq 1 ]; then
    # Check if we have all the relevant information
    if [ ! "${_intf_name_}" -o ! "${_intf_ip_}" -o ! "${_intf_netmask_}" ]; then
        _err_msg_="Required details to create interface not available"
        log_msg -s "${_err_msg_}" -l ${LOGFILE}
        return 1
    fi
    if [ ! "${SOLARIS_10}" ]; then
        if [ ! "${_intf_subnet_}" ]; then
            _err_msg_="Required subnet to create interface not available"
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi
    fi

    # Create the interface with the details
    if [ ! "${SOLARIS_10}" ]; then
        # Re-create the interface if exists
        $IPADM delete-ip ${_intf_name_} >> /dev/null 2>&1
        $IPADM create-ip ${_intf_name_} >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            _err_msg_="Unable to create interface ${_intf_name_}."
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi

        # Assign IP to the interface
        $IPADM create-addr -T static -a ${_intf_ip_}/${_intf_subnet_} ${_intf_name_}
        if [ $? -ne 0 ]; then
            _err_msg_="Unable to assign IP to ${_intf_name_}." 
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi
        log_msg -l ${LOGFILE} -s "Successfully created interface ${_intf_name_}"
    else
        # Configure temporary interface hostname file
        $ECHO "${_intf_ip_} netmask ${_intf_netmask_} broadcast + up" > ${TEM_DIR}/hostname.${_intf_name_}
        if [ ! -s ${TEM_DIR}/hostname.${_intf_name_} ]; then
            _err_msg_="Could not write to temporary hostname.${_intf_name_} file."
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi

        # Plumb the interface
        ${IFCONFIG} ${_intf_name_} unplumb >> /dev/null 2>&1
        ${IFCONFIG} ${_intf_name_} plumb >> /dev/null 2>&1
        $SLEEP 5

        # Assign IP to the interface 
        ${IFCONFIG} ${_intf_name_} ${_intf_ip_} netmask ${_intf_netmask_}  >> /dev/null 2>&1
        if [  $? -ne 0 ]; then
            _err_msg_="Failed to Configure Interface [ ${_intf_name_} ]"
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi
        $SLEEP 5

        ${IFCONFIG} ${_intf_name_} up >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            _err_msg_="Failed to activate Interface [ ${_intf_name_} ]"
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi
        $CP ${TEM_DIR}/hostname.${_intf_name_} /etc/hostname.${_intf_name_}
        if [ ! -s ${TEM_DIR}/hostname.${_intf_name_} ]; then
            _err_msg_="Could not save hostname.${_intf_name_} file."
            log_msg -s "${_err_msg_}" -l ${LOGFILE}
            return 1
        fi
    fi
fi
}

### Function: create_lun_map ###
#
# Create LUN MAP ini file
#
# Arguments:
#   none
# Return Values:
#   none
create_lun_map()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_lun_map stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_lun_map ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_nas_fs ###
#
# Create file system on NAS for data backup
#
# Arguments:
#   none
# Return Values:
#   none
create_nas_fs()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Initializing the variables to zero to calculate the total size
unset TOTAL_USED_ROOT_SIZE TOTAL_USED_ZPOOL_SIZE
TOTAL_USED_ZPOOL_SIZE=0
TOTAL_USED_ROOT_SIZE=0

# Skip stage for non CO servers
if [ ! "${CO_SERVER}" ]; then
    # Mounting newly created NAS FS
    mount_nas_shares
    insert_header_footer foot "Skipping the stage for ${CURR_SERVER_TYPE} - ${NEXT_STAGE} " ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Checking if nascli utility exists
if [ ! -s ${_nascli_} ]; then
    _err_msg_="${_nascli_} not found"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Checking if portbackup share exists and delete filesystem if share exists
_portbackup_share_=`${_nascli_} list_shares | $GREP -w "/vx/${SYS_ID}-portbackup" | $AWK '{print $1}'`
if [ "${_portbackup_share_}" ]; then
     ${_nascli_} delete_fs ${SYS_ID} portbackup
     if [ $? -ne 0 ]; then
           _err_msg_="Failed to delete portbackup share and filesystem"
           abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
     fi
fi

# Check available NAS pool size
log_msg -s "Checking available size of NAS pool ${SYS_ID}" -l ${LOGFILE}

_SFS_version_=`$CAT ${ERICSSON_FILESTOR_ETC}/nasplugin.conf |$GREP -w SFS_VERSION | $AWK -F"=" '{print $2}'`
if [ "${_SFS_version_}"  == "" ]; then
    _err_msg_="Could not fetch SFS version Details from NAS Plugin Configuration file."
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

if [ ${_SFS_version_} == 7.4 ]; then
    _size_=`$SU - storadm -c "$SSH -o StrictHostKeyChecking=no -n support@nasconsole '/opt/VRTSnas/clish/bin/clish -u master -c \"storage pool free \"'" | $GREP -w ${SYS_ID}|  $AWK '{print $2}'  | $TAIL -1`
else
    _size_=`$SU - storadm -c "$SSH -o StrictHostKeyChecking=no -n master@nasconsole storage pool free " | $GREP -w ${SYS_ID} |  $AWK '{print $2}'  | $TAIL -1`
fi

_free_size_=`$ECHO ${_size_} | $SED 's/.$//'`

if [ "${_size_: -1}" == "T" ];then
    _available_nas_pool_size_=`$ECHO "${_free_size_} * 1024" | $BC`
else
   _available_nas_pool_size_=`$ECHO "${_free_size_}" | $BC`
fi

if [ ! "${_available_nas_pool_size_}" ]; then
    _err_msg_="Could not determine available size of NAS pool."
    abort_script "${_err_msg_}"
fi

# Round off available NAS pool size
AVAILABLE_NAS_SIZE=`$PRINTF "%.${2}f" "${_available_nas_pool_size_}"`

log_msg -s "Available NAS size in GB : ${AVAILABLE_NAS_SIZE}\n" -l ${LOGFILE}

# Checking usage of both the zpools
for _line_ in `$CAT ${ORDER_FILE}`; do
    local _ip_address_=`$ECHO ${_line_} | $NAWK -F"::" '{print $1}'`
    local _serv_hostname_=`$ECHO ${_line_} | $NAWK -F"::" '{print $2}'`
    local _serv_type_=`$ECHO ${_line_} | $NAWK -F"::" '{print $3}'`
    if [ ! "${_ip_address_}" -o ! "${_serv_hostname_}" -o ! "${_serv_type_}" ]; then
        _err_msg_="Unable to retrieve IP or hostname or server type.\n"
        abort_script "${_err_msg_}"
    fi

    # If the IP found in the order file matches my IP, run locally
    if [ "${_ip_address_}" == "${HOST_IP}" ]; then
        _zpool_size_=`check_zpool_size`
        _root_fs_size_=`check_root_fs_size`
    else
        _zpool_size_=`run_remote_cmd "${_ip_address_}" ". ${SCRIPTHOME}/../lib/common_migration_functions.lib; check_zpool_size"`
        _root_fs_size_=`run_remote_cmd "${_ip_address_}" ". ${SCRIPTHOME}/../lib/common_migration_functions.lib; check_root_fs_size"`
    fi
        _zpool_size_=`$ECHO ${_zpool_size_}|$TR -d '\r'|$TR -d '\$'`
        _root_fs_size_=`$ECHO ${_root_fs_size_}|$TR -d '\r'|$TR -d '\$'`

        if [ ! "${_zpool_size_}" -o ! "${_root_fs_size_}" ]; then
            _err_msg_="Unable to retrieve zpool size or root file system size for ${_serv_hostname_}."
            abort_script "${_err_msg_}"
        fi
        log_msg -s "Total zpool size for ${_serv_hostname_}:${_serv_type_} = ${_zpool_size_}" -l ${LOGFILE}
        log_msg -s "Total root file system size for ${_serv_hostname_}:${_serv_type_} = ${_root_fs_size_}\n" -l ${LOGFILE}

        TOTAL_USED_ZPOOL_SIZE=`$ECHO "scale=2; ${TOTAL_USED_ZPOOL_SIZE} + ${_zpool_size_}" | $BC`
        TOTAL_USED_ROOT_SIZE=`$ECHO "scale=2; ${TOTAL_USED_ROOT_SIZE} + ${_root_fs_size_}" | $BC`
done

log_msg -s "Total zpool size for all servers in the deployment is ${TOTAL_USED_ZPOOL_SIZE}" -l ${LOGFILE}
log_msg -s "Total root file system size for all servers in the deployment is ${TOTAL_USED_ROOT_SIZE}" -l ${LOGFILE}

# Checking usage of NAS file systems
# Execute this function if server is Co-ordinator
if [ "${CO_SERVER}" ]; then
    check_NAS_fs_size
    log_msg -s "Total NAS software size for all servers in the deployment is ${TOTAL_USED_NAS_SIZE}" -l ${LOGFILE}

fi

# Total size used by both zpools, root and NAS file systems
_total_size_=`$ECHO "scale=2; ${TOTAL_USED_ZPOOL_SIZE}+${TOTAL_USED_ROOT_SIZE}+${TOTAL_USED_NAS_SIZE}" | $BC`

# Round off total size used by both zpools, root and NAS file systems
TOTAL_SIZE=`$PRINTF "%.${2}f" "${_total_size_}"`

    if [ ! "${TOTAL_SIZE}" ]; then
        _err_msg_="Could not determine total used size of NAS,zpool and root file systems."
        abort_script "${_err_msg_}"
    fi

log_msg -s "\nTotal size required by zpool, root and NAS for all servers in the deployment(GB) : ${TOTAL_SIZE}\n" -l ${LOGFILE}

if [ "${CO_SERVER}" ]; then
    _fs_size_=`$ECHO "scale=2; ${TOTAL_SIZE}+20" | $BC`
    _fs_name_="portbackup"
    _share_opts_="rw,no_root_squash"

    # Creating NAS FS for backup if available NAS size is greater than used size by zpools, root and NAS file systems
    if [ "${_fs_size_}" -lt "${AVAILABLE_NAS_SIZE}" ]; then
        log_msg -s "\nCreating NAS file system to take backup" -l ${LOGFILE}
        ${_nascli_} create_fs - ${_fs_size_}g - ${_fs_name_}
        if [ $? -ne 0 ]; then
            _err_msg_="Problem encountered creating NAS file system"
            abort_script "${_err_msg_}"
        fi
    else
        _err_msg_="Available NAS size is not enough to take backup."
        abort_script "${_err_msg_}"
    fi

    # Sharing newly created NAS FS
    nas_shares

    # Mounting newly created NAS FS
    mount_nas_shares
fi


insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: create_rbac_roles ###
#
# Create RBAC roles
#
# Arguments:
#   none
# Return Values:
#   none
create_rbac_roles()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_rbac_roles stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_rbac_roles ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_snapshots ### 
# 
# To create Snapshots of the server
# 
# Arguments: 
#        none 
# Return Values: 
#         none 
create_snapshots() 
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE} 

if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "Skipping on ${CURR_SERVER_TYPE} server - ${NEXT_STAGE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Snapshot command
SNAPSHOT_CMD="${ENIQ_BKUP_SW_BIN_DIR}/prep_eniq_snapshots.bsh"
if [ ! -x "${SNAPSHOT_CMD}" ]; then
    _err_msg_="Snapshot command doesn't have executable permission." 
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Start eniq services
start_eniq_services

# Creating flag to differentiate snapshot is triggered from migration script
$TOUCH ${CALLED_THROUGH_UPGRADE}

log_msg -s "\nStarting to run $BASH ${SNAPSHOT_CMD} -N" -l ${LOGFILE} 
$BASH ${SNAPSHOT_CMD} -N 
if [ $? -ne 0 ]; then 
    _err_msg_="Unable to take snapshot. Please refer logfile: ${SNAPSHOT_LOGFILE}. \n"
    $RM -rf ${CALLED_THROUGH_UPGRADE}
    $RM -rf ${ENIQ_ADMIN_BIN_DIR}/snap_label
    $RM -rf ${VAR_TMP_DIR}/snap_server_list
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
else
    # Saving created snapshot label into migration conf file
    save_snapshot_label
fi

# Removing flag after successful snapshot creation
$RM -rf ${CALLED_THROUGH_UPGRADE}

insert_header_footer foot "Successfully created Snapshots - ${NEXT_STAGE}" ${LOGFILE} 
set_next_stage `$EXPR ${ARRAY_ELEM}+1` 
}

### Function: create_users ###
#
# Creates all required Users
#
# Arguments:
#   none
# Return Values:
#   none
create_users()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

log_msg -s "\nChecking if dcuser is created." -l ${LOGFILE}
$CAT /etc/passwd | $GREP "^dcuser" >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    # Remove dcuser ssh keys
    if [ "${CO_SERVER}" ];then
         local _eniq_home_mount_=0

         _nfs_host_=`iniget Storage_NAS_HOME -f ${ENIQ_CONF_DIR}/storage.ini -v NFS_HOST`
         _share_path_=`iniget Storage_NAS_HOME -f ${ENIQ_CONF_DIR}/storage.ini -v SHARE_PATH`
         _mount_path_=`iniget Storage_NAS_HOME -f ${ENIQ_CONF_DIR}/storage.ini -v MOUNT_PATH`

         # Ensuring that the NAS_HOME Filesystem is mounted
         log_msg -t -s "Ensuring that the NAS_HOME Filesystem is mounted before proceeding further"

         # Create the mount point if its not there
         if [ ! -d ${_mount_path_} ]; then
             log_msg -s "\nCreating mount point - ${_mount_path_}" -l ${LOGFILE}
             $MKDIR -p ${_mount_path_} >> /dev/null 2>&1
             if [ $? -ne 0 ]; then
                 _err_msg_="Could not create mount point ${_mount_path_}"
                 abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
             fi
         fi

         _mount_exists_=`$MOUNT | $GREP "${_mount_path_}" | $AWK '{print $1}'`
         if [ ! "${_mount_exists_}" ]; then
              _eniq_home_mount_=1
         fi

         if [ ${_eniq_home_mount_} -eq 1 ]; then
             log_msg -s "Mounting ${_nfs_host_}:${_share_path_} on ${_mount_path_}" -l ${LOGFILE}
             $MOUNT -F nfs ${_nfs_host_}:${_share_path_} ${_mount_path_} >> /dev/null 2>&1 
             if [ $? -ne 0 ]; then
                 _err_msg_="Could not mount ${_nfs_host_}:${_share_path_} on ${_mount_path_}"
                 abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
             else
                 log_msg -s "\nSuccessfully mounted NAS_HOME Filesystem"
             fi
         else
             log_msg -s "${_mount_path_} is already mounted" -l ${LOGFILE}
         fi

         log_msg -s "\nTaking backup of dcuser .ssh keys." -l ${LOGFILE}
         $CP -rf ${DCUSER}.ssh ${DCUSER}.ssh_migbkup >> /dev/null 2>&1

         log_msg -s "\nRemoving dcuser .ssh keys." -l ${LOGFILE}
         $RM -rf ${DCUSER}.ssh >> /dev/null 2>&1
    fi
fi

# Remove CO root ssh keys if they exist
if [ "${CO_SERVER}" ]; then
    $CP -rf ${SSH_DIR} ${SSH_DIR}_migbkup >> /dev/null 2>&1
    log_msg -s "\nRemoving root .ssh keys." -l ${LOGFILE}
    $RM -rf ${SSH_DIR} >> /dev/null 2>&1
fi

#Calling create_users stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_users ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed in ${ACTION_TYPE} stage - create_users"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Remove backup of .ssh directories
$RM -rf ${SSH_DIR}_migbkup /eniq/home/dcuser/.ssh_migbkup >> /dev/null 2>&1

# Get the System User/Group. All directories are owned by this
SYSUSER=`iniget ENIQ_INSTALL_CONFIG -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v ENIQ_SYSUSER`
if [ ! "${SYSUSER}" ]; then
    _err_msg_="Could not read SYSUSER param from ${ENIQ_CONF_DIR}/${SUNOS_INI}"
    abort_script "$_err_msg_" "${EXEC_SHELL_CMD}"
fi

SYSGRP=`$ID ${SYSUSER}|$AWK '{print $2}'|$AWK -F\( '{print $2}'|$AWK -F\) '{print $1}'`
if [ ! "${SYSGRP}" ]; then
    _err_msg_="Could not read SYSGRP param from ${ENIQ_CONF_DIR}/${SUNOS_INI}"
    abort_script "$_err_msg_" "${EXEC_SHELL_CMD}"
fi

# Change ownership of /eniq/home/dcuser/.ssh
$CHOWN -R $SYSUSER:$SYSGRP ${DCUSER}.ssh
if [ $? -ne 0 ]; then
     _err_msg_="Could not change ownership of ${DCUSER}.ssh"
     abort_script "$_err_msg_" "${EXEC_SHELL_CMD}"
fi

## changing password for root user.
_host_=$HNAME
_R_PWD_1=`$CAT ${MIGRATION_CONF} | $GREP ^ROOT_PASSWORD_${_host_}= | $NAWK -F"ROOT_PASSWORD_\${_host_}=" '{print $2}'`
if [ ! "${_R_PWD_1}" ]; then
    _err_msg_="Could not read value of ROOT_PASSWORD from ${MIGRATION_CONF}"
    abort_script "${_err_msg_}"
fi
_R_PWD_2=`$ECHO ${_R_PWD_1} | $OPENSSL enc -base64 -d`
if [ ! "${_R_PWD_2}" ]; then
    err_msg_="Could not decrypt password ROOT_PASSWORD"
    abort_script "${_err_msg_}"
fi
_R_PWD_=$(/usr/bin/perl -e 'print quotemeta shift(@ARGV)' "${_R_PWD_2}")
if [ $? -ne 0 ]; then
    _err_msg_="Could not run perl command on ROOT_PASSWORD"
    abort_script "${_err_msg_}"
fi
expect <<EOF >>/dev/null 2>&1
set timeout 60
spawn su root -c passwd
expect {
"*Password:" {send -- "${_R_PWD_}\r";exp_continue}
"Re-enter new Password*" {send -- "${_R_PWD_}\r"}
timeout {send user "\nTIMEOUT\n"; exit 9}
}
expect eof
EOF
log_msg -s "\nSuccessfully changed the password for root user" -l "${LOGFILE}"
#Please do not add any spaces before EOF

## Changing password for dcuser.
_DC_PWD_1=`$CAT ${MIGRATION_CONF} | $GREP ^DC_PASSWORD_${_host_}= | $NAWK -F"DC_PASSWORD_\${_host_}=" '{print $2}'`
if [ ! "${_DC_PWD_1}" ]; then
    _err_msg_="Could not read value of DC_PASSWORD from ${MIGRATION_CONF}"
    abort_script "${_err_msg_}"
fi
_DC_PWD_2=`$ECHO ${_DC_PWD_1} | $OPENSSL enc -base64 -d`
if [ ! "${_R_PWD_2}" ]; then
    err_msg_="Could not decrypt password DC_PASSWORD"
    abort_script "${_err_msg_}"
fi
_DC_PWD_=$(/usr/bin/perl -e 'print quotemeta shift(@ARGV)' "${_DC_PWD_2}")
if [ $? -ne 0 ]; then
    _err_msg_="Could not run perl command on DC_PASSWORD"
    abort_script "${_err_msg_}"
fi
expect <<EOF >>/dev/null 2>&1
set timeout 60
spawn su root -c "passwd dcuser"
expect {
"*Password:" {send -- "${_DC_PWD_}\r";exp_continue}
"Re-enter new Password*" {send -- "$_DC_PWD_{}\r"}
timeout {send user "\nTIMEOUT\n"; exit 9}
}
expect eof
EOF

log_msg -s "\nSuccessfully changed the password for dcuser" -l "${LOGFILE}"
#Please do not add any spaces before EOF


insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_zfs_filesystem ###
#
# Creates all required ZFS Filesystems
#
# Arguments:
#   none
# Return Values:
#   none
create_zfs_filesystem()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_zfs_filesystem stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_zfs_filesystem -n
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Mounting newly created NAS FS
mount_nas_shares

# Restoring /eniq/database directory from NAS backup
cd ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/database

log_msg -q -s "Copying from ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/bkup_sw to ${ENIQ_BASE_DIR}/database" -l ${LOGFILE}
$FIND . -depth -print | $CPIO -pdmu ${ENIQ_BASE_DIR}/database >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not copy new SW from ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/database to ${ENIQ_BASE_DIR}/database"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: create_zfs_pool ###
#
# Destroys any existing ZFS pools and creates
# the new ones
#
# Arguments:
#   none
# Return Values:
#   none
create_zfs_pool()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling create_zfs_pool stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_zfs_pool ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: disable_oss_mounts ###
#
#   Creates disable_OSS for each oss mounts
#
# Arguments:
#   none
# Return Values:
#   none
disable_oss_mounts()
{
log_msg -s "Disabling OSS mounts." -l ${LOGFILE}
_oss_mount_dir_="${ENIQ_BASE_DIR}/connectd/mount_info"
_oss_list_=`$LS -1 ${_oss_mount_dir_} 2> /dev/null`

if [ "${_oss_list_}" ];then
    for _dir_ in ${_oss_list_}
    do
        log_msg -s "Creting disable_OSS file under ${_oss_mount_dir_}/${_dir_}" -l ${LOGFILE}
        $TOUCH ${_oss_mount_dir_}/${_dir_}/disable_OSS
    done
else
    log_msg -s "\nNo OSS mount present to disable on ${HNAME}" -l ${LOGFILE}
fi
}


### Function: disconnect_storage ###
#
#   Export zpools and disconnects the SAN
#
# Arguments:
#   none
# Return Values:
#   none
disconnect_storage()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip if premigration on CO is not selected in case of multiblade replacement
if [ "${CO_SERVER}" ]; then
    if [ ! -f ${MIGRATE_CO} ]; then
        insert_header_footer foot "Skipping replacement stage on ${CURR_SERVER_TYPE} - ${NEXT_STAGE}" ${LOGFILE}
        set_next_stage `$EXPR ${ARRAY_ELEM}+1`
        return 0
    fi
fi

# Do not execute if not migration backup
if [ "${ACTION_TYPE}" != "premigration" ];then
    log_msg -s "\nExport the zpools and disconnect the storage manually." -l ${LOGFILE}
    return 0
fi

# Checking if portbackup is mounted
mount_nas_shares

# Checking mandatory files in backup directory
check_mandatory_files

# Get storage data before exporting zpools
get_storage_info

# Export Zpools
export_zpools

#disconnect host from SAN
host_disconnect

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}


### Function: display_migration_info ###
#
#   Display the migration SW details

#
# Arguments:
#   none
# Return Values:
#   none
display_migration_info()
{
# Create display file
DISP_FILE=${TEM_DIR}/display_info
$RM -rf ${DISP_FILE}

# Write to display file
$ECHO "\n********************** IMPORTANT **********************" > ${DISP_FILE}
$ECHO "\n******** PLEASE SAVE THE FOLLOWING INFORMATION ********" >> ${DISP_FILE}
$ECHO "\n\nNOTE: Below details needs to be provided by user after Linux OS installation." >> ${DISP_FILE}

if [ "${STORAGE_TYPE}" == "raw" ]; then
    # Get migration SW backup info
    _mig_nas_ip_=`read_value MIG_BACKUP_NAS_IP ${MIGRATION_CONF}` || abort_script "${_mig_nas_ip_}"
    _mig_nas_dir_=`read_value MIG_BACKUP_NAS_DIR ${MIGRATION_CONF}` || abort_script "${_mig_nas_dir_}"
    _mig_scp_user_=`read_value MIG_SW_SCP_USER ${MIGRATION_CONF}` || abort_script "${_mig_scp_user_}"

    _deployment_=`$CAT ${ENIQ_CONF_DIR}${DEPLOYMENT}`
    if [ "${_deployment_}" != "ft" ]; then
        # Get server ip list in the deployment
        $CAT ${ORDER_FILE} | $NAWK -F':' '{print $1}' > ${SERVER_IP_LIST}
        for _ip_ in `cat ${SERVER_IP_LIST}`;
		do
            _hostname_=`$GETENT hosts ${_ip_} | $AWK -F' ' '{print $2}' | $CUT -d'.' -f1`
            _stor_interface_=`read_value STORAGE_INTERFACE_${_hostname_} ${MIGRATION_CONF}` || abort_script "${_stor_interface_}"
            _stor_ip_=`read_value STORAGE_IP_${_hostname_} ${MIGRATION_CONF}` || abort_script "${_stor_ip_}"
            _stor_netmask_=`read_value STORAGE_NETMASK_${_hostname_} ${MIGRATION_CONF}` || abort_script "${_stor_netmask_}"
            if [ ! ${_stor_interface_} -o ! ${_stor_ip_} -o ! ${_stor_netmask_} ]; then
                _err_msg_="Required Storage VLAN information not available"
                abort_script ${_err_msg_}
            else
                $ECHO "\nStorage VLAN Interface for ${_hostname_} \t\t: ${_stor_interface_}" >> ${DISP_FILE}
                $ECHO "Storage VLAN Interface IP for ${_hostname_} \t: ${_stor_ip_}" >> ${DISP_FILE}
                $ECHO "Storage VLAN Interface Netmask for ${_hostname_} \t: ${_stor_netmask_}" >> ${DISP_FILE}
            fi
        done
    fi 
    
    #Removing The temporary directorie(if created previously) that required for getting lunid
    $RM -rf ${VAR_TMP_DIR}/serverlist
    $RM -rf ${VAR_TMP_DIR}/server_ip.txt
    $RM -rf ${VAR_TMP_DIR}/tmp_disk_id
    $RM -rf ${VAR_TMP_DIR}/lun_mapping
    
    #Getting ZFS Poool list 
    _zfs_pool_list_=`iniget SunOS_ZFS_POOL -f ${ENIQ_CONF_DIR}/SunOS.ini`
    if [ ! "${_zfs_pool_list_}" ]; then
        _err_msg_="Could not get the list of blocks from ${ENIQ_CONF_DIR}/SunOS.ini"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
    
    #Getting Lun Map List
    _lun_map_list_=`iniget LUN_MAP_DETAILS -f ${ENIQ_CONF_DIR}/lun_map.ini`
    if [ ! "${_lun_map_list_}" ]; then
        _err_msg_="Could not get the list of blocks from ${ENIQ_CONF_DIR}/lun_map.ini"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

    if [ "${CURR_SERVER_TYPE}" == "stats_coordinator" ];then
        $PERL ${ENIQ_CORE_INST_DIR}/lib/get_ip_order.pl -f ${VAR_TMP_DIR}/serverlist
        $CAT ${VAR_TMP_DIR}/serverlist | nawk -F"::" '{print $1}' >> ${VAR_TMP_DIR}/server_ip.txt
        for _ip_address_ in `cat ${VAR_TMP_DIR}/server_ip.txt`;do
            $RM -rf ${VAR_TMP_DIR}/_zfs_pool_list_
            $RM -rf ${VAR_TMP_DIR}/_lun_map_list_
            #Getting ZFS Poool list 
            run_remote_cmd "${_ip_address_}" "source ${ENIQ_CORE_INST_DIR}/lib/common_functions.lib;iniget SunOS_ZFS_POOL -f ${ENIQ_CONF_DIR}/SunOS.ini" ${LOGFILE} ${DEFAULT_USER} disable_tty >> ${VAR_TMP_DIR}/_zfs_pool_list_
            if [ ! -f ${VAR_TMP_DIR}/_zfs_pool_list_ ]; then
                _err_msg_="Could not get the list of blocks from ${ENIQ_CONF_DIR}/SunOS.ini"
                abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
            fi
            #Getting Lun Map List
            run_remote_cmd "${_ip_address_}" "source ${ENIQ_CORE_INST_DIR}/lib/common_functions.lib;iniget LUN_MAP_DETAILS -f ${ENIQ_CONF_DIR}/lun_map.ini" ${LOGFILE} ${DEFAULT_USER} disable_tty >>${VAR_TMP_DIR}/_lun_map_list_
            if [ ! -f ${VAR_TMP_DIR}/_lun_map_list_ ]; then
                _err_msg_="Could not get the list of blocks from ${ENIQ_CONF_DIR}/lun_map.ini"
                abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
            fi
            for _pool_tag_ in `$CAT ${VAR_TMP_DIR}/_zfs_pool_list_`; do 
                _disk_id_=`run_remote_cmd "${_ip_address_}" "source ${ENIQ_CORE_INST_DIR}/lib/common_functions.lib;iniget "$_pool_tag_" -f ${ENIQ_CONF_DIR}/SunOS.ini -v disk_layout" ${LOGFILE} ${DEFAULT_USER} disable_tty`
                $ECHO "${_disk_id_}" >> ${VAR_TMP_DIR}/tmp_disk_id
            done
            # Getting the block from lun_map.ini 
            for _lun_tag_ in  `$CAT ${VAR_TMP_DIR}/_lun_map_list_`; do
                _disk_=`run_remote_cmd "${_ip_address_}" "source ${ENIQ_CORE_INST_DIR}/lib/common_functions.lib;iniget $_lun_tag_ -f ${ENIQ_CONF_DIR}/lun_map.ini -v DISK_ID" ${LOGFILE} ${DEFAULT_USER} disable_tty`
                _lun_id_=`run_remote_cmd "${_ip_address_}" "source ${ENIQ_CORE_INST_DIR}/lib/common_functions.lib;iniget ${_lun_tag_} -f ${ENIQ_CONF_DIR}/lun_map.ini -v LUN_ID" ${LOGFILE} ${DEFAULT_USER} disable_tty`
                $ECHO "$_disk_::$_lun_id_" >> ${VAR_TMP_DIR}/lun_mapping
            done
        done
    else
        #Getting the block from  Sunos.ini 
         for _pool_tag_ in ${_zfs_pool_list_}; do
            _disk_id_=`iniget $_pool_tag_ -f ${ENIQ_CONF_DIR}/SunOS.ini -v disk_layout`
            $ECHO "${_disk_id_}" >> ${VAR_TMP_DIR}/tmp_disk_id
        done

        # Getting the block from lun_map.ini 
        for _lun_tag_ in ${_lun_map_list_}; do
            _disk_=`iniget ${_lun_tag_} -f ${ENIQ_CONF_DIR}/lun_map.ini -v DISK_ID`
            _lun_id_=`iniget ${_lun_tag_} -f ${ENIQ_CONF_DIR}/lun_map.ini -v LUN_ID`
            $ECHO "$_disk_::$_lun_id_" >> ${VAR_TMP_DIR}/lun_mapping
        done
    fi
    
    declare -a _zpool_lun_id_
    counter=0
    for _disk_id_tmp_ in `$CAT ${VAR_TMP_DIR}/tmp_disk_id`;do
        _lunid_info_=`$CAT ${VAR_TMP_DIR}/lun_mapping | $GREP ${_disk_id_tmp_}`
        _zpool_lun_id_[counter]=`$ECHO ${_lunid_info_} | $NAWK -F "@" '{print $2}'`
        counter=`expr $counter + 1`
    done
    
    $ECHO "\nMigration backup server IP address : ${_mig_nas_ip_}" >> ${DISP_FILE}
    $ECHO "\nDirectory path of Migration backup : ${_mig_nas_dir_}" >> ${DISP_FILE}
    $ECHO "\nZpool LUN ID for Servers" >> ${DISP_FILE}
    $ECHO "\n---------------------------------" >> ${DISP_FILE}
    if [ "${CURR_SERVER_TYPE}" == "stats_coordinator" ];then
        $ECHO "\n LUN IDs Of Co-Ordinator " >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_sp_1        : ${_zpool_lun_id_[0]}" >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_stats_pool  : ${_zpool_lun_id_[1]}" >> ${DISP_FILE}
        $ECHO "\n---------------------------------" >> ${DISP_FILE}
        $ECHO "\n LUN IDs Of ENGINE " >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_sp_1        : ${_zpool_lun_id_[2]}" >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_stats_pool  : ${_zpool_lun_id_[3]}" >> ${DISP_FILE}
        $ECHO "\n---------------------------------" >> ${DISP_FILE}
        $ECHO "\n LUN IDs Of Reader1 " >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_sp_1        : ${_zpool_lun_id_[4]}" >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_stats_pool  : ${_zpool_lun_id_[5]}" >> ${DISP_FILE}
        $ECHO "\n---------------------------------" >> ${DISP_FILE}
        $ECHO "\n LUN IDs Of Reader2  " >> ${DISP_FILE}
        $ECHO "\n---------------------------------" >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_sp_1        : ${_zpool_lun_id_[6]}" >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_stats_pool  : ${_zpool_lun_id_[7]}" >> ${DISP_FILE}
    else
        $ECHO "\n LUN IDs Of ${HNAME} " >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_sp_1 on ${HNAME}       : ${_zpool_lun_id_[0]}" >> ${DISP_FILE}
        $ECHO "\nLUN ID of eniq_stats_pool on ${HNAME} : ${_zpool_lun_id_[1]}" >> ${DISP_FILE}
    fi
    $ECHO "\n\nNote: Above mentioned LUNs needs to be removed from corresponding Storage Group and recreated into a single LUN with bigger size within same Storage Group ." >> ${DISP_FILE}

    #cleaning temporary files
    $RM -rf ${SERVER_IP_LIST}
else
    $RM -rf ${TEM_DIR}/pool_map >> /dev/null 2>&1

    # Get zpool ID
    _count_=1
    for _entry_ in `$CAT ${MIGRATION_CONF} | $GREP "^ZPOOL_TO_IMPORT_"`; do
        _pool_info_=`read_value ZPOOL_TO_IMPORT_${_count_} ${MIGRATION_CONF}` || abort_script "${_pool_info_}"
        _zpool_name_=`$ECHO ${_pool_info_} | $CUT -f1 -d'@'`
        _zpool_id_=`$ECHO ${_pool_info_} | $CUT -f2 -d'@'`
   
        # Update pool data to the display file
        $ECHO "Zpool Name: ${_zpool_name_}  Zpool ID: ${_zpool_id_}" >> ${TEM_DIR}/pool_map
        _count_=`$EXPR $_count_ + 1`
    done

    $ECHO "\nZpool ID mapping" >> ${DISP_FILE}
    $ECHO "-----------------\n" >> ${DISP_FILE}
    $CAT ${TEM_DIR}/pool_map >> ${DISP_FILE}
fi

$CLEAR
# Display the information
$ECHO "\n\n"
$CAT ${DISP_FILE}
$ECHO "\n\n Press enter key to continue."
read
}

### Function: enable_oss_mounts ###
#
#   Creates enable_OSS for each oss mounts
#
# Arguments:
#   none
# Return Values:
#   none
enable_oss_mounts()
{
log_msg -s "Enabling OSS mounts." -l ${LOGFILE}
_oss_mount_dir_="${ENIQ_BASE_DIR}/connectd/mount_info"
_oss_list_=`$LS -1 ${_oss_mount_dir_} 2> /dev/null`

if [ "${_oss_list_}" ];then
    for _dir_ in ${_oss_list_}
    do
        log_msg -s "Enabling OSS mounts." -l ${LOGFILE}
        $RM -f ${_oss_mount_dir_}/${_dir_}/disable_OSS
    done
else
    log_msg -s "\nNo OSS mount present to enable on ${HNAME}" -l ${LOGFILE}
fi
}

### Function: enm_stor_vlan ###
#
#   save new storage Vlan info if configured.
#
# Arguments:
#   none
# Return Values:
#   none
enm_stor_vlan()
{
_diff_subnet_ENM_=`iniget IPMP_INTF_4 -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v IPMP_Group_IP | wc -l`
_SFS_to_VA_=`iniget SFS_TO_VA_MIGRATION_INDICATOR -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v SFS_to_VA_Migrated | wc -l`
if [ ${_diff_subnet_ENM_} -eq 0 ]; then
    $ECHO _diff_subnet_ENM_=NO >> ${_temp_migr_conf_}
    else
    $ECHO _diff_subnet_ENM_=YES >> ${_temp_migr_conf_}
fi
if [ ${_SFS_to_VA_} -eq 0 ]; then
     $ECHO _SFS_to_VA_=NO >> ${_temp_migr_conf_}
else
     $ECHO _SFS_to_VA_=YES >> ${_temp_migr_conf_}
fi
log_msg -s "\nsuccessfully saved the new storage Vlan info." -l ${LOGFILE}
}

### Function: export_zpools ###
#
#   Exports zpools during premigration activity
#
# Arguments:
#   none
# Return Values:
#   none
export_zpools()
{
log_msg -s "Exporting zpool(s) during ${ACTION_TYPE}" -l ${LOGFILE}
cd /

# Check zpools are healthy before export
$ZPOOL status -x >> /dev/null 2>&1
if [ $? -ne 0 ];then
    _err_msg_="ZPOOLs are not in proper state. Please check."
    abort_script "${_err_msg_}"
fi
log_msg -s "Zpools are online." -l ${LOGFILE}

if [ ! -d ${MIGRATION_LOGDIR} ];then
    $MKDIR -p ${MIGRATION_LOGDIR}
fi

# Create logfile for the current activity (Premigration)
if [ "${ACTION_TYPE}" == "premigration" ];then
    CURR_ACTION_LOGFILE=${MIGRATION_LOGDIR}/eniq_linux_${ACTION_TYPE}.log
    $RM -rf ${CURR_ACTION_LOGFILE}
    $CAT ${LOGFILE} > ${CURR_ACTION_LOGFILE}
    LOGFILE=${CURR_ACTION_LOGFILE}
fi

if [ ! "${NO_CONFIRM}" ];then
    $ECHO "\n\nWARNING: You are about to export the zpools from system"
    user_confirm
    $ECHO "\nYou have selected $_response_"
    if [ "${_response_}" != "YES" ];then
        log_msg -s "Exiting from script as user selected NOT to proceed." -l ${LOGFILE}
        $ECHO "Please export zpools manually."
        return 0
    fi
fi

_zpool_list_=`$ZPOOL list -H -o name | $EGREP -v "${ROOT_POOL}"`
log_msg -s "\nExporting following Zpools:" -l ${LOGFILE}
log_msg -s "${_zpool_list_}" -l ${LOGFILE}

# Remove pool ID info file if exists
_pool_map_disp_=${VAR_DIR}/tmp/zpool_mapping
$TOUCH ${_pool_map_disp_} >> /dev/null 2>&1

for _pool_ in $_zpool_list_
do
    # Get zpool ID for zfs storage
    if [ "${STORAGE_TYPE}" == "zfs" ]; then
        _pool_id_=`$ZPOOL get guid ${_pool_} | $EGREP -v '^NAME[        ].*' | $AWK '{print $3}'`

        # Update pool data to the display file
        $CAT ${_pool_map_disp_} | $GREP "Zpool Name: ${_pool_} " >> /dev/null
        if [ $? -ne 0 ]; then
            $ECHO "Zpool Name: ${_pool_}  Zpool ID: ${_pool_id_}" >> ${_pool_map_disp_}
        fi
    fi

    # Check and delete zpool SWAP files
    $SWAP -l | $GREP ${_pool_} >> /dev/null 2>&1
    if [ $? -eq 0 ];then
        _swap_file_=`$SWAP -l | $GREP ${_pool_} | $AWK '{print $1}'`
        log_msg -s "Deleting $_swap_file_ from ${_pool_} pool." -l ${LOGFILE}
        $SWAP -d ${_swap_file_} >> /dev/null 2>&1
    fi

    $ZPOOL export -f $_pool_ >> /dev/null 2>&1
    $ZPOOL list -H -o name | $GREP $_pool_ >> /dev/null
    if [ $? -eq 0 ];then
        _err_msg_="Pool $_pool_ not exported. Export manually."
        abort_script "$_err_msg_"
    fi
    log_msg -s "\nExported Zpool: ${_pool_}" -l ${LOGFILE}
done

log_msg -s "\nAll zpools have been exported." -l ${LOGFILE}

}

### Function: generate_keys ###
#
# Generate ssh keys for root
#
# Arguments:
#    none
# Return Values:
#    none
generate_keys()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Remove CO root ssh keys if they exist
if [ "${CO_SERVER}" ]; then
    $CP -rf ${SSH_DIR} ${SSH_DIR}_migbkup >> /dev/null 2>&1
    log_msg -s "\nRemoving root .ssh keys." -l ${LOGFILE}
    $RM -rf ${SSH_DIR} >> /dev/null 2>&1
fi

# Check if hostsync exists
$SVCS -H ${HOSTSYNC_SMF_ID} >> /dev/null 2>&1
if [ $? -eq 0 ];then
    log_msg -s "\n${HOSTSYNC_SMF_ID} exists. Disabling it." -l ${LOGFILE}
    check_and_manage_smf ${HOSTSYNC_SMF_ID} disable
fi

# Calling generate_keys stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -n -s generate_keys -u root -l ${LOGFILE}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    
    # Restore back the user ssh directories, if failed
    $MV ${SSH_DIR}_migbkup ${SSH_DIR} >/dev/null 2>&1
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Start hostsync
log_msg -s "\nEnabling ${HOSTSYNC_SMF_ID} SMF." -l ${LOGFILE}
check_and_manage_smf ${HOSTSYNC_SMF_ID} enable

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: get_array_element ###
#
# Get the current array element number
#
# Arguments:
#   none
# Return Values:
#   none
get_array_element()
{
_num_elements_=${#ENIQ_CORE_STAGES[*]}
_array_length_=`${EXPR} ${_num_elements_} - 1`

for (( _elem_=0; _elem_<=${_array_length_}; _elem_++ )); do
    $ECHO ${ENIQ_CORE_STAGES[${_elem_}]} | $GREP -w ${NEXT_STAGE} >> /dev/null 2>&1
    if [ $? -eq 0 ]; then
        ARRAY_ELEM=${_elem_}
        break
    fi
done
}

### Function: get_backup_vlan_data ###
#
# get detail for backup vlan configuration
#
# Arguments:
#   none 
# Return Values:
#   none
get_backup_vlan_data()
{
_disp_file_=${TEM_DIR}/disp_file
$RM -rf ${_disp_file_}

if [ ! "$1" ];then
    _err_msg_="File name is required"
    abort_script "${_err_msg_}"
fi
_temp_backup_vlan_file_=$1

unset _backup_vlan_exist_ _backup_interface_name_ _backup_interface_ip_ _backup_interface_netmask_ _backup_interface_gateway_

# Return if not CO blade
if [ ! "${CO_SERVER}" ]; then
    log_msg -s "\nGetting Backup VLAN information for $HNAME ..." -l ${LOGFILE}
    _host_=$HNAME
    # Check if CO migration conf is created
    if [ -s ${MIGR_CO_CONF} ]; then
        # Check if BKUP_VLAN_EXIST parameter is there
        $CAT ${MIGR_CO_CONF} | $GREP ^BKUP_VLAN_EXIST_${_host_}= >> /dev/null 2>&1
        if [ $? -eq 0 ]; then
            _backup_vlan_exist_=`read_value BKUP_VLAN_EXIST_${_host_} ${MIGR_CO_CONF}` || abort_script "${_backup_vlan_exist_}"
            _backup_interface_name_=`read_value BKUP_VLAN_INTF_NAME_${_host_} ${MIGR_CO_CONF}` || abort_script "${_backup_interface_name_}"
            _backup_interface_ip_=`read_value BKUP_VLAN_INTF_IP_${_host_} ${MIGR_CO_CONF}` || abort_script "${_backup_interface_ip_}"
            _backup_interface_netmask_=`read_value BKUP_VLAN_INTF_NETMASK_${_host_} ${MIGR_CO_CONF}` || abort_script "${_backup_interface_netmask_}"
            _backup_interface_gateway_=`read_value BKUP_VLAN_INTF_GATEWAY_${_host_} ${MIGR_CO_CONF}` || abort_script "${_backup_interface_gateway_}"
       fi

       # Check if backup VLAN info need to be saved in conf file
       if [ "${_backup_vlan_exist_}" -a "${_backup_interface_name_}" -a "${_backup_interface_ip_}" -a "${_backup_interface_netmask_}" -a "${_backup_interface_gateway_}" ]; then
           # Set value in migration conf
           set_conf_value BKUP_VLAN_EXIST_${_host_} ${_backup_vlan_exist_} ${_temp_backup_vlan_file_}
           set_conf_value BKUP_VLAN_INTF_NAME_${_host_} ${_backup_interface_name_} ${_temp_backup_vlan_file_}
           set_conf_value BKUP_VLAN_INTF_IP_${_host_} ${_backup_interface_ip_} ${_temp_backup_vlan_file_}
           set_conf_value BKUP_VLAN_INTF_NETMASK_${_host_} ${_backup_interface_netmask_} ${_temp_backup_vlan_file_}
           set_conf_value BKUP_VLAN_INTF_GATEWAY_${_host_} ${_backup_interface_gateway_} ${_temp_backup_vlan_file_}
           log_msg -s "\nSuccessfully saved Backup VLAN information on ${_host_}." -l ${LOGFILE}
       fi
    fi
else
    # Creating available interface list to vaidate user input interface name
    if [ ! "${SOLARIS_10}" ];then
        $DLADM show-link -p -o LINK | $SORT -u > ${TEM_DIR}/interface_list
    else
        $DLADM show-dev | $AWK '{print $1}' > ${TEM_DIR}/interface_list
    fi
    if [ ! -s ${TEM_DIR}/interface_list ];then
        _err_msg_="Could not fetch available interfaces."
        abort_script "${_err_msg_}"
    fi

    # Loop through to get backup VLAN info for each blade
    for _line_ in `$CAT ${ORDER_FILE}`; do
        _host_=`$ECHO ${_line_} | $NAWK -F'::' '{print $2}'`
        _srv_name_=`$ECHO ${_line_} | $NAWK -F'::' '{print $3}'`
        while true
        do
            $CLEAR
            log_msg -s "\nGetting Backup VLAN information for ${_host_} [${_srv_name_}]..." -l ${LOGFILE}

            # Ask if Backup VLAN is configured
            while [ 1 ]
            do
                ask_for_input "If Backup VLAN is configured on ${_host_} (Yy/Nn) : \n"
                _backup_vlan_exist_=${USER_VALUE}
                case ${_backup_vlan_exist_} in
                    y|Y) _response_="YES"
                    break
                    ;;
                    n|N) _response_="NO"
                    break
                    ;;
                    *) $ECHO "Invalid input. Enter again."
                    ;;
                esac
            done

            # Check response from user 
            if [ "${_response_}" == "NO" ];then
                log_msg -s "\nUser selected Backup VLAN as NOT configured." -l ${LOGFILE}  
                break
            else
                log_msg -q -s "\nUser selected Backup VLAN as configured. Continuing." -l ${LOGFILE}

                # Ask for Backup VLAN interface name
                while true
                do
                    ask_for_input "Name of Backup VLAN interface on ${_host_}: \n"
                    $CAT ${TEM_DIR}/interface_list | $GREP -w ${USER_VALUE} >> /dev/null 2>&1
                    if [ $? -eq 0 ]; then
                        _backup_interface_name_=${USER_VALUE}
                        break
                    else
                        $ECHO "not valid interface name ${USER_VALUE}"
                        continue
                    fi
                done

                # Ask for Backup VLAN interface IP 
                while true
                do
                    ask_for_input "IP of Backup VLAN interface on ${_host_}: \n"
                    validate_ip ${USER_VALUE}
                    if [ $? -eq 0 ]; then
                        _backup_interface_ip_=${USER_VALUE}
                        break
                    fi
                done

                # Ask for Backup VLAN interface netmask IP
                while true
                do
                    ask_for_input "Netmask of Backup VLAN interface on ${_host_}: \n"
                    validate_ip ${USER_VALUE}
                    if [ $? -eq 0 ]; then
                        _backup_interface_netmask_=${USER_VALUE}
                        break
                    fi
                done

                # Ask for Backup VLAN interface gateway IP
                while true
                do
                    ask_for_input "Gateway of Backup VLAN interface on ${_host_}: \n"
                    validate_ip ${USER_VALUE}
                    if [ $? -eq 0 ]; then
                        _backup_interface_gateway_=${USER_VALUE}
                        break
                    fi
                done

                $ECHO "\nDisplaying Backup VLAN information for ${_host_}" > ${_disp_file_}
                $ECHO "------------------------------------------------\n" >> ${_disp_file_}
                $ECHO "Backup VLAN interface exists on ${_host_}\t : ${_backup_vlan_exist_}" >> ${_disp_file_}
                $ECHO "Backup VLAN interface name to be configured on ${_host_}\t : ${_backup_interface_name_}" >> ${_disp_file_}
                $ECHO "Backup VLAN interface IP to be configured on ${_host_}\t : ${_backup_interface_ip_}" >> ${_disp_file_}
                $ECHO "Backup VLAN Netmask to be configured on ${_host_}\t : ${_backup_interface_netmask_}" >> ${_disp_file_}
                $ECHO "Backup VLAN Gateway to be configured on ${_host_}\t : ${_backup_interface_gateway_}" >> ${_disp_file_}

                # Displaying data to user
                $CLEAR
                $CAT ${_disp_file_}
                user_confirm
                if [ "${_response_}" != "YES" ];then
                    log_msg -s "\nBackup VLAN details will be asked again as user selected NOT to proceed." -l ${LOGFILE}
                    continue
                fi

                set_conf_value BKUP_VLAN_EXIST_${_host_} ${_backup_vlan_exist_} ${_temp_backup_vlan_file_}
                set_conf_value BKUP_VLAN_INTF_NAME_${_host_} ${_backup_interface_name_} ${_temp_backup_vlan_file_}
                set_conf_value BKUP_VLAN_INTF_IP_${_host_} ${_backup_interface_ip_} ${_temp_backup_vlan_file_}
                set_conf_value BKUP_VLAN_INTF_NETMASK_${_host_} ${_backup_interface_netmask_} ${_temp_backup_vlan_file_}
                set_conf_value BKUP_VLAN_INTF_GATEWAY_${_host_} ${_backup_interface_gateway_} ${_temp_backup_vlan_file_}
                log_msg -s "\nSuccessfully taken Backup VLAN information for ${_host_} [${_srv_name_}]" -l ${LOGFILE}
                break
            fi
        done
    done
fi

# Removing temporary file
$RM -rf ${_disp_file_} >> /dev/null 2>&1

log_msg -s "\nSuccessfully completed step to get Backup VLAN information." -l ${LOGFILE}

}

### Function: get_migration_data ###
#
# Get the required migration data from user
# and create migration.conf during premigration
#
# Arguments:
#   none
# Return Values:
#   none
get_migration_data()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

_temp_migr_conf_=${TEM_DIR}/migration.conf.$$.$$
$RM -rf ${_temp_migr_conf_}
$TOUCH ${_temp_migr_conf_}
while [ 1 ];do
    if [ ! -s ${MIGRATION_CONF} ];then
        # Save hostname
        set_conf_value "SERVER_HOSTNAME" ${HNAME} ${_temp_migr_conf_}

        # Set migration type
        set_conf_value "MIGRATION_ACTION" "migration" ${_temp_migr_conf_}
        set_conf_value "REPLACEMENT" "${REPLACEMENT}" ${_temp_migr_conf_}
        
        # Get values from user
        save_sw_loc_path 

        # Save current base_sw and om_sw location
        # in case we have to recover the server
        save_recovery_info 

        # Save Storage related information
        if [ "${STORAGE_TYPE}" == "raw" ];then
            # Save Storage details in case of raw
            save_san_data ${_temp_migr_conf_}
            # Save Migration SW scp information
            save_mig_scp_info ${_temp_migr_conf_}
            if [ ! "${BACKUP}" ]; then
                # Save Storage Vlan information
                save_storage_vlan_info ${_temp_migr_conf_}
            fi
        else
            set_conf_value "SAN_DEVICE_TYPE" "zfs" ${_temp_migr_conf_}
        fi
        # get new storage value
         enm_stor_vlan
        # Get Backup vlan information
        if [ ! "${BACKUP}" ]; then
            get_backup_vlan_data ${_temp_migr_conf_}
        fi
        # Get MWS IP, OMBS IP, NAS Console IP from user
        save_ip_data
        # Get feature information from user
        save_feature_info
        # Get password information from user
        save_password_info

        # Save zpool IDs
        _num_=1
        for _pool_name_ in `$ZPOOL list -H -o name | $GREP -vw "rpool"`; do
            # Get id of the pool
            _pool_id_=`$ZPOOL get guid ${_pool_name_} | $EGREP -v '^NAME[        ].*' | $AWK '{print $3}' 2>>/dev/null`

            # Update pool data in conf file
            set_conf_value "ZPOOL_TO_IMPORT_${_num_}" "${_pool_name_}@${_pool_id_}" ${_temp_migr_conf_}
            _num_=`$EXPR $_num_ + 1`
        done
  
        if [ ! -s ${_temp_migr_conf_} ];then
            _err_msg_="Configuration file ${_temp_migr_conf_} is empty."
            abort_script "${_err_msg_}"
        fi

        # Save the migr.conf
        $CP ${_temp_migr_conf_} ${MIGRATION_CONF}
        if [ $? -ne 0 ];then
            _err_msg_="Unable to update ${MIGRATION_CONF}"
            abort_script "${_err_msg_}"
        fi
        $RM -rf ${_temp_migr_conf_}
        break
    else
        # Determine whether conf file needs to be removed
        _conf_replace_val_=`read_value REPLACEMENT ${MIGRATION_CONF}` || abort_script ${_conf_replace_val_}
        $GREP STORAGE_PASS_ ${MIGRATION_CONF} >> /dev/null 2>&1
        if [[ $? -ne 0 ]] || [[ "${_conf_replace_val_}" != "${REPLACEMENT}" ]]; then 
            # If REPLACEMENT value doesn't match, re-create file
            $RM -rf ${MIGRATION_CONF}
            continue
        fi

        log_msg -s "\nMigration config file already created.\n" -l ${LOGFILE}
        if [ "${CO_SERVER}" ]; then
            _disp_exclued_="HOSTNAME|STORAGE_GROUP_|MIG_BACKUP_|MIG_SW_|ZPOOL_"
            $ECHO "\nMigration Config Details" 
            $ECHO "========================\n"
            $CAT ${MIGRATION_CONF} | $EGREP -v ${_disp_exclued_}
            $ECHO "\nPlease confirm the details.\n"
            user_confirm
            if [ "${_response_}" != "YES" ]; then
                $RM -rf ${MIGRATION_CONF}
                continue
            fi
        else
            log_msg -s "\nRemoving Migration config file.\n" -l ${LOGFILE}
            $RM -rf ${MIGRATION_CONF}
            continue
        fi
        break
    fi
done

log_msg -q -s "\n## USER PROVIDED MIGRATION VALUES ##\n" -l "${LOGFILE}"
$CAT ${MIGRATION_CONF} >> "${LOGFILE}"

if [ ! "${BACKUP}" ]; then
    # Display required details for migration
    if [ "${CO_SERVER}" ]; then
        display_migration_info
    fi
fi

# Copy Migration SW to nas location
if [ "${STORAGE_TYPE}" == "raw" ]; then
    copy_migr_conf_to_nas
    if [ $? -ne 0 ]; then
        _err_msg_="Error occured during backing up migration config files to ${MIG_SW_BACKUP_DIR}."
        abort_script "${_err_msg_}"
    fi
log_msg -s "\nSuccessfully backed up Migration SW scripts to ${MIG_SW_BACKUP_DIR}." -l ${LOGFILE}
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: get_next_stage ###
#
# Get the stage to be run
#
# Arguments:
#   $1 : Stage to be set to. Either numeric value or 'done'
# Return Values:
#   none
get_next_stage()
{
ARRAY_ELEM=0

if [ -s $STAGEFILE ]; then

    NEXT_STAGE=`$CAT $STAGEFILE | $GEGREP -v '^[[:blank:]]*#' | $SED -e 's| ||g'`

    if [ ! "${NEXT_STAGE}" ]; then
        _err_msg_="Failed to read stage from ${STAGEFILE}, exiting."
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

    if [ "${NEXT_STAGE}" == "${STOP_STAGE}" ]; then
        return 0
    else
        $ECHO ${ENIQ_CORE_STAGES[*]} | $GREP -w ${NEXT_STAGE} >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            _err_msg_="Specified stage ${NEXT_STAGE} is not a valid stage"
            abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
        fi
    fi

    # Get the element number so we can move along the array
    get_array_element
else
    $MKDIR -p `$DIRNAME $STAGEFILE`
    if [ $? -ne 0 ]; then
        _err_msg_="Failed to create directory `$DIRNAME ${STAGEFILE}`, exiting."
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

    NEXT_STAGE=${ENIQ_CORE_STAGES[${ARRAY_ELEM}]}
fi
}

### Function: get_new_lun_id ###
#
# Get the merged/new LUN ID from user
#
# Arguments:
#     None
# Return values:
#     None
get_new_lun_id()
{
# Take the LUN ID of the newly created LUN
while :; do
    clear
    _disp_file_=${TEM_DIR}/disp_file
    $RM -rf ${_disp_file_} >> /dev/null 2>&1
    print_heading "New LUN IDs Information" > ${_disp_file_}

    _new_luns_="eniq_sp_1 eniq_stats_pool"

    for _lun_id_ in ${_new_luns_} ; do
        # Get newly created LUN IDs
        while :; do
            $ECHO "\nEnter LUN ID of the newly created LUN for ${_lun_id_} : \c"
            read _new_lun_id_
            if [ -z ${_new_lun_id_} ]; then
                continue
            fi

            $ECHO ${_new_lun_id_} | $EGREP "^[0-9]+$" >> /dev/null 2>&1
            if [ $? -ne 0 ]; then
                $ECHO "EXPECTED NUMERIC VALUE\n"
                continue
            fi

            $ECHO "ID of the newly created LUN for ${_lun_id_}: ${_new_lun_id_}" >> ${_disp_file_}
            break
        done
    done

    # Display values to user
    $CAT ${_disp_file_}
    user_confirm

    if [ "${_response_}" != "YES" ];then
        continue
    fi

    break
done

# Save the user input values
for _pool_ in $_new_luns_ ; do
    if [ ${_pool_} == eniq_sp_1 ]; then
        _pool_param_name_='ENIQ_SP_LUN_ID'
        _pool_id_=`$CAT ${_disp_file_} |$GREP ${_pool_} |$AWK -F":" '{print $2}'`
        set_conf_value "${_pool_param_name_}" "${_pool_id_}" ${MIGRATION_CONF}
    else
        _pool_param_name_='ENIQ_STATS_LUN_ID'
        _pool_id_=`$CAT ${_disp_file_} |$GREP ${_pool_} |$AWK -F":" '{print $2}'`
        set_conf_value "${_pool_param_name_}" "${_pool_id_}" ${MIGRATION_CONF}
    fi
done
}

### Function: get_replacement_server_list ###
#
#   Create the list of servers to be replaced 
#   to run premigration from CO
#
# Arguments:
#   none
# Return Values:
#   none
get_replacement_server_list()
{
_selected_servers_=${VAR_DIR}/tmp/serverlist.premigration
_server_list_file_=${TEM_DIR}/available_servers
_menu_opt_=1

# Check if the replacement list was populated in previous run
if [ ! -s ${_selected_servers_} ]; then
    log_msg -s "\nStarting Premigration Wizard for Replacement.\n" -l ${LOGFILE}
    # Display available server list if replacement
    # Write to display file
    $CLEAR
    $ECHO "Available Servers:\n==================\n"
    while read serverinfo; do
        _srv_=`$ECHO ${serverinfo} | $AWK -F:: '{print $2}'`
        $ECHO "[${_menu_opt_}] ${_srv_}" >> ${_server_list_file_}
        _menu_opt_=$((_menu_opt_ + 1))
    done < ${ORDER_FILE}
    $CAT ${_server_list_file_}
    $ECHO "\n==================\n" 
    $ECHO "Select the server(s) to be replaced from the list above" 
    $ECHO "For multiple servers enter numbers in comma (,) separated format [e.g. 1,3]"

    # Take input from user
    while [ 1 ]; do
        _invalid_num_=0
        $RM -rf ${_selected_servers_} >>/dev/null
        ask_for_input "server number(s): \c"
        $ECHO ${USER_VALUE} | $EGREP "[0-9][\ ,]|[0-9]" >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            $ECHO "Enter numbers in comma (,) separated format only"
            continue
        fi

        _num_list_=`$ECHO ${USER_VALUE} | sed -e 's/ //g' -e s'/,/ /g'`
        for _num_ in ${_num_list_}; do
            if [ $_num_ -ge $_menu_opt_ 2>/dev/null  -o $_num_ -lt 1 2>/dev/null ]; then
                _invalid_num_=1
                break
            fi
            $CAT ${_server_list_file_} | $GREP "\[$_num_\]" | $AWK '{print $2}' >> ${_selected_servers_}
        done

        # Check if any invalid input is there
        if [ $_invalid_num_ -eq 1 ]; then
            $RM -rf ${_selected_servers_} >>/dev/null
            $ECHO "Enter numbers only"
            continue
        else
            # User selected proper servers numbers
            break
        fi
    done
    log_msg -s "User has selected the following server(s) for replacement: " -l ${LOGFILE}
    $CAT ${_selected_servers_}
fi

# Modify server order list file
$RM -rf ${ORDER_FILE}.tmp >> /dev/null 2>&1
while read _server_; do
    # Update server order file
    $CAT ${ORDER_FILE} | $GREP ${_server_} >> ${ORDER_FILE}.tmp
done < ${_selected_servers_}
if [  -s ${ORDER_FILE}.tmp ]; then
    $MV  ${ORDER_FILE}.tmp  ${ORDER_FILE}
fi
}


### Function: get_server_list ###
#
#   Create the server list to run premigration from CO
#
# Arguments:
#   none
# Return Values:
#   none
get_server_list()
{
if [ ! "${CO_SERVER}" ]; then
    log_msg -s "\nSkipping this step on non-Coordinator server. \n" -l ${LOGFILE}
    return 0
fi

# Create server order file 
IP_ORDER_SCRIPT=${MIGRATION_LIB}/get_ip_order.pl
ORDER_FILE=${TEM_DIR}/server_order_list
$RM -rf ${ORDER_FILE} >> /dev/null

# Get server order
if [ -f ${IP_ORDER_SCRIPT} ]; then
    # Check NASd is online if raw
    if [ "${STORAGE_TYPE}" == "raw" ]; then
        check_and_manage_smf ${NASd_SMF_ID} enable
    fi
    $PERL ${IP_ORDER_SCRIPT} -f ${ORDER_FILE}
else
    _err_msg_="${IP_ORDER_SCRIPT} file not found."
    abort_script "$_err_msg_"
fi

if [ ! -s ${ORDER_FILE} ]; then
    _err_msg_="Could not get server details from ${ORDER_FILE}."
    abort_script "${_err_msg_}"
fi

# Check how many entries are in server list file
if [ `$CAT ${ORDER_FILE} | $WC -l` -eq 1 ]; then
    if [ "${ACTION_TYPE}" == "premigration" ]; then
        log_msg -s "\nINFO: Premigration triggered on Standalone server." -l ${LOGFILE}
    fi
    if [ "${STORAGE_TYPE}" == "zfs" ]; then
        # Reset REPLACEMENT value to NO for rack
        REPLACEMENT="NO"
    fi

    # Set CO for premigration as standalone server will be treated as CO
    $RM -rf ${MIGRATE_CO} >> /dev/null
    $TOUCH ${MIGRATE_CO}
    if [ ! -f ${MIGRATE_CO} ]; then
        _err_msg_="Couldn't create the flag ${MIGRATE_CO} to enable ${ACTION_TYPE} on server."
        abort_script "${_err_msg_}"
    fi

    # No need to manipulate server order list for Standalone
    # Go back to stage list
    return 0
fi

# Reverse the server order because we are disabling services
$RM -rf ${ORDER_FILE}.tmp >> /dev/null
$CAT ${ORDER_FILE} | $TAIL -r > ${ORDER_FILE}.tmp
$MV ${ORDER_FILE}.tmp ${ORDER_FILE}

# Modify server order file for replacement
if [ "${REPLACEMENT}" == "YES" ]; then
    get_replacement_server_list
fi
}


### Function: get_storage_info ###
#
#   Copy Storage information to temporary file before zpool(s) are exported
#
# Arguments:
#   none 
# Return Values:
#   none
get_storage_info()
{
if [ "${STORAGE_TYPE}" == "raw" ]; then
    if [ ! -s ${ENIQ_CONF_DIR}/${SUNOS_INI} ]; then
        _err_msg_="Could not locate file ${ENIQ_CONF_DIR}/${SUNOS_INI}."
        abort_script "$_err_msg_"
    fi

    if [ ! -s ${ENIQ_CONF_DIR}/${BLK_STOR_INI} ]; then
        _err_msg_="Could not locate file ${ENIQ_CONF_DIR}/${BLK_STOR_INI}."
        abort_script "$_err_msg_"
    fi

    _file_list_="${ENIQ_CONF_DIR}/${SUNOS_INI}
    ${ENIQ_CONF_DIR}/${BLK_STOR_INI}
    ${MIGRATION_CONF}"

    for _file_ in ${_file_list_}; do
        if [ ! -s ${_file_} ]; then
            _err_msg_="Could not locate file ${_file_}."
            abort_script "$_err_msg_"
        fi
    done

    log_msg -s "\nCopying necessary storage and zpool information into temporary file..." -l ${LOGFILE}
    _temp_storage_info_="${TEM_DIR}/temp_storage_info"
    $RM -rf ${_temp_storage_info_}
    log_msg -q -s "INFO: Storage information will be stored in ${_temp_storage_info_}" -l ${LOGFILE}
    $TOUCH ${_temp_storage_info_}
    if [ $? -ne 0 ]; then
        _err_msg_="Could not create ${_temp_storage_info_}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

    # Get zpool ID
    count=1
    for _entry_ in `$CAT ${MIGRATION_CONF} | $GREP "^ZPOOL_TO_IMPORT_"`; do
        _pool_info_=`read_value ZPOOL_TO_IMPORT_$count ${MIGRATION_CONF}` || abort_script "${_pool_info_}"
        _zpool_name_=`$ECHO ${_pool_info_} | $CUT -f1 -d'@'`
        _zpool_id_=`$ECHO ${_pool_info_} | $CUT -f2 -d'@'`

        # Update pool data to the display file
        $ECHO "ZPOOL_NAME_$count=${_zpool_name_}" >> ${_temp_storage_info_}
        $ECHO "ZPOOL_ID_$count=${_zpool_id_}" >> ${_temp_storage_info_}
        count=`$EXPR $count + 1`
    done

    # Copy SAN device type (vnx/clariion) to temporary file
    _san_device_=`iniget SAN_DEV -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v SAN_DEVICE`
    if [ ! "${_san_device_}" ]; then
        _err_msg_="Could not read SAN_DEVICE value from ${SUNOS_INI}."
        abort_script "${_err_msg_}"
    fi
    $ECHO "SAN_DEVICE_TYPE=${_san_device_}" >> ${_temp_storage_info_} 
    log_msg -s "\nCopying SAN device type in ${_temp_storage_info_}" -l ${LOGFILE}

    # Determine number of storages
    log_msg -s "\nDetermining number of SAN connected" -l ${LOGFILE}
    _stor_count_=`iniget BLK_STORAGE_DEV_DETAILS -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} | $WC -l`
    if [ ${_stor_count_} -eq 0 ]; then
        _err_msg_="Unable to get connected SAN count from ${BLK_STOR_INI}."
        abort_script "${_err_msg_}"
    fi

    count=1
    while [ $count -le $_stor_count_ ]; do
        _spa_ip_=`iniget BLK_STORAGE_DEV_DETAILS_$count -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_IP_SPA`
        if [ ! "${_spa_ip_}" ]; then
            _err_msg_="Could not read BLK_STORAGE_IP_SPA value from ${BLK_STOR_INI}."
               abort_script "${_err_msg_}"
        fi

        _stor_grp_=`iniget BLK_STORAGE_DEV_DETAILS_$count -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_GROUP_NAME`
        if [ ! "${_stor_grp_}" ]; then
            _err_msg_="Could not read BLK_STORAGE_GROUP_NAME value from ${BLK_STOR_INI}."
            abort_script "${_err_msg_}"
        fi

        $ECHO "STORAGE_SPA_$count=${_spa_ip_}" >> ${_temp_storage_info_}
        log_msg -q -s "\nCopying STORAGE_SPA_$count in ${_temp_storage_info_}" -l ${LOGFILE}
        $ECHO "STORAGE_GROUP_$count=${_stor_grp_}" >> ${_temp_storage_info_}
        log_msg -q -s "Copying STORAGE_GROUP_$count in ${_temp_storage_info_}" -l ${LOGFILE}
        count=$((count+1))
    done
fi
}

### Function: host_disconnect ###
#
#   Disconnects the SAN
#
# Arguments:
#   none
# Return Values:
#   none
host_disconnect()
{
if [ "${STORAGE_TYPE}" == "raw" ]; then
    # Check temp_storage_data file exists with neccessary information
    log_msg -s "\nGetting required storage information." -l ${LOGFILE}
    if [ ! -s ${_temp_storage_info_} ]; then
        _err_msg_="Unable to get required storage information. ${_temp_storage_info_} file doesn't exists."
        abort_script "${_err_msg_}"
    fi
    _navisec_options_=""

    # Check storage device type (vnx/clariion) 
    _san_device_type_=`$CAT ${_temp_storage_info_} | $GREP "SAN_DEVICE_TYPE" | $CUT -d'=' -f2` 
    if [ ! ${_san_device_type_} ]; then
        _err_msg_="Unable to get SAN_DEVICE_TYPE information from ${_temp_storage_info_}"
        abort_script "${_err_msg_}"
    fi

    # Check storage count
    _str_cnt_=`$CAT ${_temp_storage_info_} | $EGREP "^STORAGE_SPA_[1-9]=" | $WC -l`
    if [ ! ${_str_cnt_} ]; then
        _err_msg_="Unable to get the count of connected SAN from ${_temp_storage_info_}"
        abort_script "${_err_msg_}"
    fi

    # Loop through the information and disconnect host from storage
    count=1
    while [ $count -le $_str_cnt_ ]; do
        # Get specific values from storage data file
        _str_spa_=`$CAT ${_temp_storage_info_} | $GREP "^STORAGE_SPA_$count=" | $CUT -d'=' -f2`
        if [ ! ${_str_spa_} ]; then
            _err_msg_="Unable to get the STORAGE_SPA_$count from ${_temp_storage_info_}"
            abort_script "${_err_msg_}"
        fi

        _str_grp_=`$CAT ${_temp_storage_info_} | $GREP "^STORAGE_GROUP_$count=" | $CUT -d'=' -f2`
        if [ ! ${_str_grp_} ]; then
            _err_msg_="Unable to get the STORAGE_GROUP_$count from ${_temp_storage_info_}"
            abort_script "${_err_msg_}"
        fi

        # Set options to pass to naviseccli command
        _navisec_options_="-h ${_str_spa_} -secfilepath ${ERICSSON_SAN_PLUGINS_DIR}/${_san_device_type_}/cred/ storagegroup -disconnecthost -host ${HNAME} -gname ${_str_grp_}"
        log_msg -s "Disconnecting Storage..." -l ${LOGFILE}
        log_msg -q -s "Executing the command:\n"  -l ${LOGFILE}
        # Run command to disconnect hosts
        $EXPECT <<EOF >>${LOGFILE} 2>&1
        set timeout 60
        spawn ${NAVISPHERE}/naviseccli ${_navisec_options_}
        expect {
        "Please input your selection" {send "2\r";exp_continue}
        "(y/n)" {send "y\r"}
        timeout {send_user "\nTIMEOUT!\n"; exit 9}
        }
        expect eof
EOF
#Please do not add any spaces before EOF 

        if [ $? -ne 0 ];then
            _err_msg_="Could not Disconnect Storage for $HNAME. Please disconnect the SAN manually."
            log_msg -s "Disconnecting Storage failed... Importing zpools again" -l ${LOGFILE} 

            # Import zpools
            import_zpools
            _err_msg_="Rectify the issue and re-run the script again."
            abort_script "$_err_msg_"
        fi
        count=`$EXPR $count + 1`
    done
    log_msg -s "\nSuccessfully disconnected SAN for $HNAME." -l ${LOGFILE}
fi
}

### Function: import_zpools ###
#
#  Importing zpools if disconnecting storage failed 
#
# Arguments:
#   none
# Return Values:
#   none
import_zpools()
{
if [ "${STORAGE_TYPE}" == "raw" ]; then
    # Get current zpool list from server
    _curr_pools_=`$ZPOOL list -H -o name | $GREP -v rpool`

    # Flag to check zpool import needed or not
    _import_needed_=0

    # Check zpool count from temporary file
    _zpool_cnt_=`$CAT ${_temp_storage_info_} | $EGREP "^ZPOOL_NAME_[1-9]=" | $WC -l`
     if [ ! ${_zpool_cnt_} ]; then
        _err_msg_="Unable to get the count of Zpools from ${_temp_storage_info_}"
        abort_script "${_err_msg_}"
    fi

    # Loop through the information and import zpools 
    count=1
    while [ $count -le $_zpool_cnt_ ]; do
        # Get zpool name from temporary file
        _zpool_name_=`$CAT ${_temp_storage_info_} | $GREP "^ZPOOL_NAME_$count=" | $CUT -d'=' -f2`
        if [ ! ${_zpool_name_} ]; then
            _err_msg_="Unable to get the ZPOOL_NAME_$count from ${_temp_storage_info_}"
            abort_script "${_err_msg_}"
        fi

        # Get zpool id from temporary file
        _zpool_id_=`$CAT ${_temp_storage_info_} | $GREP "^ZPOOL_ID_$count=" | $CUT -d'=' -f2`
        if [ ! ${_zpool_id_} ]; then
            _err_msg_="Unable to get the ZPOOL_ID_$count from ${_temp_storage_info_}"
            abort_script "${_err_msg_}"
        fi

        # If no zpools are available 
        if [ ! "${_curr_pools_}" ];then
            _import_needed_=1
        else
            # Check if exported zpools are already imported or not
            $ECHO ${_curr_pools_} | $GREP -w ${_zpool_name_} >> /dev/null 2>&1
            if [ $? -ne 0 ];then
                _import_needed_=1
            else
                _import_needed_=0
            fi
        fi

        # Import zpools 
        if [ ${_import_needed_} -eq 1 ];then
        _pools_list_=`$ZPOOL import | $GREP "pool:" | $AWK '{print $2}' 2>/dev/null`
            if [ ! "${_pools_list_}" ];then
                _err_msg_="Could not find any pool to import."
                abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
            else
                $ECHO "${_pools_list_}" | $GREP ${_zpool_name_} >> /dev/null 2>&1
                if [ $? -ne 0 ];then
                    _err_msg_="Required pool is not available to import."
                    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
                fi
            fi

            $ZPOOL import -f ${_zpool_id_}
            if [ $? -ne 0 ];then
                _err_msg_="Could not import zpool ${_zpool_name_}."
                abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
            fi
            log_msg -q -s "Zpool ${_pool_name_} imported successfully." -l ${LOGFILE}
        else
            log_msg -q -s "Zpool ${_pool_name_} already imported." -l ${LOGFILE}
        fi

        count=`$EXPR $count + 1`
    done
fi

}

### Function: install_backup_sw ###
#
# Installs scripts into /eniq/bkup_sw
#
# Arguments:
#   none
# Return Values:
#   none
install_backup_sw()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Mounting newly created NAS FS
mount_nas_shares

# Restoring /eniq/bkup_sw directory from NAS backup
cd ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/bkup_sw

log_msg -q -s "Copying from ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/bkup_sw to ${ENIQ_BASE_DIR}/bkup_sw" -l ${LOGFILE}
$FIND . -depth -print | $CPIO -pdmu ${ENIQ_BASE_DIR}/bkup_sw >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not copy new SW from ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/bkup_sw to ${ENIQ_BASE_DIR}/bkup_sw"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: install_connectd_sw ###
#
# Installs scripts into /eniq/connectd
#
# Arguments:
#   none
# Return Values:
#   none
install_connectd_sw()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Mounting newly created NAS FS
mount_nas_shares

# Restoring /eniq/connectd directory from backup
cd ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/connectd

log_msg -q -s "Copying from ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/connectd to ${ENIQ_BASE_DIR}/connectd" -l ${LOGFILE}
$FIND . -depth -print | $CPIO -pdmu ${ENIQ_BASE_DIR}/connectd >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not copy new SW from ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_BASE_DIR}/connectd to ${ENIQ_BASE_DIR}/connectd"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: install_extra_fs ###
#
# Installs extra file systems if required
# 
# Arguments:
#   none
# Return Values:
#   none
install_extra_fs()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling install_extra_fs stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s install_extra_fs ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Ensuring that the NAS filesystem with backup data is mounted
mount_nas_shares

#Restoring the configuration files from portbackup to /etc/vfstab file
$EGREP -i 'BIS|netanserver' ${ETC_DIR}/${VFSTAB} >> /dev/null 2>&1

if [ $? -ne 0 ]; then
    $EGREP -i 'BIS|netanserver' ${_mountpoint_}/${HNAME}/ROOT/etc/${VFSTAB} >> /dev/null 2>&1

    if [ $? -eq 0 ]; then
        log_msg -t -s "Restoring configuration files at ${ETC_DIR}/${VFSTAB}\n" -l ${LOGFILE}
        $EGREP -i 'BIS|netanserver' ${_mountpoint_}/${HNAME}/ROOT/etc/${VFSTAB} > ${TEM_DIR}/${VFSTAB}
        while read _check_mount_
        do
           _mount_point_=`$ECHO ${_check_mount_} | $NAWK '{print $3}'`
           $MKDIR -p ${_mount_point_} >> /dev/null 2>&1
        done < ${TEM_DIR}/${VFSTAB}

        if [ -s ${TEM_DIR}/${VFSTAB} ]; then
           $CAT ${TEM_DIR}/${VFSTAB} >> ${ETC_DIR}/${VFSTAB}
        fi

        while read _check_mount_
        do
           _dev_to_mount_=`$ECHO ${_check_mount_} | $NAWK '{print $1}'`
           _mount_point_=`$ECHO ${_check_mount_} | $NAWK '{print $3}'`
           $MOUNT -F nfs ${_dev_to_mount_} ${_mount_point_}
            if [ $? -ne 0 ]; then
                _err_msg_="Error while mounting entries in ${VFSTAB} file"
                abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
            fi
        done < ${TEM_DIR}/${VFSTAB}
        
        $SVCADM enable "svc:/network/nfs/client:default"
        
    fi
fi

#Restoring local_logs fro NAS backup
if [ ! -f ${VAR_TMP_DIR}/local_logs_sucess ]; then
    #calculating size of local_logs in porbackup excluding files which are already copied dueing continue_eniq_migration
    _size_local_ext4_=`$DF -k ${ENIQ_LOG_DIR} |$AWK 'NR==2{print $4}'`
    _size_used_local_logs_=`$DU -sk ${ENIQ_LOG_DIR} |$AWK '{print $1}'`
    _size_local_port_=`$DU -sk ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_LOG_DIR} |$AWK '{print $1}'`
    _size_local_logs_port_exclude_=`$EXPR ${_size_local_port_} - ${_size_used_local_logs_}`
    #Copying the selective local local_logs excluding the directorie that are already copied during continue_eniq_recovery
    if [ ${_size_local_logs_port_exclude_} -lt ${_size_local_ext4_} ]; then
        EXLUD_LOG_DIR="installation|migration|iq|esm|hostsync|rolling_snapshot_logs|connectd|NASd|snapshot_logs|eniq_services_log|replacement|sw_log"
        _log_dir_=`$LS ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_LOG_DIR} |$EGREP -v '${EXLUD_LOG_DIR}'`
            for _dir_ in `$ECHO ${_log_dir_}`; do
                if [ -d ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_LOG_DIR}/${_dir_} ] || [ -f ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_LOG_DIR}/${_dir_} ]; then
                    log_msg -s "\nCopying ${ENIQ_LOG_DIR}/${_dir_} from NAS backup" -l ${LOGFILE}
                    $CP -pr ${_mountpoint_}/${HNAME}/ZFS/${ENIQ_LOG_DIR}/${_dir_} ${ENIQ_LOG_DIR}
                    if [ $? -ne 0 ]; then
                        _err_msg_="unable to copy ${_dir_}"
                        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
                    fi
                fi
            done
    else
        log_msg -s "\nWARNING:Sufficient space is not available on the file system. Unable to copy some of the directories of local_logs from portbackup." -l ${LOGFILE}
    fi
fi

#Restoring /eniq/data/mapping/ossidMapping.txt
if [ -f ${_mountpoint_}/${HNAME}/ROOT/${ENIQ_DATA_MAPPING_DIR}/ossidMapping.txt ]; then
    log_msg -s "\nCopying ${ENIQ_DATA_MAPPING_DIR}/ossidMapping.txt from NAS backup" -l ${LOGFILE}
    $CP -pr ${_mountpoint_}/${HNAME}/ROOT/${ENIQ_DATA_MAPPING_DIR} ${ENIQ_DATA_DIR}
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: install_host_syncd ###
#
# Install the service and daemon to sync hosts file
#
# Arguments:
#   none
# Return Values:
#   none
#
install_host_syncd()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling install_host_syncd stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s install_host_syncd ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: install_nasd ###
#
# Install the nas daemon
#
# Arguments:
#   none
# Return Values:
#   none
#
install_nasd()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling install_nasd stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s install_nasd ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: install_nas_sw ###
#
# Install NAS software
#
# Arguments:
#   none
# Return Values:
#   none
install_nas_sw()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling install_nas_sw stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s install_nas_sw ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: install_service_scripts ###
#
# Installs scripts into /eniq/smf
#
# Arguments:
#   none
# Return Values:
#   none
install_service_scripts()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Run the stage
$BASH ${ENIQ_CORE_INST_SCRIPT} -s install_service_scripts ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: insert_header_footer ###
#
#   Insert a stage header/footer message
#
# Arguments:
#   $1 : head/foot
#   $2 : Message
#   $3 : Logfile
# Return Values:
#   none
insert_header_footer()
{
if [ $# -ne 3 ]; then
    _err_msg_="3 Parameters must be passed to header/footer function"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

if [ "$1" != "head" -a "$1" != "foot" ]; then
    _err_msg_="Only Param of head/foot is allowed...exiting!"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi
_type_=$1

_msg_=$2

_logfile_=$3
$MKDIR -p `$DIRNAME ${_logfile_}`
if [ $? -ne 0 ]; then
    _err_msg_="Could not create directory `$DIRNAME ${_logfile_}`"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

$TOUCH -a ${_logfile_}
if [ $? -ne 0 ]; then
    _err_msg_="Could not write to file ${_logfile_}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

_time_=`$DATE '+%Y-%b-%d_%H.%M.%S'`
if [ "$_type_" == "head" ]; then
    $ECHO "\n=====================================================" | $TEE -a ${_logfile_}
    $ECHO "-----------------------------------------------------" | $TEE -a ${_logfile_}
    $ECHO "$_time_ : $_msg_" | $TEE -a ${_logfile_}
    $ECHO "-----------------------------------------------------" | $TEE -a ${_logfile_}
fi

if [ "$_type_" == "foot" ]; then
    $ECHO "\n-----------------------------------------------------" | $TEE -a ${_logfile_}
    $ECHO "$_time_ : $_msg_" | $TEE -a ${_logfile_}
    $ECHO "-----------------------------------------------------" | $TEE -a ${_logfile_}
    $ECHO "=====================================================\n" | $TEE -a ${_logfile_}
fi
}

### Function: mount_nas_shares ###
#
# Mounting newly created NAS filesystem
#
# Arguments:
#   none
# Return Values:
#   none
mount_nas_shares()
{
# Checking if storage.ini file exists or not
if [ ! -s ${ENIQ_CONF_DIR}/${STORAGE_INI} ]; then
    log_msg -s "${ENIQ_CONF_DIR}/${STORAGE_INI} not found, or is empty" -l ${LOGFILE}
    return 1
fi

# Set a flag to change mount ownership
_change_owner_=0

if [ -s ${ENIQ_CONF_DIR}/${SUNOS_INI} ]; then
    # Get the System User/Group. All directories are owned by this
    _sysuser_=`iniget ENIQ_INSTALL_CONFIG -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v ENIQ_SYSUSER`
    if [ ! "${_sysuser_}" ]; then
        log_msg -s "Could not read parameter ENIQ_SYSUSER from file\n${ENIQ_CONF_DIR}/${ENIQ_INI}" -l ${LOGFILE}
        return 1
    fi

    if [ "${_sysuser_}" ]; then
        $CAT /etc/passwd | $EGREP "^${_sysuser_}" >> /dev/null 2>&1
        if [ $? -eq 0 ]; then
            _sysgrp_=`$ID ${_sysuser_} | $AWK '{print $2}' | $AWK -F\( '{print $2}' | $AWK -F\) '{print $1}'`
            if [ ! "${_sysgrp_}" ]; then
                log_msg -s "Could not determine group ID of ${_sysuser_}" -l ${LOGFILE}
                return 1
            fi
            _change_owner_=1
        fi
    fi
fi

_nas_alias_=nas6
_nas_dir_=/vx/${SYS_ID}-portbackup
_mountpoint_=/eniq/portbackup

# Create the mountpoint if its not there
if [ ! -d ${_mountpoint_} ]; then
    log_msg -s "\nCreating mountpoint - ${_mountpoint_}" -l $LOGFILE
    $MKDIR -p ${_mountpoint_}
    if [ $? -ne 0 ]; then
        _err_msg_="Could not create mountpoint ${_mountpoint_}"
        abort_script "${_err_msg_}"
    fi
fi

# Flag to determine if a mount is required
_mount_=0

log_msg -s "Checking if ${_mountpoint_} is already mounted\n" -l ${LOGFILE}
_mount_exists_=`mount | $GREP "${_mountpoint_}" | $AWK '{print $1}'`
if [ ! "${_mount_exists_}" ]; then
    _mount_=1
fi

if [ ${_mount_} -eq 1 ]; then
    log_msg -s "Mounting ${_nas_alias_}:${_nas_dir_} on ${_mountpoint_}" | $TEE -a ${LOGFILE}
    mount -F nfs ${_nas_alias_}:${_nas_dir_} ${_mountpoint_} >> /dev/null 2>&1
    if [ $? -ne 0 ]; then
        _err_msg_="Could not mount ${_nas_alias_}:${_nas_dir_} on ${_mountpoint_}"
        abort_script "${_err_msg_}"
    fi

    if [ ${_change_owner_} -eq 1 ]; then
        log_msg -s "Changing ownership of ${_mountpoint_} to ${_sysuser_}:${_sysgrp_}" >> ${LOGFILE}
        $CHOWN ${_sysuser_}:${_sysgrp_} ${_mountpoint_} >> /dev/null 2>&1
        if [ $? -ne 0 ]; then
            _err_msg_="Could not change ownership of ${_mountpoint_} to ${_sysuser_}:${_sysgrp_}"
            abort_script "${_err_msg_}"
        fi
    fi
fi
}

### Function: nas_share ###
#
# Share newly created NAS filesystem
#
# Arguments:
#   none
# Return Values:
#   none
nas_shares()
{
# Checking deployment type
if [ -s ${ENIQ_CONF_DIR}${DEPLOYMENT} ]; then
    _deployment_=`$CAT ${ENIQ_CONF_DIR}${DEPLOYMENT}`
else
    _err_msg_="Parameter \"deployment\" incorrectly specified, or is missing from boot command"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

if [ "${_deployment_}" == "ft" ]; then
    # we only configure 1 vlan i.e. the oss services vlan
    _intf_list_=`iniget IPMP -f ${ENIQ_CONF_DIR}/${IPMP_INI} | $HEAD -1`
    if [ ! "${_intf_list_}" ]; then
        _err_msg_="Could not build a list of IPMP blocks from ${ENIQ_CONF_DIR}/${IPMP_INI}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
else
    # we configured both services and storage vlans
    _intf_list_=`iniget IPMP -f ${ENIQ_CONF_DIR}/${IPMP_INI}`
    if [ ! "${_intf_list_}" ]; then
        _err_msg_="Could not build a list of IPMP blocks from ${ENIQ_CONF_DIR}/${IPMP_INI}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
fi

# Check if SFS to VA is performed or not
_sfs_va_flag_=0

_sfs_va_=`iniget SFS_TO_VA_MIGRATION_INDICATOR -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v SFS_to_VA_Migrated`
if [ "${_sfs_va_}" == "Y" ];then
    _sfs_va_flag_=1
    log_msg -s "\nSFS to VA migration has been performed for this deployment" -l ${LOGFILE}
fi

for _intf_ in ${_intf_list_}; do
    _vlan_name_=`iniget ${_intf_} -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v IPMP_Group_Name`
    if [ ! "${_vlan_name_}" ]; then
        _err_msg_="Could not read IPMP_Group_Name for ${_intf_} in ${ENIQ_CONF_DIR}/${IPMP_INI}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

    # Choose ENM storage vlan if we have SFS to VA performed for non-ft deployment
    if [ ${_sfs_va_flag_} -eq 1 -a "${_vlan_name_}" != "enm_stor_grp" -a "${_deployment_}" != "ft" ];then
        continue
    # Choose Storage vlan for non-ft deployment if available
    elif [ ${_sfs_va_flag_} -eq 0 -a "${_vlan_name_}" != "stor_grp" -a  "${_deployment_}" != "ft" ]; then
        continue
    else
        _intf_ip_=`iniget ${_intf_} -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v IPMP_Group_IP`
        if [ ! "${_intf_ip_}" ]; then
            _err_msg_="Could not read IPMP_Group_IP value for ${_intf_} in ${ENIQ_CONF_DIR}/${IPMP_INI}"
            abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
       fi

       # Reading value of IPMP_Group_Netmask from /eniq/installation/config/ipmp.ini
       _net_mask_ip_=`iniget ${_intf_} -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v IPMP_Group_Netmask`
        if [ ! "${_net_mask_ip_}" ]; then
            _err_msg_="Could not read IPMP_Group_Netmask value for ${_intf_} in ${ENIQ_CONF_DIR}/${IPMP_INI}"
           abort_script "$_err_msg_"
        fi

        #Generating subnet value using NetMask IP
        _subnet_=`get_network_from_netmask ${_net_mask_ip_}`
         if [ ! "${_subnet_}" ]; then
            _err_msg_="Could not generate subnet value using NetMask IP"
            abort_script "$_err_msg_"
         fi

        # Reading NETMASKS based on vlan
        if [ "${_vlan_name_}" == "enm_stor_grp" ]; then
              _netmask_attr_="ENM_STOR_NETMASK"
        elif [ "${_vlan_name_}" == "stor_grp" ]; then
              _netmask_attr_="STOR_NETMASK"
        fi

        if [ "${_deployment_}" == "ft" ]; then
            # Reading Network IP from /etc/netmasks
            _network_ip_=`$CAT /etc/netmasks | $GREP ${_net_mask_ip_} | $EGREP -v "#" | $AWK -F" " '{print $1}'`
            if [ ! "${_network_ip_}" ]; then
                _err_msg_="Could not read network IP from /etc/netmasks"
                abort_script "$_err_msg_"
            fi
        else
            # Reading Network IP from ipmp ini
            _network_ip_=`iniget ${_netmask_attr_} -f ${ENIQ_CONF_DIR}/${IPMP_INI} -v NETMASKS | $CUT -d":" -f1`
            if [ ! "${_network_ip_}" ]; then
                _err_msg_="Could not read Network IP from ${ENIQ_CONF_DIR}/${IPMP_INI}"
                abort_script "$_err_msg_"
            fi
        fi
    fi

    # Sharing newly created FS over the subnet
    log_msg -l ${LOGFILE} -s "Sharing NFS over the subnet ip ${_network_ip_}/${_subnet_}"
    ${_nascli_} add_client - ${_network_ip_}/${_subnet_} ${_share_opts_} ${_fs_name_}
    if [ $? -ne 0 ]; then
        _err_msg_="Problem encountered adding ${_network_ip_}/${_subnet_} as a client of NAS"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

done

log_msg -s "Successfully shared NAS filesystem" -l ${LOGFILE}
}


### Function: populate_nasd_config ###
#
# Populate the NASd config file
#
# Arguments:
#   none
# Return Values:
#   none
populate_nasd_config()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling populate_nasd_config stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s populate_nasd_config ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: print_heading ###
#
#   Displays message to user formatted with 
#   '-' in next line
#
# Arguments:
#   $1 : Message to be formatted
# Return Values:
#   None
print_heading()
{
_msg_="$*"
_len_=50
if [ "${_msg_}" ]; then
    _len_=`$ECHO ${_msg_} | $WC -c`
fi

# Print message followed by line of '-' below
$ECHO "\n${_msg_}" 
$PRINTF "%-${_len_}s\n" | $TR ' ' '-'
}

### Function: read_snap_label ###
#
# To read snapshot label from rollback_conf file
#
# Arguments:
#    $1 : snapshot type
# Return Values:
#     none
read_snap_label()
{
if [ ! "$1" ]; then
  _err_msg_="Snapshot type not passed\n"
  abort_script "${_err_msg_}"
fi

_type_=$1
    
case ${_type_} in
san) _san_snap_label_=`$CAT ${VAR_TMP_DIR}/rollback_conf | $GREP -w ${HNAME} | $AWK -F":" '{print \$2}'`
     _param_to_update_="SAN_SNAP_LABEL"
     _param_value_=${_san_snap_label_}
     ;;
nas) _nas_snap_label_=`$CAT ${VAR_TMP_DIR}/rollback_conf | $GREP -w ${HNAME} | $AWK -F":" '{print \$2}'`
     _param_to_update_="NAS_SNAP_LABEL"
     _param_value_=${_nas_snap_label_}
     ;;
\?) $ECHO "Invalid type"
    usage_msg
    exit 1
    ;;
esac

set_conf_value "${_param_to_update_}" ${_param_value_} ${_temp_snapshot_conf_}
}

### Function: recover_dwhdb ###
#
# To restore the dwhdb database.
#
# Arguments:
#    none
# Return Values:
#     none
recover_dwhdb()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip stage on non-Coordinator blades
if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for ${CURR_SERVER_TYPE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip stage if it's Rack server
if [ "${STORAGE_TYPE}" == "zfs" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for Rack" ${LOGFILE} 
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Check required script exist
local _recover_iq_script_="${ENIQ_BKUP_SW_BIN_DIR}/recover_iq.bsh"
check_for_file -s ${_recover_iq_script_}

# Enable the NAS Services
log_msg -t -l ${LOGFILE} -s "Enabling NAS services"
$SVCADM enable -st ${NASd_SMF_ID} >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not enable ${NASd_SMF_ID}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
else
    log_msg -t -s "Enabled NAS services\n" -l ${LOGFILE}
fi

# Waiting for NAS milestone service to come online
log_msg -t -l ${LOGFILE} -s "Waiting for NAS milestone service to come online on ${HNAME}"
_count_=0
while [ 1 ];
do
    _milestone_state_=`$SVCS -H -o state ${NAS_MILESTONE_SMF_ID}`
    if [ "${_milestone_state_}" == "online" ]; then
        log_msg -t -s "${NAS_MILESTONE_SMF_ID} service is online on ${HNAME}\n" -l ${LOGFILE}
        break
    fi
    $SLEEP 30
    let _count_=_count_+1
    if [ ${_count_} -eq 5 ]; then
        _err_msg_="${NAS_MILESTONE_SMF_ID} SMF not online. Check ${ENIQ_LOG_DIR}/NASd/NASd.log"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
done

log_msg -q -s "Starting to run $BASH ${_recover_iq_script_} -N" -l ${LOGFILE}
$BASH ${_recover_iq_script_} -N | $TEE -a ${LOGFILE}
_rc_recover_dwhdb_=`$ECHO ${PIPESTATUS[0]}`
if [ "${_rc_recover_dwhdb_}" -ne 0 ]; then
    _err_msg_="Exiting... Unable to restore the dwhdb database \n"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: recover_nas ###
#
# To rollback NAS Filesystems.
#
# Arguments:
#    none
# Return Values:
#     none
recover_nas()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip stage if it's Rack server
if [ "${STORAGE_TYPE}" == "zfs" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for Rack" ${LOGFILE} 
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip stage on non-Coordinator blades
if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for ${CURR_SERVER_TYPE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Disable NAS service
log_msg -t -l ${LOGFILE} -s "Disabling NAS services on ${HNAME}"
cd /
$SVCADM disable -st ${NASd_SMF_ID} >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _smf_status_=`$SVCS -H ${NASd_SMF_ID} | $AWK '{print $1}'`
    if [ "${_smf_status_}" == "maintenance" ];then
        $SVCADM clear ${NASd_SMF_ID} >> /dev/null 2>&1
    fi
    $SVCADM disable -st ${NASd_SMF_ID} >> /dev/null 2>&1
    if [ $? -ne 0 ]; then
        _err_msg_="Could not disable ${NASd_SMF_ID}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
fi

log_msg -t -s "Disabled NAS services on ${HNAME}\n" -l ${LOGFILE}

# Getting NAS snapshot label from migration_conf file
local NAS_SNAP_LABEL=`read_value NAS_SNAP_LABEL ${MIGRATION_CONF}` || abort_script "${NAS_SNAP_LABEL}" "${EXEC_SHELL_CMD}"

if [ -z "${NAS_SNAP_LABEL}" ]; then
    _err_msg_="Failed to get NAS snapshot label on ${HNAME}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
else
    log_msg -t -l ${LOGFILE} -s "NAS snapshots label is ${NAS_SNAP_LABEL} on ${HNAME}\n"
fi

if [ ! -f ${NAS_RECOVERY_SUCCESS} ]; then
    log_msg -t -l ${LOGFILE} -s "Rolling NAS snapshots with label ${NAS_SNAP_LABEL} on ${HNAME}"
    cd /
    $BASH ${ENIQ_BKUP_SW_BIN_DIR}/manage_nas_snapshots.bsh -a rollback -f ALL -n ${NAS_SNAP_LABEL} -N | $TEE -a ${LOGFILE}
    _rc_nas_=`$ECHO ${PIPESTATUS[0]}`
    if [ "${_rc_nas_}" -ne 0 ]; then
        _err_msg_="Failed to rollback NAS snapshot with label \"${NAS_SNAP_LABEL}\""
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    else
        $TOUCH ${NAS_RECOVERY_SUCCESS}
        log_msg -t -s "Successfully rolled back NAS snapshot on ${HNAME}\n" -l ${LOGFILE}
    fi
else
   log_msg -t -s "Already rolled back NAS snapshot on ${HOST_NAME}\n" -l ${LOGFILE}
fi

# Enable the NAS Services
log_msg -t -l ${LOGFILE} -s "Enabling NAS services on ${HNAME}"
$SVCADM enable -st ${NASd_SMF_ID} >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not enable ${NASd_SMF_ID}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
else
    log_msg -t -s "Enabled NAS services on ${HNAME}\n" -l ${LOGFILE}
fi

# Waiting for NAS milestone service to come online
log_msg -t -l ${LOGFILE} -s "Waiting for NAS milestone service to come online on ${HNAME}"
_count_=0
while [ 1 ];
do
    _milestone_state_=`$SVCS -H -o state ${NAS_MILESTONE_SMF_ID}`
    if [ "${_milestone_state_}" == "online" ]; then
        log_msg -t -s "${NAS_MILESTONE_SMF_ID} service is online on ${HNAME}\n" -l ${LOGFILE}
        break
    fi
    $SLEEP 30
    let _count_=_count_+1
    if [ ${_count_} -eq 5 ]; then
        _err_msg_="${NAS_MILESTONE_SMF_ID} SMF not online. Check ${ENIQ_LOG_DIR}/NASd/NASd.log"
        abort_script "${_err_msg_}"
    fi
done

#copying ssh key file
    if [ "${CO_SERVER}" ]; then
        $CP -p ${SSH_DIR}/id_rsa.pub ${ENIQ_ADMIN_DIR}/etc
        if [ $? -ne 0 ]; then
            _err_msg_="Failed to copy ${SSH_DIR}/id_rsa.pub file."
            abort_script "${_err_msg_}"
        fi
    fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: recover_repdb ###
#
# To restore the repository database.
#
# Arguments:
#    none
# Return Values:
#     none
recover_repdb()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip stage on non-Coordinator blades
if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for ${CURR_SERVER_TYPE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip stage if it's Rack server
if [ "${STORAGE_TYPE}" == "zfs" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for Rack" ${LOGFILE} 
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Check required script exist
local _repdb_restore_script_="${ENIQ_BKUP_SW_BIN_DIR}/repdb_restore.bsh"
check_for_file -s ${_repdb_restore_script_}

# Get the System User.
_sysuser_=`iniget ENIQ_INSTALL_CONFIG -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v ENIQ_SYSUSER`
if [ ! "${_sysuser_}" ]; then
    _err_msg_="Could not read parameter ENIQ_SYSUSER from ${ENIQ_CONF_DIR}/${SUNOS_INI} file"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

log_msg -q -s "Starting to run $SU - ${_sysuser_} -c "\"$BASH ${_repdb_restore_script_} -c ${CLI_CONF_DIR}"\"" -l ${LOGFILE}
$SU - ${_sysuser_} -c "$BASH ${_repdb_restore_script_} -c ${CLI_CONF_DIR}" | $TEE -a ${LOGFILE}
_rc_recover_repdb_=`$ECHO ${PIPESTATUS[0]}`
if [ "${_rc_recover_repdb_}" -ne 0 ]; then
    _err_msg_="Exiting... Unable to restore the repository database \n"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: recover_san ###
#
# To rollback SAN Snapshot.
#
# Arguments:
#    none
# Return Values:
#     none
recover_san()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip stage on non-Coordinator blades
if [ ! "${CO_SERVER}" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for ${CURR_SERVER_TYPE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip stage if it's Rack server
if [ "${STORAGE_TYPE}" == "zfs" ]; then
    insert_header_footer foot "INFO: Skipping ${ACTION_TYPE} Stage - ${NEXT_STAGE} for Rack" ${LOGFILE} 
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Getting SAN snapshot label from migration_conf file
local SAN_SNAP_LABEL=`read_value SAN_SNAP_LABEL ${MIGRATION_CONF}` || abort_script "${SAN_SNAP_LABEL}" "${SAN_SNAP_LABEL}"

if [ -z "${SAN_SNAP_LABEL}" ]; then
    _err_msg_="Failed to get SAN snapshot label on ${HNAME}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
else
    log_msg -t -l ${LOGFILE} -s "SAN snapshots label is ${SAN_SNAP_LABEL} on ${HNAME}\n"
fi

if [ ! -f ${SAN_RECOVERY_SUCCESS} ]; then
    log_msg -t -l ${LOGFILE} -s "Rolling SAN snapshots with label ${SAN_SNAP_LABEL} on ${HNAME}"
    cd /
    $BASH ${ENIQ_BKUP_SW_BIN_DIR}/manage_san_snapshots.bsh -a rollback -f ALL -n ${SAN_SNAP_LABEL} -N | $TEE -a ${LOGFILE}
    _rc_san_=`$ECHO ${PIPESTATUS[0]}`
    if [ "${_rc_san_}" -ne 0 ]; then
        _err_msg_="Failed to rollback SAN snapshot with label \"${SAN_SNAP_LABEL}\""
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    else
        $TOUCH ${SAN_RECOVERY_SUCCESS}
        log_msg -t -s "Successfully rolled back SAN snapshot on ${HNAME}\n" -l ${LOGFILE}
    fi
else
    log_msg -t -s "Already rolled back SAN snapshot on ${HOST_NAME}\n" -l ${LOGFILE}
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: recover_sentinel ###
#
# To recove sentinel.
#
# Arguments:
#    none
# Return Values:
#     none
recover_sentinel()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling eniq_sentinel_install.bsh to install only SMF for sentinel license server
$BASH ${ENIQ_SENTINEL_DIR}/admin/eniq_sentinel_install.bsh -a smf
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Ensure licensing service is online
if [ "${CO_SERVER}" ];then
    check_and_manage_smf ${SENTINEL_SMF_ID} enable
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: recovery_cleanup ###
#
#   Cleanup recovery temporary files/dirs
#
# Arguments:
#   none
# Return Values:
#   none
recovery_cleanup()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Enabling the OSS mounts
enable_oss_mounts

log_msg -s "Cleaning up temporary directory used for recovery." -l ${LOGFILE}
$RM -rf ${TEM_DIR}

log_msg -s "Cleaning up flag ${MIGR_PROGRESS} used for recovery." -l ${LOGFILE}
$RM -rf ${MIGR_PROGRESS}

log_msg -s "Cleaning up flag ${NAS_RECOVERY_SUCCESS} and ${SAN_RECOVERY_SUCCESS} used for recovery." -l ${LOGFILE}
$RM -rf ${NAS_RECOVERY_SUCCESS}
$RM -rf ${SAN_RECOVERY_SUCCESS}

$TOUCH ${MIGR_SUCCESS}

log_msg -s "Cleaning up ${ENIQ_CONF_DIR}/${IPMP_INI}.recovery used for recovery." -l ${LOGFILE}
$RM -rf ${ENIQ_CONF_DIR}/${IPMP_INI}.recovery

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
}

### Function: remote_premigration ###
#
# Runs required premigration stages from CO
#
# Arguments:
#   none
# Return Values:
#   none
remote_premigration()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip if Rack
if [ "${STORAGE_TYPE}" != "raw" ]; then
    insert_header_footer foot "Rack Migration - Skipping - ${NEXT_STAGE}" ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Skip stage if not CO server
if [ "${CO_SERVER}" != "YES" ]; then  
    insert_header_footer foot "Skipping ${ACTION_TYPE} stage for ${CURR_SERVER_TYPE} - ${NEXT_STAGE} " ${LOGFILE} 
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Get server order
if [ `$CAT ${ORDER_FILE} | $WC -l` -eq 1 ]; then
    # Check if the activity type is not replacement
    if [ "${REPLACEMENT}" != "YES" ]; then
        log_msg -s "Premigration triggered in Single Blade environment. " -l ${LOGFILE}
        insert_header_footer foot "Skipping ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}
        set_next_stage `$EXPR ${ARRAY_ELEM}+1`
        return 0
    fi
    log_msg -s "Blade replacement premigration triggered for only one server." -l ${LOGFILE}
fi

# Command to run on remote 
_optional_=""
if [ "${REPLACEMENT}" == "YES" ]; then
    _optional_="-R"
fi

_premigration_cmd_="$BASH ${MIGRATION_BIN}/eniq_solaris_linux_migration.bsh -a ${ACTION_TYPE} ${_optional_}"
_ssh_cmd_="$SSH -o StrictHostKeyChecking=no -o BatchMode=yes -q -l root"

for _entry_ in `$CAT ${ORDER_FILE}`; do
    _server_ip_=`$ECHO "${_entry_}" | $NAWK -F"::" '{print $1}'`
    _server_name_=`$ECHO "${_entry_}" | $NAWK -F"::" '{print $2}'`

    # No need to run script remotely on local. 
    # Next stage will be triggered following stage list.
    if [ "${_server_ip_}" == "${HOST_IP}" ]; then
        # Set CO for premigration if CO is listed in order file
        # This flag is required in case CO is not selected for Replacement
        $RM -rf ${MIGRATE_CO} >> /dev/null
        $TOUCH ${MIGRATE_CO}
        if [ ! -f ${MIGRATE_CO} ]; then
            _err_msg_="Couldn't create the flag ${MIGRATE_CO} to exclude CO from ${ACTION_TYPE}"
            abort_script "${_err_msg_}"
        fi
        continue
    fi

    # Check if premigration success flag is present
    run_remote_cmd "${_server_ip_}" "$LS ${MIGR_SUCCESS} >>/dev/null 2>&1" "$LOGFILE"
    if [ $? -eq 0 ]; then
        log_msg -s "Premigration steps are already completed on ${_server_name_}" -l $LOGFILE
        continue
    fi

    # Check the presence of migration script
    run_remote_cmd "${_server_ip_}" "$LS ${MIGRATION_BIN}/eniq_linux_migration.bsh >>/dev/null 2>&1" "$LOGFILE"
    if [ $? -ne 0 ]; then
        _err_msg_="Premigration script ${MIGRATION_BIN}/eniq_linux_migration.bsh not found on ${_server_name_}"
        abort_script "${_err_msg_}"
    fi

    # If premigration not completed, run/resume activity   
    log_msg -h -s "Starting Premigration on ${_server_name_}" -l ${LOGFILE}
    run_remote_cmd "${_server_ip_}" "${_premigration_cmd_}" "$LOGFILE" | $TEE -a ${LOGFILE}
    if [ $? -ne 0 ]; then
        _err_msg_="Premigration failed on ${_server_name_}. Check logfile ${LOGFILE}"
        abort_script "${_err_msg_}"
    else
        run_remote_cmd "${_server_ip_}" "$LS ${MIGR_SUCCESS} >>/dev/null 2>&1" "$LOGFILE"
        if [ $? -ne 0 ]; then
            _err_msg_="Premigration not completed on ${_server_name_}. Check logfile ${LOGFILE}"
            abort_script "${_err_msg_}"
        fi
        log_msg -h -s "Completed Premigration on ${_server_name_}" -l ${LOGFILE}
    fi
done 

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: recreate_db_symlinks ###
#
# Updates sym_links_ini and recreate_db_symlinks
#
# Arguments:
#   none
# Return Values:
#   none
recreate_db_symlinks()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip if engine
if [ "${CO_SERVER}" != "YES" -a "${RD_SERVER}" != "YES" ]; then
    insert_header_footer foot "Skipping the stage for ${CURR_SERVER_TYPE} - ${NEXT_STAGE} " ${LOGFILE}
    set_next_stage `$EXPR ${ARRAY_ELEM}+1`
    return 0
fi

# Check required files and scripts exist
local _update_sym_links_ini_="${ENIQ_CORE_INST_DIR}/bin/update_sym_links_ini.bsh"
check_for_file -s ${_update_sym_links_ini_}

# Calling update_sym_links_ini script
$BASH ${ENIQ_CORE_INST_DIR}/bin/update_sym_links_ini.bsh -N -M
if [ $? -ne 0 ]; then
    _err_msg_="Unable to update sym_links.ini file."
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Calling stage create_db_sym_links from eniq_core_install 
$BASH ${ENIQ_CORE_INST_SCRIPT} -s create_db_sym_links -R -n -l ${LOGFILE}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}
set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: save_feature_info ###
#
# Save feature info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_feature_info()
{
log_msg -s "\nGetting license information for migration...\n" -l "${LOGFILE}"
if [ ! "${CO_SERVER}" ];then
    log_msg -s "\nSkipping this step on non-Coordinator server. \n" -l "${LOGFILE}"
    return 0
fi
while [ 1 ]; do
    $CLEAR
    #Taking User Inputs
    while [ 1 ]; do
        ask_for_input "ENIQ license File path for Migration"
        _server_lic_path_=${USER_VALUE}
        if [ ! -s ${_server_lic_path_} ]; then
            $ECHO "Not a valid license path"
            continue
        fi
        break
    done

    # Taking User Confirmation
    _disp_file1_=${TEM_DIR}/disp_file1
    $ECHO "\nENIQ License File Path for migration" > ${_disp_file1_}
    $ECHO "---------------------------------------" >> ${_disp_file1_}
    $PRINTF '%-40s' "License File Path for migration" >> ${_disp_file1_}
    $ECHO ": ${_server_lic_path_}" >> ${_disp_file1_}
    $CAT ${_disp_file1_}
    $ECHO "\n\nPlease confirm if the above details are correct for migration."
    user_confirm
    if [ "${_response_}" != "YES" ];then
        continue
    fi
    break
done
#Saving Information to temporary Config file
set_conf_value "MIG_LIC_INFO_PATH" ${_server_lic_path_} ${_temp_migr_conf_}
log_msg -s "ENIQ License Path is provided.\n" -l "${LOGFILE}"

#Backing up the license file
log_msg -s "\nBacking up the ${_server_lic_path_} file to ${VAR_TMP_UPGRADE} \n" -l "${LOGFILE}"
$CP -p ${_server_lic_path_} ${VAR_TMP_UPGRADE}
if [ $? -ne 0 ]; then
     _err_msg_="Could not backup ${_server_lic_path_} file to ${VAR_TMP_UPGRADE}"
     abort_script "${_err_msg_}"
fi
}

### Function: save_ip_data ###
#
# Save migration info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_ip_data()
{
log_msg -s "\nGetting IP details for migration...\n" -l "${LOGFILE}"
if [ ! "${CO_SERVER}" ];then
   
   #Checking backup Vlan is Exist Or not
   $CAT ${MIGR_CO_CONF} | $GREP  "BKUP_VLAN_EXIST_"  >>/dev/null 2>&1   
   _status_bkup_vlan_exist_=$?
   
   # Check if CO migration conf is created
    if [ -s ${MIGR_CO_CONF} ]; then
        _server_mws_ip_=`read_value MIG_MWS_IP ${MIGR_CO_CONF}` || abort_script "${_server_mws_ip_}"
        _server_nas_ip_=`read_value MIG_NAS_IP ${MIGR_CO_CONF}` || abort_script "${_server_nas_ip_}"
        if [ ${_status_bkup_vlan_exist_} -eq 0 ] ; then       
            _server_ombs_ip_=`read_value MIG_OMBS_IP ${MIGR_CO_CONF}` || abort_script "${_server_ombs_ip_}"
        fi
    fi

elif [ ! "${_server_mws_ip_}" -o ! "${_server_nas_ip_}" -o ! "${_server_ombs_ip_}" ]; then
    #Taking user input for MWS IP
    $CLEAR
    while [ 1 ]; do
        while [ 1 ]; do
            $ECHO "Please enter MWS IP"
            #Taking the Default MWS IP 
            _def_server_mws_ip_=`$CAT ${ENIQ_CONF_DIR}/eniq_sw_locate | $AWK -F"@" '{print $1}'`
            $ECHO "Hit enter For ( $_def_server_mws_ip_ )"
            read USER_VALUE 
            if [ ! "${USER_VALUE}" ]; then
                _server_mws_ip_=${_def_server_mws_ip_}
            else
                _server_mws_ip_=${USER_VALUE}
            fi
            validate_ip ${_server_mws_ip_}
            if [ $? -ne 0 ]; then
                continue
            fi
            #Checking whether IP is reachable or not
            $PING ${_server_mws_ip_}  >> /dev/null 2>&1
            if [ $? -ne 0 ]; then
                continue
            fi 
            break
        done
        #Taking user input for NAS Console IP
        if [ "${STORAGE_TYPE}" == "raw" ]; then
            while [ 1 ]; do
                $ECHO "\nPlease enter NAS Console IP"
                #Taking the Default Nas Console Ip 
                _def_server_nas_ip_=`$CAT /etc/hosts | $GREP -w nasconsole | $AWK '{print $1}'`
                $ECHO "Hit enter For ( ${_def_server_nas_ip_} )"
                read USER_VALUE
                if [ ! "${USER_VALUE}" ]; then
                    _server_nas_ip_=${_def_server_nas_ip_}
                else
                    _server_nas_ip_=${USER_VALUE}
                fi
                validate_ip ${_server_nas_ip_}
                if [ $? -ne 0 ]; then
                    continue
                fi
                #Checking whether IP is reachable or not
                $PING ${_server_nas_ip_}  >> /dev/null 2>&1
                if [ $? -ne 0 ]; then
                    continue
                fi
                break
            done
        fi

        #Taking user input for OMBS IP
        _host_=$HNAME
        #Cheking the scenario where user do not have OMBS solution
        $CAT ${_temp_migr_conf_} | $GREP  "BKUP_VLAN_EXIST_"  >>/dev/null 2>&1
        _status_bkup_vlan_exist_=$?
        if [ ${_status_bkup_vlan_exist_} -eq 0 ] ; then
            while [ 1 ]; do
                ask_for_input "OMBS IP"
                _server_ombs_ip_=${USER_VALUE}
                validate_ip ${_server_ombs_ip_}
                if [ $? -ne 0 ]; then
                    continue
                fi
                #Checking whether IP is reachable or not
                $PING ${_server_ombs_ip_}  >> /dev/null 2>&1
                if [ $? -ne 0 ]; then
                    continue
                fi
                break
            done
        fi
        # Taking User Confirmation
        _disp_file1_=${TEM_DIR}/disp_file1
        if [ ${_status_bkup_vlan_exist_} -eq 0 ] ; then
            $ECHO "\nENIQ MWS IP, OMBS IP, NAS Console IP for migration" > ${_disp_file1_}
        else
            $ECHO "\nENIQ MWS IP,NAS Console IP for migration" > ${_disp_file1_}
        fi
        $ECHO "-------------------------------------------------------------------------" >> ${_disp_file1_}
        $PRINTF '%-25s'  "MWS IP" >> ${_disp_file1_}
        $ECHO ": ${_server_mws_ip_}" >> ${_disp_file1_}
        $PRINTF '%-25s'  "NAS Console IP" >> ${_disp_file1_}
        $ECHO ": ${_server_nas_ip_}" >> ${_disp_file1_}
        if [ ${_status_bkup_vlan_exist_} -eq 0 ] ; then
            $PRINTF '%-25s'  "OMBS IP" >> ${_disp_file1_}
            $ECHO ": ${_server_ombs_ip_}" >> ${_disp_file1_}
        fi
        $CAT ${_disp_file1_}
        $ECHO "\n\nPlease confirm if the above details are correct for migration."
        user_confirm
        if [ "${_response_}" != "YES" ];then
            continue
        fi
        break
    done
fi
#Saving Information to Temporary Config file
set_conf_value "MIG_MWS_IP" ${_server_mws_ip_} ${_temp_migr_conf_}
if [ ${_status_bkup_vlan_exist_} -eq 0 ] ; then
    set_conf_value "MIG_OMBS_IP" ${_server_ombs_ip_} ${_temp_migr_conf_}
fi
set_conf_value "MIG_NAS_IP" ${_server_nas_ip_} ${_temp_migr_conf_}
if [ ${_status_bkup_vlan_exist_} -eq 0 ] ; then
    log_msg -s "MWS IP, OMBS IP, NAS Console IP are provided.\n" -l "${LOGFILE}"
else
    log_msg -s "MWS IP, NAS Console IP are provided.\n" -l "${LOGFILE}"
fi

}

### Function: save_mig_scp_info ##
#
# Save migration info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_mig_scp_info()
{
if [ ! "$1" ];then
    _err_msg_="File name is required"
    abort_script "${_err_msg_}"
fi
_temp_conf_file_=$1

# Get NAS IP
_nas_name_=`$DF -hk ${ENIQ_BACKUP_DIR} | $GREP -v 'Filesystem' | $AWK -F\: '{print $1}'`
_nas_dir_=`$DF -hk ${ENIQ_BACKUP_DIR} | $GREP -v 'Filesystem' | $AWK '{print $1}' | $CUT -f2 -d:`
if [ ! "${_nas_name_}" -o ! "${_nas_dir_}" ];then
    _err_msg_="NAS information for ${ENIQ_BACKUP_DIR} not found."
    abort_script "${_nas_name_}"
fi
_nas_ip_=`$CAT /etc/hosts | $GREP -w $_nas_name_ | $AWK '{print $1}'`
_nas_user_=`$CAT ${ENIQ_CONF_DIR}/ssh_input_file | $GREP R_USER_S= | $CUT -f2 -d\"`

# Insert value in migration.conf
set_conf_value MIG_BACKUP_NAS_IP $_nas_ip_ $_temp_conf_file_
set_conf_value MIG_BACKUP_NAS_DIR ${MIG_BACKUP_NAS_DIR} $_temp_conf_file_
set_conf_value MIG_SW_SCP_USER "${_nas_user_}" $_temp_conf_file_
}

### Function: save_password_info ###
#
# Save OS user password info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_password_info()
 {
log_msg -s "\nGetting password information for migration...\n" -l "${LOGFILE}"

_disp_file_=${TEM_DIR}/disp_file
$RM -rf ${_disp_file_}

unset _root_pwd_ _dc_pwd_

# Get the System User and Group. All directories are owned by this
SYSUSER=`iniget ENIQ_INSTALL_CONFIG -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v ENIQ_SYSUSER`
if [ $? -ne 0 ]; then
    _err_msg_="Could not read SYSUSER param from ${ENIQ_CONF_DIR}/${SUNOS_INI}"
    abort_script "$_err_msg_"
fi

SYSGRP=`$ID ${SYSUSER}|$NAWK '{print $2}'|$NAWK -F\( '{print $2}'|$NAWK -F\) '{print $1}'`
if [ ! "${SYSGRP}" ]; then
    _err_msg_="Could not read SYSGRP param from ${ENIQ_CONF_DIR}/${SUNOS_INI}"
    abort_script "$_err_msg_"
fi
if [ "${CO_SERVER}" ]; then
#Dcuser Password
DBA_PASSWORD=`inigetpassword DB -f ${CLI_CONF_DIR}/${ENIQ_INI} -v DBAPassword`
if [ ! ${DBA_PASSWORD} ]; then
  if [ -f ${ENIQ_BASE_DIR}/sw/installer/dbusers ]; then
            DBA_PASSWORD=`${ENIQ_BASE_DIR}/sw/installer/dbusers dba dwh`
     if [ ! "${DBA_PASSWORD}" ] ; then
                _err_msg_="Could not get dwhdb DBA Password"
                abort_script "$_err_msg_"
     fi
   fi
fi

REP_PORT=`iniget REP -v PortNumber -f ${CLI_CONF_DIR}/niq.ini`
REP_ENG=`iniget REP -v ServerName -f ${CLI_CONF_DIR}/niq.ini`
if [ ! "${REP_PORT}" -o ! "${REP_ENG}" ]; then
        _err_msg_="Could not read db values from ${CLI_CONF_DIR}/${ENIQ_INI}"
        abort_script "$_err_msg_"
fi
#removing the old connection strings
$RM -rf /var/tmp/con_str_encrypt.txt.*


#Initialising the connection string for dwhdb
rep_connection_string="-nogui -onerror exit -c \"eng=${REP_ENG};links=tcpip{host=localhost;port=${REP_PORT}};uid=dba;pwd=${DBA_PASSWORD}\""
rep_connection_string_enc=${VAR_TMP_DIR}/con_str_encrypt.txt.$$

# encrypt the connection string.
get_encrypt_file "${rep_connection_string}" "${rep_connection_string_enc}"


DBISQL="$(ls /eniq/sybase_iq/IQ-*/bin64/dbisql)"
if [ ! -x "$DBISQL" ]; then
    _err_msg_="$DBISQL commands not found or not executable."
    abort_script "$_err_msg_"
fi
    log_msg -s "\nFetching password information for dcuser from database...\n" -l "${LOGFILE}"
    $SU - $SYSUSER -c "$DBISQL  @${rep_connection_string_enc} \"select PASSWORD from etlrep.meta_databases where USERNAME='dcuser';OUTPUT TO /tmp/dcuser_pwd.txt;\"" >/dev/null 2>&1
    if [ $? -ne 0 ]; then
        _err_msg_="Could not execute command."
        abort_script "$_err_msg_" "${EXEC_SHELL_CMD}"
    fi
    _dc_pwd_=`$CAT /tmp/dcuser_pwd.txt | $HEAD -1 | $NAWK -F"\'" '{print $2}' | $OPENSSL enc -base64`
    if [ ! "{_dc_pwd_}" ]; then
        _err_msg_="Could not fetch/encrypt password for dcuser."
        abort_script "$_err_msg_" "${EXEC_SHELL_CMD}"
    fi
fi
# Return if not CO blade
if [ ! "${CO_SERVER}" ]; then
    log_msg -s "\nGetting OS user password information for $HNAME ..." -l ${LOGFILE}
    _host_=$HNAME
    # Check if CO migration conf is created. Get values from there
    if [ -s ${MIGR_CO_CONF} ]; then
        # Check if ROOT_PASSWORD parameter is there
        $CAT ${MIGR_CO_CONF} | $GREP ^ROOT_PASSWORD_${_host_}= >> /dev/null 2>&1
        if [ $? -eq 0 ]; then
            _root_pwd_=`$CAT ${MIGR_CO_CONF} | $GREP ^ROOT_PASSWORD_${_host_}= | $NAWK -F"ROOT_PASSWORD_\${_host_}=" '{print $2}'`
            if [ ! "${_root_pwd_}" ]; then
                abort_script "${_root_pwd_}"
            fi
        fi
        $CAT ${MIGR_CO_CONF} | $GREP ^DC_PASSWORD_${_host_}= >> /dev/null 2>&1
        if [ $? -eq 0 ]; then
            _dc_pwd_=`$CAT ${MIGR_CO_CONF} | $GREP ^DC_PASSWORD_${_host_}= | $NAWK -F"DC_PASSWORD_\${_host_}=" '{print $2}'` 
            if [ ! "${_dc_pwd_}" ]; then
                abort_script "${_dc_pwd_}"
            fi
        fi

       # Check if Password info need to be saved in conf file
       if [ "${_root_pwd_}" -o "${_dc_pwd_}" ]; then
           # Set value in migration conf
           set_conf_value ROOT_PASSWORD_${_host_} ${_root_pwd_} ${_temp_migr_conf_}
           set_conf_value DC_PASSWORD_${_host_} ${_dc_pwd_} ${_temp_migr_conf_}
           log_msg -s "\nSuccessfully saved OS user password information on ${_host_}." -l ${LOGFILE}
       fi
    else
        _err_msg_="File ${MIGR_CO_CONF} is empty."
        abort_script "${_err_msg_}"
    fi
else
    # Loop through to get password info for each blade
    for _line_ in `$CAT ${ORDER_FILE}`; do
        _host_=`$ECHO ${_line_} | $NAWK -F'::' '{print $2}'`
        _srv_name_=`$ECHO ${_line_} | $NAWK -F'::' '{print $3}'`
       
            $CLEAR
            log_msg -s "\nGetting OS user password information for ${_host_} [${_srv_name_}]..." -l ${LOGFILE}
            # Ask for root user password.
            while true
            do
                $ECHO "Please enter password for root user on ${_host_}:"
                passwd=$(/usr/bin/perl -e 'system ("stty -echo");my $_temp_password_ =<STDIN>; print $_temp_password_; system ("stty echo");')
                $ECHO "\nRe-Enter password for root user on ${_host_}:"
                passwd1=$(/usr/bin/perl -e 'system ("stty -echo");my $_temp_password_1 =<STDIN>;print $_temp_password_1; system ("stty echo");')
                if [ "${passwd}" == "${passwd1}" ]; then
                    break
                else
                    $ECHO "password mismatch. Try again..."
                    $SLEEP 2
                    continue
                fi
            done
            _root_pwd_=`$ECHO ${passwd} | $OPENSSL enc -base64`
            if [ ! "{_root_pwd_}" ]; then
                _err_msg_="Could not encrypt password for root."
                abort_script "$_err_msg_" "${EXEC_SHELL_CMD}"
            fi
            set_conf_value ROOT_PASSWORD_${_host_} ${_root_pwd_} ${_temp_migr_conf_}
            set_conf_value DC_PASSWORD_${_host_} ${_dc_pwd_} ${_temp_migr_conf_}
            log_msg -s "\nSuccessfully taken OS user password information for ${_host_} [${_srv_name_}]" -l ${LOGFILE}       
    done
fi

# Removing temporary file
$RM -rf ${_disp_file_} >> /dev/null 2>&1
log_msg -s "\nSuccessfully completed step to get OS user password information." -l ${LOGFILE}
}

### Function: save_recovery_info ###
#
# Save migration info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_recovery_info()
{
log_msg -s "Getting current ENIQ Base SW and OM SW paths..." -l "${LOGFILE}"

if [ ! "${CO_SERVER}" ];then
    # Check if CO migration conf is created. Get values from there
    if [ -s ${MIGR_CO_CONF} ]; then
        _curr_base_sw_=`read_value REC_BASE_SW_LOC ${MIGR_CO_CONF}` || abort_script "${_curr_base_sw_}"
        _curr_om_sw_=`read_value REC_OM_SW_LOC ${MIGR_CO_CONF}` || abort_script "${_curr_om_sw_}"
    fi
else
    if [ "${REPLACEMENT}" == "YES" ]; then
        # Should be same as SW location for migration
        _curr_base_sw_=`read_value MIG_BASE_SW_LOC ${_temp_migr_conf_}` || abort_script "${_curr_base_sw_}"
        _curr_om_sw_=`read_value MIG_OM_SW_LOC ${_temp_migr_conf_}` || abort_script "${_curr_om_sw_}"
    fi
fi

# Store the values
if [ ! "${_curr_base_sw_}" -o ! "${_curr_om_sw_}" ]; then
    # Ask SW location path info for migration conf file
    common_save_sw_loc_path "$_temp_migr_conf_" "recovery" "curr"
    if [ $? -ne 0 ]; then
        _err_msg_="Unable to fetch required details from ${ENIQ_CONF_DIR}/${BLK_STOR_INI}"
        abort_script "${_err_msg_}"
    fi
    log_msg -s "Current ENIQ Base SW and OM SW locations are provided.\n" -l "${LOGFILE}"
else
    # Save SW location path info in migration conf file 
    set_conf_value "REC_BASE_SW_LOC" ${_curr_base_sw_} ${_temp_migr_conf_}
    set_conf_value "REC_OM_SW_LOC" ${_curr_om_sw_} ${_temp_migr_conf_}
fi

log_msg -s "Current ENIQ Base SW and OM SW locations successfully saved.\n" -l "${LOGFILE}"
}


### Function: save_san_data ###
#
# Save storage info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_san_data()
{
if [ ! "$1" ];then
    _err_msg_="File name is required"
    abort_script "${_err_msg_}"
fi
_temp_conf_file_=$1

log_msg -s "Getting the Storage details..." -l "${LOGFILE}"

# Save Storage information
set_conf_value "SAN_DEVICE_TYPE" ${SAN_DEVICE} ${_temp_conf_file_}

_storage_conf_file_=${ERICSSON_STOR_DIR}/san/plugins/${SAN_DEVICE}/etc/clariion.conf

# Get host agent IP
_host_agent_ip_=`iniget BLK_STORAGE_INTERF -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_INTERF_IP`
if [ ! "${_host_agent_ip_}" ];then
    _err_msg_="Unable to retrieve HOST AGENT IP from ${ENIQ_CONF_DIR}/${BLK_STOR_INI} file."
    abort_script "${_err_msg_}"
fi
set_conf_value "HOST_AGENT_IP" ${_host_agent_ip_} ${_temp_conf_file_}

_storage_list_=`iniget BLK_STORAGE_DEV_DETAILS -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI}`
_count_=1
for _storage_ in $_storage_list_; do
    _stor_name_=`iniget ${_storage_} -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_NAME`
    _stor_spa_=`iniget ${_storage_} -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_IP_SPA`
    _stor_spb_=`iniget ${_storage_} -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_IP_SPB`
    _stor_group_=`iniget ${_storage_} -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_GROUP_NAME`
    _stor_admin_=`iniget ${_storage_} -f ${ENIQ_CONF_DIR}/${BLK_STOR_INI} -v BLK_STORAGE_USERNAME`
    if [ ! "${_stor_name_}" -o ! "${_stor_spa_}" -o ! "${_stor_spb_}" -o \
         ! "${_stor_group_}" -o ! "${_stor_admin_}" ];then
        _err_msg_="Unable to fetch required details from ${ENIQ_CONF_DIR}/${BLK_STOR_INI}"
        abort_script "${_err_msg_}"
    fi

    # Run in non-CO to get the Storage password 
    if [[ ! ${BACKUP} ]] && [[ ! "${CO_SERVER}" ]]; then
        # Check if the password exists in CO migration conf
        _stor_pass_=`read_value STORAGE_PASS_${_count_} ${MIGR_CO_CONF}` || abort_script "${_stor_pass_}"
    fi

    if [[ ! "${BACKUP}" ]] && [[ ! ${_stor_pass_} ]]; then
        # Get Password from user
        _disp_file_=${TEM_DIR}/disp_file
        $ECHO "\nDisplaying details of Storage ${_stor_name_}" > ${_disp_file_}
        $ECHO "----------------------------------------------" >> ${_disp_file_}
        $ECHO "Storage Name\t\t: ${_stor_name_}" >> ${_disp_file_}
        $ECHO "Storage SP A IP Address\t: ${_stor_spa_}" >> ${_disp_file_}
        $ECHO "Storage SP B IP Address\t: ${_stor_spb_}" >> ${_disp_file_}
        $ECHO "Storage Admin userid\t: ${_stor_admin_}" >> ${_disp_file_}
        while [ 1 ]; do
            while true
        do
            $ECHO "Please enter Storage Admin password for ${_stor_name_}:"
            _stor_pass_=$(/usr/bin/perl -e 'system ("stty -echo");my $_temp_san_password_ =<STDIN>; print $_temp_san_password_; system ("stty echo");')
            $ECHO "\nRe-Enter Storage Admin password for ${_stor_name_}:"
            _stor_pass1_=$(/usr/bin/perl -e 'system ("stty -echo");my $_temp_san_password_1 =<STDIN>; print $_temp_san_password_1; system ("stty echo");')
            if [ "${_stor_pass_}" == "${_stor_pass1_}" ]; then
                break
            else
                $ECHO "password mismatch. Try again..."
                $SLEEP 2
                continue
            fi
        done
        _stor_pass_=`$ECHO ${_stor_pass_} | $OPENSSL enc -base64`
        if [ -z $_stor_pass_ ]; then
            _err_msg_="Unable to encrypted password"
            abort_script "${_err_msg_}" 
        fi
            $CLEAR            
            # Take user confirmation
            $CAT ${_disp_file_}
            $ECHO "\nPlease confirm the Storage details for ${_stor_name_}"
            user_confirm
            if [ "${_response_}" != "YES" ];then
                break
            else
                continue
            fi
        done
    fi

    # Set values
    set_conf_value "STORAGE_NAME_${_count_}" ${_stor_name_} ${_temp_conf_file_}
    set_conf_value "STORAGE_SPA_${_count_}" ${_stor_spa_} ${_temp_conf_file_}
    set_conf_value "STORAGE_SPB_${_count_}" ${_stor_spb_} ${_temp_conf_file_}
    set_conf_value "STORAGE_ADMIN_${_count_}" ${_stor_admin_} ${_temp_conf_file_}
    set_conf_value "STORAGE_GROUP_${_count_}" ${_stor_group_} ${_temp_conf_file_}
    if [ ! ${BACKUP} ];then
        set_conf_value "STORAGE_PASS_${_count_}" ${_stor_pass_} ${_temp_conf_file_}
    fi
    $RM -rf ${_disp_file_}
    _count_=`$EXPR $_count_ + 1`
    # Unset _stor_pass_
    unset _stor_pass_ _stor_pass1_
done

log_msg -s "Storage details are saved successfully.\n" -l "${LOGFILE}"
}

### Function: save_snapshot_label ###
#
# To save snapshot label for SAN and NAS into migration_conf file
#
# Arguments:
#    none
# Return Values:
#     none
save_snapshot_label()
{
_temp_snapshot_conf_=${VAR_TMP_DIR}/snapshot.conf.$$.$$
$RM -rf ${_temp_snapshot_conf_}
$TOUCH ${_temp_snapshot_conf_}

# Get SAN and NAS snapshot label
if [ "${STORAGE_TYPE}" != "zfs" ]; then
    read_snap_label san
    read_snap_label nas
fi

# Update migration_conf file with snapshot data
if [ -s ${MIGRATION_CONF} ]; then
    $CAT ${_temp_snapshot_conf_} >> ${MIGRATION_CONF}
    if [ $? -ne 0 ]; then
        _err_msg_="Unable to update ${MIGRATION_CONF} file with snapshot data"
        abort_script "${_err_msg_}"
    fi
else
    _err_msg_="${MIGRATION_CONF} file not found"
    abort_script "${_err_msg_}"
fi

$RM -rf ${_temp_snapshot_conf_}
$RM -rf ${VAR_TMP_DIR}/rollback_conf
}

### Function: save_storage_vlan_info ###
#
#   Get the storage vlan information and store it
#   for interface creation in migration action
#
# Arguments:
#   none 
# Return Values:
#   none
save_storage_vlan_info()
{

# Return if not CO blade
if [ ! "${CO_SERVER}" ]; then
    log_msg -s "Skipping Storage VLAN information for non CO blade" -l ${LOGFILE}
    return 0
fi

log_msg -s "\nGetting Storage VLAN information..." -l ${LOGFILE}

if [ ! "$1" ];then
    _err_msg_="File name is required"
    abort_script "${_err_msg_}"
fi

_temp_stor_file_=$1

_deployment_=`$CAT ${ENIQ_CONF_DIR}${DEPLOYMENT}`
if [ ! "${_deployment_}" ]; then
    _err_msg_="Cannot determine the deployment of server"
    abort_script "_err_msg_"
fi
_stor_interface_="$CAT ${ENIQ_CONF_DIR}/${IPMP_INI} | $GEGREP -A 8 \"\[IPMP_INTF_2\]\" | $GREP "IPMP_Group_Intf" | $CUT -d '=' -f2 | $CUT -d ' ' -f1"
_stor_ip_="$CAT ${ENIQ_CONF_DIR}/${IPMP_INI} | $GEGREP -A 8 \"\[IPMP_INTF_2\]\" | $GREP "IPMP_Group_IP=" | $CUT -d '=' -f2"
_stor_netmask_="$CAT ${ENIQ_CONF_DIR}/${IPMP_INI} | $GEGREP -A 8 \"\[IPMP_INTF_2\]\" | $GREP "IPMP_Group_Netmask" | $CUT -d '=' -f2"

if [ ${_deployment_} != 'ft' ];then
    
    #Get server ip list in the deployment
    $CAT ${ORDER_FILE} | $NAWK -F':' '{print $1}' > ${SERVER_IP_LIST}
    for _ip_ in `cat ${SERVER_IP_LIST}`;
    do
        _host_=`$GETENT hosts ${_ip_} | $AWK -F' ' '{print $2}' | $CUT -d'.' -f1`
        run_remote_cmd "${_host_}" "$LS ${ENIQ_CONF_DIR}/${IPMP_INI}" >> /dev/null 2>&1
        if [ $? -eq 0 ];then
            STOR_INTERFACE=`run_remote_cmd "${_host_}" "${_stor_interface_}"`
            STOR_INTERFACE=`$ECHO $STOR_INTERFACE | $TR -d '\r'`
            if [ ! "${STOR_INTERFACE}" ];then
                _err_msg_="Failed to get interface name for ${_host_}"
                abort_script "${_err_msg_}"
            fi
            set_conf_value STORAGE_INTERFACE_EXIST_${_host_} "YES" ${_temp_stor_file_}
            if [ $? -ne 0 ];then
                _err_msg_="Failed to set interface existence for ${_host_}"
                abort_script "${_err_msg_}"
            fi
            set_conf_value STORAGE_INTERFACE_${_host_} "${STOR_INTERFACE}" ${_temp_stor_file_}
            if [ $? -ne 0 ];then
                _err_msg_="Failed to set interface name for ${_host_}"
                abort_script "${_err_msg_}"
            fi
            STOR_IP=`run_remote_cmd "${_host_}" "${_stor_ip_}"`
            STOR_IP=`$ECHO $STOR_IP | $TR -d '\r'`
            if [ ! "${STOR_IP}" ];then
                _err_msg_="Failed to get storage IP for ${_host_}"
                abort_script "${_err_msg_}"
            fi
            set_conf_value STORAGE_IP_${_host_} "${STOR_IP}" ${_temp_stor_file_}
            if [ $? -ne 0 ];then
                _err_msg_="Failed to set interface IP for ${_host_}"
                abort_script "${_err_msg_}"
            fi
            STOR_NETMASK=`run_remote_cmd "${_host_}" "${_stor_netmask_}"`
            STOR_NETMASK=`$ECHO $STOR_NETMASK | $TR -d '\r'`
            if [ ! "${STOR_NETMASK}" ];then
                _err_msg_="Failed to get Storage Netmask for ${_host_}"
                abort_script "${_err_msg_}"
            fi
            set_conf_value STORAGE_NETMASK_${_host_} "${STOR_NETMASK}" ${_temp_stor_file_}
            if [ $? -ne 0 ];then
                _err_msg_="Failed to set interface netmask for ${_host_}"
                abort_script "${_err_msg_}"
            fi
        else
            _err_msg_="${ENIQ_CONF_DIR}/${IPMP_INI} file not found for ${_host_}"
            abort_script "${_err_msg_}"
        fi
    done
else
    log_msg -s "\nPremigration is triggered on ${_deployment_} deployment." -l ${LOGFILE}
fi

#cleaning temporary files
$RM -rf ${SERVER_IP_LIST}
log_msg -s "\nSuccessfully stored the information of Storage VLAN for all the blade(s)" -l ${LOGFILE}
}

### Function: save_sw_loc_path ###
#
# Save migration info to migration.conf
#
# Arguments:
#   none
# Return Values:
#   none
save_sw_loc_path()
{
log_msg -s "\nGetting the ENIQ Base SW, OM SW and Feature SW paths for migration...\n" -l "${LOGFILE}"

if [ ! "${CO_SERVER}" ];then
    # Check if CO migration conf is created
    if [ -s ${MIGR_CO_CONF} ]; then
        _base_sw_path_=`read_value MIG_BASE_SW_LOC ${MIGR_CO_CONF}` || abort_script "${_base_sw_path_}"
        _om_sw_path_=`read_value MIG_OM_SW_LOC ${MIGR_CO_CONF}` || abort_script "${_om_sw_path_}"
        _feat_sw_path_=`read_value MIG_FEAT_SW_LOC ${MIGR_CO_CONF}` || abort_script "${_feat_sw_path_}"
    fi
fi

if [ ! "${_base_sw_path_}" -o ! "${_om_sw_path_}" ]; then
    # In case of replacement premigration ENIQ will be on same SW
    if [ "${REPLACEMENT}" == "YES" ]; then
        common_save_sw_loc_path "$_temp_migr_conf_" "migration" "curr"
        if [ $? -ne 0 ]; then
            _err_msg_="Unable to save required SW details."
            abort_script "${_err_msg_}"
        fi
    else
        # Ask SW location path info for migration conf file
        common_save_sw_loc_path "$_temp_migr_conf_" "migration" "new"
        if [ $? -ne 0 ]; then
            _err_msg_="Unable to get required SW details from user."
            abort_script "${_err_msg_}"
        fi
    fi
    log_msg -s "ENIQ Base SW and OM SW locations are provided.\n" -l "${LOGFILE}"
else
    # Save SW location path info in migration conf file 
    set_conf_value "MIG_BASE_SW_LOC" ${_base_sw_path_} ${_temp_migr_conf_}
    set_conf_value "MIG_OM_SW_LOC" ${_om_sw_path_} ${_temp_migr_conf_}
    set_conf_value "MIG_FEAT_SW_LOC" ${_feat_sw_path_} ${_temp_migr_conf_}
fi

log_msg -s "ENIQ Base SW, OM SW and Feature SW locations successfully saved.\n" -l "${LOGFILE}"
}

### Function: set_next_stage ###
#
# Set up the stage to be run
#
# Arguments:
#   $1 : Stage to be set to. Either numeric value or last stage of stagefile
# Return Values:
#   none
set_next_stage()
{
# Do I have to reset stage
if [ "${USER_STAGE}" -a "${NO_RESET_STAGE}" ]; then
    return 0
fi

_stage_time_=`$DATE '+%Y-%b-%d_%H.%M.%S'`
$ECHO "# Setting new stage at $_stage_time_" > ${STAGEFILE} | $TEE -a ${LOGFILE}
$ECHO "${ENIQ_CORE_STAGES[$1]} " >> ${STAGEFILE} | $TEE -a ${LOGFILE}
}

### Function: setup_ipmp ###
#
# Set up IPMP.
#
# Arguments:
#   none
# Return Values:
#   none
setup_ipmp()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling setup_ipmp stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s setup_ipmp ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: setup_cron_for_cleanup ###
#
# Sets the cron for cleanup of root backup from ZFS pool
#
# Arguments:
#   none
# Return Values:
#   none
setup_cron_for_cleanup()
{
# Verify if the cronjob for cleanup is already present in crontab
crontab -l | $GREP "eniq_linux_migration.bsh -a cleanup" >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    crontab -l > /tmp/crontab.tmp
    $ECHO "0 0 1 * * $FIND ${MIGR_SUCCESS} -mtime +30 -exec $BASH ${MIGRATION_BIN}/eniq_linux_migration.bsh -a cleanup \;" >> /tmp/crontab.tmp
    crontab /tmp/crontab.tmp
fi

# Verify if the cronjob is added
crontab -l | $GREP "eniq_linux_migration.bsh -a cleanup" >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Failed to add the cronjob in crontab"
    abort_script "$_err_msg_"
fi
}

### Function: setup_env ###
#
# Set up environment variables for script.
#
# Arguments:
#   none
# Return Values:
#   none
setup_env()
{
# Define root user's home
ROOT_HOME=/root/

# Setting the env HOME to /root for console run
export HOME=/root

# ENIQ SSH Directory
SSH_DIR=${ROOT_HOME}.ssh

# ENIQ DCUSER Directory
DCUSER=/eniq/home/dcuser/

# var directory
VAR_DIR=/var

# var tmp directory
VAR_TMP_DIR=${VAR_DIR}/tmp

# /etc/inet directory
ETC_INET_DIR=/etc/inet

# Unpacked Core Software directory for migration
VAR_TMP_UPGRADE=${VAR_DIR}/tmp/upgrade/

# ENIQ Directories
if [ ! "${ENIQ_BASE_DIR}" ]; then
    # Directory on the root filesystem
    ENIQ_BASE_DIR=/eniq
fi

# ENIQ SSH Directory for dcuser
DCUSER_DIR=${ENIQ_BASE_DIR}/home/dcuser/

ENIQ_INST_DIR=${ENIQ_BASE_DIR}/installation
ENIQ_CORE_INST_DIR=${ENIQ_INST_DIR}/core_install
ENIQ_LOG_DIR=${ENIQ_BASE_DIR}/local_logs
ENIQ_CONF_DIR=${ENIQ_INST_DIR}/config
DEPLOYMENT=/extra_params/deployment
ENIQ_DATA_DIR=${ENIQ_BASE_DIR}/data
ENIQ_DATA_MAPPING_DIR=${ENIQ_DATA_DIR}/mapping

# ENIQ Template directory
ENIQ_TEMPL_DIR="${ENIQ_CORE_INST_DIR}/templates/stats"

# Core etc dir
ENIQ_CORE_ETC_DIR=${ENIQ_CORE_INST_DIR}/etc

# ENIQ Admin Directory
ENIQ_ADMIN_DIR=${ENIQ_BASE_DIR}/admin
ENIQ_SENTINEL_DIR=${ENIQ_BASE_DIR}/sentinel
ENIQ_BACKUP_DIR=${ENIQ_BASE_DIR}/backup

# Admin bin dir
ENIQ_ADMIN_BIN_DIR=${ENIQ_ADMIN_DIR}/bin

# ENIQ Core install script
ENIQ_CORE_INST_SCRIPT=${ENIQ_CORE_INST_DIR}/bin/eniq_core_install.bsh

# ENIQ Log Directory 
LOG_DIR=${ENIQ_BASE_DIR}/log 
SW_LOG_DIR=${LOG_DIR}/sw_log 

# ENIQ SW conf directory
CLI_CONF_DIR=${ENIQ_BASE_DIR}/sw/conf

# ENIQ Bkup SW dir 
ENIQ_BKUP_SW_DIR=${ENIQ_BASE_DIR}/bkup_sw 
ENIQ_BKUP_SW_BIN_DIR=${ENIQ_BKUP_SW_DIR}/bin 

# Sentinel dir
ENIQ_SENTINEL_BIN_DIR=${ENIQ_SENTINEL_DIR}/bin
ENIQ_SENTINEL_ENV=${ENIQ_SENTINEL_DIR}/etc/sentinel.env

# Set the log for Create Snapshots stage
SNAPSHOT_LOGFILE_DIR=${SW_LOG_DIR}/rolling_snapshot_logs
SNAPSHOT_LOGFILE=${SNAPSHOT_LOGFILE_DIR}/prep_eniq_snapshots.log

# Migration Directories
MIGRATION_CORE=`$DIRNAME ${SCRIPTHOME}`
if [ "${ACTION_TYPE}" == "premigration" -o "${ACTION_TYPE}" == "recovery" -o "${ACTION_TYPE}" == "cleanup" ]; then
    MIGRATION_LIB=${ENIQ_CORE_INST_DIR}/lib
fi
MIGRATION_HOME=`$DIRNAME ${MIGRATION_CORE}`

MIGRATION_BIN=${MIGRATION_CORE}/bin
MIGRATION_ETC=${MIGRATION_CORE}/etc
MIGRATION_LOGDIR=${MIGRATION_HOME}/log

# ERICSSON Directory
ERICSSON_DIR=/ericsson 
ERICSSON_STOR_DIR=${ERICSSON_DIR}/storage
ERICSSON_BIN_DIR=${ERICSSON_STOR_DIR}/bin
ERICSSON_SAN_PLUGINS_DIR=${ERICSSON_STOR_DIR}/san/plugins
ERICSSON_STOR_PLUGIN_DIR=${ERICSSON_STOR_DIR}/plugins
ERICSSON_FILESTOR_ETC=${ERICSSON_STOR_PLUGIN_DIR}/filestore/etc

# Navisphere bin directory
NAVISPHERE=/opt/Navisphere/bin

# ENIQ Crontab Directory
CRONTABS_DIR=${VAR_DIR}/spool/cron/crontabs

# Hostname Information
HNAME=`${MYHOSTNAME}`
RD1_HNAME=`$CAT /etc/hosts|$GREP -iw dwh_reader_1|$AWK '{print $2}'`
RD2_HNAME=`$CAT /etc/hosts|$GREP -iw dwh_reader_2|$AWK '{print $2}'`
HOST_IP=`$GETENT hosts ${HNAME} | $AWK '{print $1}' | $HEAD -1`

# Source the common migration functions
if [ -s ${SCRIPTHOME}/../lib/common_migration_functions.lib ]; then
    . ${SCRIPTHOME}/../lib/common_migration_functions.lib
else
    _err_msg_="File ${SCRIPTHOME}/../lib/common_migration_functions.lib not found"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Source the common functions
_common_functions_list_="common_functions.lib common_core_install_functions.lib"
for lib_file in ${_common_functions_list_}; do
    if [ -s ${MIGRATION_LIB}/${lib_file} ]; then
        . ${MIGRATION_LIB}/${lib_file}
    else
        _err_msg_="File ${MIGRATION_LIB}/${lib_file} not found"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
done

# File to hold stage information
if [ "${ACTION_TYPE}" == "premigration" ]; then
    STAGEFILE=${MIGRATION_ETC}/eniq_linux_migr_stage
else
   STAGEFILE=${MIGRATION_ETC}/eniq_linux_recov_stage
fi

# Migration status files
MIGR_PROGRESS=${VAR_DIR}/tmp/solaris_${ACTION_TYPE}_in_progress
MIGR_SUCCESS=${VAR_DIR}/tmp/solaris_${ACTION_TYPE}_success

#Server host/ip temp list
SERVER_IP_LIST="/tmp/server_ip_list"

# Check config dir is present
if [ ! -d "${ENIQ_CONF_DIR}" ]; then
    _err_msg_="${ENIQ_CONF_DIR} is required for $ACTIVITY"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Determine SAN Device type if raw
if [ "${STORAGE_TYPE}" == "raw" ];then
    SAN_DEVICE=`$CAT ${ENIQ_CONF_DIR}/san_details | $EGREP "^SAN_DEVICE=" | $AWK -F\= '{print $2}'`
    if [ ! "${SAN_DEVICE}" ]; then
        _err_msg_="Could not read SAN_DEVICE type from ${ENIQ_CONF_DIR}/san_details."
        abort_script "${_err_msg_}"
    fi
fi

# File containing the type of installation. Eg. statistics
INST_TYPE_FILE=${ENIQ_CONF_DIR}/ericsson_use_config
if [ ! -s "${INST_TYPE_FILE}" ]; then
    _err_msg_="ENIQ install type not defined in ${INST_TYPE_FILE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi
# Read the installation type - should be "stats"
INSTALL_TYPE=`$CAT ${INST_TYPE_FILE} | $AWK -F\= '{print $2}'`

# Templates Directory
MIGRATION_TEMPL_DIR="${MIGRATION_CORE}/templates/${INSTALL_TYPE}"

# Get current server type
CURR_SERVER_TYPE=`$CAT ${ENIQ_CONF_DIR}/installed_server_type | $EGREP -v  '^[[:blank:]]*#' | $SED -e 's/ //g'`
if [ ! "${CURR_SERVER_TYPE}" ]; then
    _err_msg_="Could not determine which server type this is"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Get CO server hostname
SERVICE_NAME_FILE=${ENIQ_CONF_DIR}/service_names
if [ ! -f "${SERVICE_NAME_FILE}" ]; then
    _err_msg_="${SERVICE_NAME_FILE} not found on server."
    abort_script "${_err_msg_}"
fi
CO_HOSTNAME=`$CAT ${SERVICE_NAME_FILE} | $GREP "dwhdb$" | $NAWK -F"::" '{print $2}'`

# Migration config file
MIGRATION_CONF_FILE=linux_migration_${HNAME}.conf
MIGRATION_CONF=${ENIQ_CONF_DIR}/${MIGRATION_CONF_FILE}

# Check if server is coordinator type
CO_SERVER=""
if [ "${CURR_SERVER_TYPE}" == "stats_coordinator" -o "${CURR_SERVER_TYPE}" == "eniq_stats" ]; then 
    CO_SERVER="YES"
fi

# Check if server is Reader type
RD_SERVER=""
if [ "${CURR_SERVER_TYPE}" == "stats_iqr" ]; then
    RD_SERVER="YES"
fi

# Zpool backup directory
ZPOOL_FOR_BACKUP=eniq_sp_1
BACKUP_DIR=${ZPOOL_FOR_BACKUP}/migration_root_backup

# Config files backup directory
CONFIG_BACKUP_DIR=${ENIQ_CONF_DIR}/conf_bkup_migration

# Migration SW backup directory
MIG_SW_DIR_NAME=migration_sw
MIG_SW_BACKUP_DIR=${ENIQ_BACKUP_DIR}/${MIG_SW_DIR_NAME}
MIGR_CO_CONF=${MIG_SW_BACKUP_DIR}/linux_migration_${CO_HOSTNAME}.conf

# Configuration flags
MIGRATE_CO=${VAR_DIR}/tmp/migrate_co

# Migration Backup directory
MIGRATION_BACKUP_DIR=/eniq/portbackup

# Getting the NAS pool ID from storage.ini file
SYS_ID=`iniget Storage_NAS_GENERAL -f ${ENIQ_CONF_DIR}/${STORAGE_INI} -v SYS_ID` 
if [ ! "${SYS_ID}" ]; then
    _err_msg_="Could not determine SYS_ID from ${ENIQ_CONF_DIR}/${STORAGE_INI}"
    abort_script "${_err_msg_}"
fi

#Directory path of Migration backup  
MIG_BACKUP_NAS_DIR=/vx/${SYS_ID}-portbackup

# Getting NAS API target directory
_nas_sw_target_dir_=`iniget NAS_STORAGE_API -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v NAS_API_TARGET_DIR`
if [ ! "${_nas_sw_target_dir_}" ]; then
    _err_msg_="Failed to get NAS API target dir information from ${ENIQ_CONF_DIR}/${SUNOS_INI}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Defining NASCLI utility
_nascli_=${_nas_sw_target_dir_}/bin/nascli

# Read location of storage API command
_stor_api_cmd_=`iniget STOR_API -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v STOR_API_CMD`
if [ ! "${_stor_api_cmd_}" ]; then
    _err_msg_="Could not read STOR_API_CMD param from ${ENIQ_CONF_DIR}/${SUNOS_INI}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Script to delete ini blocks from .ini files
if [ ! -s ${SCRIPTHOME}/../lib/inidel.pl ]; then
    _err_msg_="Cannot locate ${SCRIPTHOME}/../lib/inidel"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
else
    INIDEL=${SCRIPTHOME}/../lib/inidel.pl
fi

# NAS rollback success file
NAS_RECOVERY_SUCCESS=${VAR_TMP_DIR}/nas_${ACTION_TYPE}_success

# SAN rollback success file
SAN_RECOVERY_SUCCESS=${VAR_TMP_DIR}/san_${ACTION_TYPE}_success

# Flag file to check snapshot is created through migration script
CALLED_THROUGH_UPGRADE=${ENIQ_ADMIN_BIN_DIR}/called_through_upgrade

# Configuration files
VFSTAB=vfstab
ETC_DIR=/etc

}

### Function: setup_SMF_scripts ###
#
# Set up SMF start/stop scripts
#
# Arguments:
#   none
# Return Values:
#   none
setup_SMF_scripts()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling setup_SMF_scripts stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s setup_SMF_scripts ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: setup_update_disp_file ###
#
# Set up a file to display out for update details
#
# Arguments:
#   none
# Return Values:
#   none
setup_update_disp_file()
{
# Set up a file to display out
$RM -f ${TEM_DIR}/disp_file

_update_=0

if [ -s ${ENIQ_CORE_ETC_DIR}/features_to_be_managed ]; then
    $ECHO "\nENIQ Features" >> ${TEM_DIR}/disp_file
    $ECHO "=============" >> ${TEM_DIR}/disp_file
    $CAT ${TEM_DIR}/feature_output_list2 >> ${TEM_DIR}/disp_file
    $ECHO "\nDo you wish to continue to update the features above (Yy/Nn)\n" >> ${TEM_DIR}/disp_file
    _update_=1
fi

# If there is nothing to upgrade then delete the disp file
if [ ${_update_} -eq 0 ]; then
    $RM -f ${TEM_DIR}/disp_file
fi
}

### Function: show_server_info ###
#
#   Display current server info
#
# Arguments:
#   none
# Return Values:
#   none
show_server_info()
{
_system_config_=`/usr/sbin/prtdiag | $HEAD -1`
_system_release_=`cat /etc/release | $HEAD -1 | $XARGS`

log_msg -s "******* Current System Information *********" -l ${LOGFILE}
$ECHO "=============================================="
log_msg -s "${_system_config_}" -l ${LOGFILE}
log_msg -s "OS Version:   ${_system_release_}" -l ${LOGFILE}

$ECHO "=============================================="
}


### Function: start_eniq_services ###
#
# Start all ENIQ services
#
# Arguments:
#   none
# Return Values:
#   none
start_eniq_services()
{
# Ensure NASd is online
if [ "${STORAGE_TYPE}" == "raw" ]; then
    check_and_manage_smf ${NASd_SMF_ID} enable
fi

# Ensure licensing service is online
if [ "${CO_SERVER}" ];then
    check_and_manage_smf ${SENTINEL_SMF_ID} enable
fi

# Enable all the ENIQ services
log_msg -l ${LOGFILE} -s "Starting the ENIQ services on $HNAME. Please wait..."
$BASH ${ENIQ_ADMIN_DIR}/bin/manage_eniq_services.bsh -a start -s ALL -N >> ${LOGFILE}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to start ENIQ services."
    abort_script "$_err_msg_"
fi
}

### Function: stop_eniq_services ###
#
# Stop all ENIQ services
#
# Arguments:
#   none
# Return Values:
#   none
stop_eniq_services()
{
# Ensure NASd is online
if [ "${STORAGE_TYPE}" == "raw" ]; then
    check_and_manage_smf ${NASd_SMF_ID} enable
fi

# Disable all the ENIQ services
log_msg -l ${LOGFILE} -s "Stopping the ENIQ services on $HNAME. Please wait..."
$BASH ${ENIQ_ADMIN_DIR}/bin/manage_eniq_services.bsh -a stop -s ALL -N >> ${LOGFILE}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to stop ENIQ services."
    abort_script "$_err_msg_"
fi
}

### Function: stop_smf_services ###
#
# Stop all SMF services
#
# Arguments:
#   none
# Return Values:
#   none
stop_smf_services()
{
# Get the SMF list to disable
smf_list="${SENTINEL_SMF_ID} ${DDC_SMF_ID} ${HOSTSYNC_SMF_ID} ${AUTO_LU_SMF_ID}"
if [ "${STORAGE_TYPE}" == "raw" ]; then
    # Disable NAS, hostsync, ddc and auto_lu services
    smf_list="${smf_list} ${NASd_SMF_ID}"
else
    smf_list="${smf_list} ${CRON_SMF_ID}"
fi

for _smf_ in ${smf_list}; do
    $SVCS -H ${_smf_} >> /dev/null 2>&1
    if [ $? -ne 0 ];then
        continue
    fi
    log_msg -l ${LOGFILE} -s "Disabling service ${_smf_}"
    check_and_manage_smf ${_smf_} disable
    if [ $? -ne 0 ]; then
        _err_msg_="Failed to stop SMF service ${_smf_}."
        abort_script "$_err_msg_"
    fi
done
}

### Function: update_config_files ###
#
#   Update ENIQ config files with new values
#   after OS migration/recovery
#
# Arguments:
#   none
# Return Values:
#   none
update_config_files()
{
_conf_bkup_dir_=${CONFIG_BACKUP_DIR}

# Update SunOS.ini with disk_layout
# Create a temporary copy
$RM -rf ${TEM_DIR}/SunOS.ini
$CP -p ${ENIQ_CONF_DIR}/SunOS.ini ${TEM_DIR}/SunOS.ini

local _pool_list_ _pool_name_ _pool_disk_ _new_pool_disk_
 _pool_list_=`iniget SunOS_ZFS_POOL -f ${ENIQ_CONF_DIR}/SunOS.ini`

for _pool_ in $_pool_list_ ; do
    _pool_name_=`iniget ${_pool_} -f ${ENIQ_CONF_DIR}/SunOS.ini -v name`
    _pool_disk_=`iniget ${_pool_} -f ${ENIQ_CONF_DIR}/SunOS.ini -v disk_layout`
    _new_pool_disk_=`$ZPOOL status $_pool_name_ | $EGREP -e 'c[0-9].*t[0-9].*' | $AWK '{print $1}'`
    if [ "${STORAGE_TYPE}" == "zfs" ];then
        _new_pool_disk_=`$ECHO ${_new_pool_disk_}`
    fi
    if [ "${_pool_disk_}" -a "${_new_pool_disk_}" ]; then
        if [ "${_pool_disk_}" != "${_new_pool_disk_}" ]; then
            log_msg -q -s "Updating pool ${_pool_} with disk value ${_new_pool_disk_}." -l ${LOGFILE}
            if [ "${STORAGE_TYPE}" == "raw" ];then
                iniset ${_pool_} -f ${TEM_DIR}/SunOS.ini disk_layout=$_new_pool_disk_
            else
                iniset ${_pool_} -f ${TEM_DIR}/SunOS.ini disk_layout="$_new_pool_disk_"
            fi 
            if [ $? -ne 0 ];then
                _err_msg_="Unable to update disk name of zpool ${_pool_name_}."
                $CP -p ${_conf_bkup_dir_}/SunOS.ini ${ENIQ_CONF_DIR}/SunOS.ini
                abort_script "${_err_msg_}"
            fi
        else
            log_msg -s "Pool ${_pool_name_} disk name is updated.\n" -l ${LOGFILE}
        fi
    else
        _err_msg_="Unable to get disk name for zpool ${_pool_name_}."
        $CP -p ${_conf_bkup_dir_}/SunOS.ini ${ENIQ_CONF_DIR}/SunOS.ini
        abort_script "${_err_msg_}"
    fi
done
$MV ${TEM_DIR}/SunOS.ini ${ENIQ_CONF_DIR}/SunOS.ini
log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/SunOS.ini with zpool disks.\n" -l ${LOGFILE}

# Update dumpadm_device with dumpadm device name
if [ -f "${ENIQ_CONF_DIR}/dumpadm_device" ]; then
    _file_dump_dev_=`$CAT ${ENIQ_CONF_DIR}/dumpadm_device`
    if [ ! "${SOLARIS_10}" ];then
        _dump_dev_=`$DUMPADM | $EGREP "[[:blank:]]*Dump device[[:blank:]]*\:" | $AWK '{print $4}'`
    else
        _dump_dev_=`$DUMPADM | $EGREP "[[:blank:]]*Dump device[[:blank:]]*\:" | $AWK '{print $3}'`
    fi
    if [ ! "${_dump_dev_}" ];then
        err_msg_="Unable to find dump device from system."
        abort_script "${_err_msg_}"
    fi

    if [ "${_file_dump_dev_}" != "${_dump_dev_}" ]; then
        $RM -rf ${TEM_DIR}/dumpadm_device
        log_msg -q -s "Updating ${_file_dump_dev_} value with ${_dump_dev_}." -l ${LOGFILE}
        $CAT ${ENIQ_CONF_DIR}/dumpadm_device | $SED -e "s|${_file_dump_dev_}|${_dump_dev_}|" > ${TEM_DIR}/dumpadm_device
        $MV ${TEM_DIR}/dumpadm_device ${ENIQ_CONF_DIR}/dumpadm_device
        log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/dumpadm_device with dumpadm device name.\n" -l ${LOGFILE}
    else
        log_msg -s "Dump device name is already updated.\n" -l ${LOGFILE}
    fi
fi

# Update rdisk_config with root disk name
if [ -f "${ENIQ_CONF_DIR}/rdisk_config" ]; then
    $RM -f ${TEM_DIR}/new_rdisk
    if [ ! "${SOLARIS_10}" ];then
        $ZPOOL status ${ROOT_POOL} | $EGREP -e 'c[0-9].*t[0-9].*' -e 'c[0-9].*d[0-9].*' | $AWK '{print $1}' > ${TEM_DIR}/new_rdisk
    else
        _rdisk_name_=`$DF -hk / | $GREP -v "^Filesystem" | $AWK '{print $1}'`
        _rdisk_md_=`$ECHO ${_rdisk_name_} | $EGREP '^\/dev\/md\/' | $AWK -F\/ '{print $NF}'`
        $METASTAT ${_rdisk_md_} | $EGREP '^[\/dev\/dsk\/]*c[0-9]+(t|d).*[        ]' | $AWK '{print $1}' | $AWK -F\/ '{print $NF}' > ${TEM_DIR}/new_rdisk
    fi

    if [ ! -s "${TEM_DIR}/new_rdisk" ];then
        err_msg_="Unable to find root disk name from system."
        abort_script "${_err_msg_}"
    fi

    $DIFF ${ENIQ_CONF_DIR}/rdisk_config ${TEM_DIR}/new_rdisk >>/dev/null 2>&1
    if [ $? -ne 0 ]; then
        log_msg -q -s "Updating ${ENIQ_CONF_DIR}/rdisk_config file." -l ${LOGFILE}
        $CP ${TEM_DIR}/new_rdisk ${ENIQ_CONF_DIR}/rdisk_config
        log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/rdisk_config with root disk name.\n" -l ${LOGFILE}
    else
        log_msg -s "Root disk name already updated.\n" -l ${LOGFILE}
    fi
fi


# Update swap_device with swap location name
if [ -f "${ENIQ_CONF_DIR}/swap_device" ]; then
    _file_swap_dev_=`$CAT ${ENIQ_CONF_DIR}/swap_device`
    if [ ! "${SOLARIS_10}" ];then
        _swap_dev_=`$SWAP -l | $GREP "/dev/zvol/dsk/rpool" | $AWK '{print $1}'`
    else
        _swap_dev_=`$SWAP -l | $EGREP -v '^swapfile|\/dev\/zvol' | $AWK '{print $1}'`
    fi
    if [ ! "${_swap_dev_}" ];then
        err_msg_="Unable to find swap device name from system."
        abort_script "${_err_msg_}"
    fi

    if [ "${_file_swap_dev_}" != "${_swap_dev_}" ]; then
        $RM -rf ${TEM_DIR}/swap_device
        log_msg -q -s "Updating ${_file_swap_dev_} value with ${_swap_dev_}." -l ${LOGFILE}
        $CAT ${ENIQ_CONF_DIR}/swap_device | $SED -e "s|${_file_swap_dev_}|${_swap_dev_}|" > ${TEM_DIR}/swap_device
        $MV ${TEM_DIR}/swap_device ${ENIQ_CONF_DIR}/swap_device
        log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/swap_device with swap device name.\n" -l ${LOGFILE}
    else
        log_msg -s "Swap device name is already updated.\n" -l ${LOGFILE}
    fi
fi


# Update eniq_sw_locate with new ENIQ SW location information
    if [ -f "${ENIQ_CONF_DIR}/eniq_sw_locate" ]; then
        _file_sw_loc_=`$CAT ${ENIQ_CONF_DIR}/eniq_sw_locate`
    fi
    _sw_loc_=`$ECHO $BASE_SW_DIR | $CUT -d/ -f3- | $SED 's/\//@\//1' | $SED 's/\/eniq_base_sw//'`
    if [ ! "${_sw_loc_}" ];then
        err_msg_="Unable to find ENIQ SW location information."
        abort_script "${_err_msg_}"
    fi

    if [ "${_file_sw_loc_}" != "${_sw_loc_}" ]; then
        $RM -rf ${TEM_DIR}/eniq_sw_locate
        log_msg -q -s "Updating ENIQ SW path with ${_sw_loc_}." -l ${LOGFILE}
        if [ -f "${ENIQ_CONF_DIR}/eniq_sw_locate" ]; then
            $CAT ${ENIQ_CONF_DIR}/eniq_sw_locate | $SED -e "s|${_file_sw_loc_}|${_sw_loc_}|" > ${TEM_DIR}/eniq_sw_locate
            $MV ${TEM_DIR}/eniq_sw_locate ${ENIQ_CONF_DIR}/eniq_sw_locate
            log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/eniq_sw_locate with ENIQ SW location.\n" -l ${LOGFILE}
        else
            $ECHO ${_sw_loc_} > ${ENIQ_CONF_DIR}/eniq_sw_locate
            log_msg -s "Successfully created ${ENIQ_CONF_DIR}/eniq_sw_locate with ENIQ SW location.\n" -l ${LOGFILE}
        fi
    else
        log_msg -s "ENIQ SW location is already updated.\n" -l ${LOGFILE}
    fi


# Update om_sw_locate with new OM SW location information
    if [ -f "${ENIQ_CONF_DIR}/om_sw_locate" ]; then
        _file_om_loc_=`$CAT ${ENIQ_CONF_DIR}/om_sw_locate`
    fi
    _om_loc_=`$ECHO $OM_SW_DIR | $CUT -d/ -f3- | $SED 's/\//@\//1' | $SED 's/\/om//'`
    if [ ! "${_om_loc_}" ];then
        err_msg_="Unable to find OM SW location information."
        abort_script "${_err_msg_}"
    fi

    if [ "${_file_om_loc_}" != "${_om_loc_}" ]; then
        $RM -rf ${TEM_DIR}/om_sw_locate
        log_msg -q -s "Updating OM SW path with ${_om_loc_}." -l ${LOGFILE}
        if [ -f "${ENIQ_CONF_DIR}/om_sw_locate" ]; then
            $CAT ${ENIQ_CONF_DIR}/om_sw_locate | $SED -e "s|${_file_om_loc_}|${_om_loc_}|" > ${TEM_DIR}/om_sw_locate
            $MV ${TEM_DIR}/om_sw_locate ${ENIQ_CONF_DIR}/om_sw_locate
            log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/om_sw_locate with OM SW location.\n" -l ${LOGFILE}
        else
            $ECHO ${_om_loc_} > ${ENIQ_CONF_DIR}/om_sw_locate
            log_msg -s "Successfully created ${ENIQ_CONF_DIR}/om_sw_locate with OM SW location.\n" -l ${LOGFILE}
       fi
    else
        log_msg -s "OM SW location is already updated.\n" -l ${LOGFILE}
    fi

# Update disks_labeled file with re-named disks

    if [ -f ${TEM_DIR}/disks_labeled ]; then
        $RM -rf ${TEM_DIR}/disks_labeled
    fi

    log_msg -q -s "Updating ZFS disk names in disks_labeled file from ${ENIQ_CONF_DIR}/SunOS.ini." -l ${LOGFILE}
    local _iq_sys_main_disk_list_ _main_db_disk_list_ _temp_db_disk_list_ _zpool_disk_list_
    _zpool_disk_list_=`iniget SunOS_ZFS_POOL -f ${ENIQ_CONF_DIR}/SunOS.ini`

    for _zpool_ in ${_zpool_disk_list_} ; do
        _zpool_disk_=`iniget ${_zpool_} -f ${ENIQ_CONF_DIR}/SunOS.ini -v disk_layout`
        $ECHO ${_zpool_disk_} | $TR ' ' '\n' >> ${TEM_DIR}/disks_labeled
    done

    # Skipping this step for rack
    if [ "${STORAGE_TYPE}" == "raw" ];then
        _iq_sys_main_disk_list_=`iniget DB_DISK_ALLOC -f ${ENIQ_CONF_DIR}/${SYM_LINKS_INI} -v IQ_SYS_MAIN_DISKS`
        _main_db_disk_list_=`iniget DB_DISK_ALLOC -f ${ENIQ_CONF_DIR}/${SYM_LINKS_INI} -v MAINDB_DISKS`
        _temp_db_disk_list_=`iniget DB_DISK_ALLOC -f ${ENIQ_CONF_DIR}/${SYM_LINKS_INI} -v TEMPDB_DISKS`
        log_msg -q -s "Updating disk names in disks_labeled file from ${ENIQ_CONF_DIR}/${SYM_LINKS_INI}." -l ${LOGFILE}

        if [ "${CO_SERVER}" ]; then
            _db_label_disk_list_="${_iq_sys_main_disk_list_} ${_main_db_disk_list_} ${_temp_db_disk_list_}"
        elif [ "${RD_SERVER}" ]; then
            _db_label_disk_list_="${_temp_db_disk_list_}"
        fi
        $ECHO  ${_db_label_disk_list_} | $TR ' ' '\n' >>  ${TEM_DIR}/disks_labeled
    fi
    $MV ${TEM_DIR}/disks_labeled ${ENIQ_CONF_DIR}/disks_labeled
    log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/disks_labeled with disk names.\n" -l ${LOGFILE}
}


### Function: update_core_scripts ###
#
#   Update the core SW scripts to make them
#   compatible to handle Solaris 11
#
# Arguments:
#   none
# Return Values:
#   none
update_core_scripts()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Skip if premigration on CO is not selected in case of multiblade replacement
if [ "${CO_SERVER}" ]; then
    if [ ! -f ${MIGRATE_CO} ]; then
        insert_header_footer foot "Skipping replacement stage on ${CURR_SERVER_TYPE} - ${NEXT_STAGE}" ${LOGFILE}
        set_next_stage `$EXPR ${ARRAY_ELEM}+1`
        return 0
    fi
fi

log_msg -s "\nStarting to copy necessary core scripts for Migration." -l ${LOGFILE}
copy_new_sw
if [ $? -ne 0 ]; then
    _err_msg_="Error occurred during updating ENIQ core scripts."
    abort_script "${_err_msg_}"
fi
log_msg -s "\nSuccessfully updated core SW scripts." -l ${LOGFILE}

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: update_cron_files ###
#
#   Update the cron files from nas
#   
# Arguments:
#   none
# Return Values:
#   none
update_cron_files()
{
insert_header_footer head "Entering Linux ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}
# Mounting portbackup from nas
mount_nas_shares

# Restoring root cron entries from portbackup
_root_cronlist=${_mountpoint_}/${HNAME}/ROOT${CRONTABS_DIR}/root
log_msg -s "\nRestoring root cron entries from portbackup." -l ${LOGFILE}
$CP -p ${_root_cronlist} ${CRONTABS_DIR}/
    if [ ! -s ${CRONTABS_DIR}/root ]; then
        _err_msg_="File ${CRONTABS_DIR}/root is empty."
        abort_script "${_err_msg_}"
    fi

# Restoring dcuser cron entries from portbackup
_dcuser_cronlist_=${_mountpoint_}/${HNAME}/ROOT${CRONTABS_DIR}/dcuser
if [ -s ${_dcuser_cronlist_} ]; then
    log_msg -s "\nRestoring dcuser cron entries from portbackup." -l ${LOGFILE}
    $CP -p ${_dcuser_cronlist_} ${CRONTABS_DIR}/
    if [ ! -s ${CRONTABS_DIR}/dcuser ]; then
        _err_msg_="File ${CRONTABS_DIR}/dcuser is empty."
        abort_script "${_err_msg_}"
    fi
fi
log_msg -s "\nSuccessfully updated cron files." -l ${LOGFILE}

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}


### Function: update_ENIQ_env_files ###
#
# Update ENIQ env file
#
# Arguments:
#   none
# Return Values:
#   none
update_ENIQ_env_files()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling update_ENIQ_env_files stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s update_ENIQ_env_files -u ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ];then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: update_defaultrouter_file ###
#
# Updates /etc/defaultrouter file
#
# Arguments:
#   none
# Return Values:
#   none
update_defaultrouter_file()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling update_defaultrouter_file stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s update_defaultrouter_file ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: update_dns_files ###
#
# Updates DNS files
#
# Arguments:
#   none
# Return Values:
#   none
update_dns_files()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling update_dns_files stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s update_dns_files ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: update_ini_file ###
#
# Update ini files e.g. SunOS.ini
#
# Arguments:
#   none
# Return Values:
#   none
#
update_ini_file()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Remove SunOS.ini file from temporary directory
$RM -rf ${TEM_DIR}/SunOS.ini

# Get SAN device name
SAN_DEVICE_NAME=`read_value STORAGE_NAME_1 ${MIGRATION_CONF}` || abort_script "${SAN_DEVICE_NAME}" "${EXEC_SHELL_CMD}"

# Get LUN ID for eniq_sp_1 zpool
ENIQ_SP_LUN_ID=`read_value ENIQ_SP_LUN_ID ${MIGRATION_CONF}` || abort_script "${ENIQ_SP_LUN_ID}" "${EXEC_SHELL_CMD}"

# Get LUN ID for server specific zpool
ENIQ_STATS_LUN_ID=`read_value ENIQ_STATS_LUN_ID ${MIGRATION_CONF}` || abort_script "${ENIQ_STATS_LUN_ID}" "${EXEC_SHELL_CMD}"

# Getting the new controller value for eniq_sp pool
_ctrl_val_eniq_sp_pool_=`${_stor_api_cmd_} --action listluns | $GREP -w "${SAN_DEVICE_NAME}@${ENIQ_SP_LUN_ID}" | $AWK -F";" '{print $2}'`
 
# Getting the new controller value for server specific pool
_ctrl_val_stats_pool_=`${_stor_api_cmd_} --action listluns | $GREP -w "${SAN_DEVICE_NAME}@${ENIQ_STATS_LUN_ID}" | $AWK -F";" '{print $2}'`

# Getting the old controller value for eniq_sp pool
_eniq_sp_disk_layout_=`iniget SunOS_ZFS_POOL_1 -f ${ENIQ_CONF_DIR}/SunOS.ini -v disk_layout`

# Getting the old controller value for server specific pool
_eniq_stats_disk_layout_=`iniget SunOS_ZFS_POOL_2 -f ${ENIQ_CONF_DIR}/SunOS.ini -v disk_layout`

# Removing and Updating disks_labeled file with new LUNs
$RM -rf ${TEM_DIR}/disks_labeled
$CAT ${ENIQ_CONF_DIR}/disks_labeled | $SED -e "s|${_eniq_sp_disk_layout_}|${_ctrl_val_eniq_sp_pool_}|" | $SED -e "s|${_eniq_stats_disk_layout_}|${_ctrl_val_stats_pool_}|" > ${TEM_DIR}/disks_labeled
$MV ${TEM_DIR}/disks_labeled ${ENIQ_CONF_DIR}/disks_labeled >> /dev/null 2>&1
if [ $? -ne 0 ];then
    _err_msg_="Failed to update the ${ENIQ_CONF_DIR}/disks_labeled file"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

log_msg -t -s "Updating the disk layout value for both zpools in the ${ENIQ_CONF_DIR}/SunOS.ini file" -l ${LOGFILE}

# Updating disk layout value for eniq_sp_1 zpool
iniset SunOS_ZFS_POOL_1 -f ${ENIQ_CONF_DIR}/SunOS.ini  disk_layout="${_ctrl_val_eniq_sp_pool_}"
if [ $? -ne 0 ]; then
    _err_msg_="Failed to update the disk layout value for eniq_sp_1 zpool in the ${ENIQ_CONF_DIR}/SunOS.ini file"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Updating disk layout value for server specific zpool
iniset SunOS_ZFS_POOL_2 -f ${ENIQ_CONF_DIR}/SunOS.ini  disk_layout="${_ctrl_val_stats_pool_}"
if [ $? -ne 0 ]; then
     _err_msg_="Failed to update the disk layout value for server specific zpool in the ${ENIQ_CONF_DIR}/SunOS.ini file"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# Remove installation, smf, local_logs from zfs file list in SunOS.ini
log_msg -s "Getting list of required fs filesystems from ${ENIQ_TEMPL_DIR}/${SUNOS_INI}_${STORAGE_TYPE}\n" -l ${LOGFILE}
_fs_list_=`iniget SunOS_ZFS_FS -f ${ENIQ_CONF_DIR}/${SUNOS_INI}`
for _fs_ in ${_fs_list_}; do
    _fs_name_=`iniget ${_fs_} -f ${ENIQ_TEMPL_DIR}/${SUNOS_INI}_${STORAGE_TYPE} -v name | awk -F"/" '{print $2}'`
    _fs_name_1_=`iniget ${_fs_} -f ${ENIQ_CONF_DIR}/${SUNOS_INI} -v name | awk -F"/" '{print $2}'`
    if [ "${_fs_name_}" != "${_fs_name_1_}" ];then
        ${INIDEL} -g SunOS_ZFS_FS -p ${_fs_} -i ${ENIQ_CONF_DIR}/SunOS.ini -o ${TEM_DIR}/SunOS.ini
        $MV ${TEM_DIR}/SunOS.ini ${ENIQ_CONF_DIR}/SunOS.ini
    fi
done

log_msg -s "Successfully removed installation, smf, local_logs from zfs file list in ${ENIQ_CONF_DIR}/SunOS.ini.\n" -l ${LOGFILE}

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

### Function: update_ipmp_ini ###
#
#   Update ipmp.ini for migration
#
# Arguments:
#   none
# Return Values:
#   none
update_ipmp_ini()
{
# Make a temp copy of ipmp.ini
log_msg -s "\nCopying ${ENIQ_CONF_DIR}/${IPMP_INI} to ${TEM_DIR}/${IPMP_INI}" -l ${LOGFILE}
$CP -p ${ENIQ_CONF_DIR}/${IPMP_INI} ${TEM_DIR}/${IPMP_INI}
if [ $? -ne 0 ];then
    _err_msg_="Could not copy ${ENIQ_CONF_DIR}/${IPMP_INI} to ${TEM_DIR}/${IPMP_INI}"
    abort_script "${_err_msg_}"
fi

_update_intf_=0

# Get list of interfaces available
log_msg -s "\nGetting list of interfaces available on current server." -l ${LOGFILE}
if [ ! "${SOLARIS_10}" ];then
    $DLADM show-link -p -o LINK | $SORT -u > ${TEM_DIR}/interface_list
else
    $DLADM show-dev | $AWK '{print $1}' > ${TEM_DIR}/interface_list
fi

if [ ! -s ${TEM_DIR}/interface_list ];then
    _err_msg_="Could not fetch available interfaces."
    abort_script "${_err_msg_}"
fi

# Get configured interface names
log_msg -s "\nGetting configured interface names from ${IPMP_INI}." -l ${LOGFILE}
# Set interface list according to deployment type
_depl_type_=`$CAT ${ENIQ_CONF_DIR}${DEPLOYMENT}`
if [ "${_depl_type_}" == "ft" ];then
    _intf_list_=`iniget IPMP -f ${TEM_DIR}/${IPMP_INI} | $HEAD -1`
else
    _intf_list_=`iniget IPMP -f ${TEM_DIR}/${IPMP_INI}`
fi

for _name_ in ${_intf_list_}
do
    _ipmp_group_intf_=`iniget ${_name_} -f ${TEM_DIR}/${IPMP_INI} -v IPMP_Group_Intf`
    _intf_count_=`$ECHO ${_ipmp_group_intf_} | $WC -w | $SED 's/ //g'`
    if [ $_intf_count_ -eq 1 ];then
        _primary_intf_=${_ipmp_group_intf_}
    elif [ $_intf_count_ -eq 2 ];then
        _primary_intf_=`$ECHO ${_ipmp_group_intf_} | $CUT -f1 -d" "`
        _secondary_intf_=`$ECHO ${_ipmp_group_intf_} | $CUT -f2 -d" "`
    else
        _err_msg_="Unable to fetch interfaces for ${_name_} group."
        abort_script "${_err_msg_}"
    fi

    # Check if primary interface name is available in interfaces list
    log_msg -s "Checking if primary interface name is available in interfaces list" -l ${LOGFILE}
    $CAT ${TEM_DIR}/interface_list | $GREP ${_primary_intf_} >> /dev/null
    if [ $? -ne 0 ];then
        _update_intf_=1
    else
        log_msg -s "\nInterface name is same. No need to update ipmp.ini." -l ${LOGFILE}
        return 0
    fi

    if [ $_update_intf_ -eq 1 ];then
        $RM -rf ${TEM_DIR}/intf_mapping
        log_msg -s "\nInterfaces name will be changed." -l ${LOGFILE}

        # Map interfaces with devices
        while read _intf_
        do
            _device_=`$DLADM show-phys -p -o device $_intf_`
            $ECHO "$_intf_::$_device_" >> ${TEM_DIR}/intf_mapping
        done < ${TEM_DIR}/interface_list

        # Check if dev name matches with intf name
        $CAT ${TEM_DIR}/intf_mapping | $GREP $_primary_intf_ >> /dev/null
        if [ $? -ne 0 ];then
            _err_msg_="Interfaces names cannot be updated using current device mapping."
            abort_script "$_err_msg_"
        fi

        # Interface to be updated
        _update_list_="${_primary_intf_}"
        if [ "$_secondary_intf_" ];then
            _update_list_="${_update_list_} ${_secondary_intf_}"
        fi

        # Update intf names
        for _intf_name_ in $_update_list_
        do
            _new_intf_name_=`$CAT ${TEM_DIR}/intf_mapping | $GREP $_intf_name_ | $AWK -F"::" '{print $1}'`
            log_msg -s "\nUpdating ${_intf_name_} value with ${_new_intf_name_}" -l ${LOGFILE}
            $CAT ${TEM_DIR}/${IPMP_INI} | $SED -e "s|^\(IPMP_Group_Intf=\)\(.*\)${_intf_name_}\(.*\)|\1\2${_new_intf_name_}\3|g" > ${TEM_DIR}/${IPMP_INI}.tmp
            $MV ${TEM_DIR}/${IPMP_INI}.tmp ${TEM_DIR}/${IPMP_INI}
        done
    fi
done

# Copy the updated IPMP_INI to config dir
$CP ${TEM_DIR}/${IPMP_INI} ${ENIQ_CONF_DIR}/${IPMP_INI}
if [ $? -ne 0 ];then
    _err_msg_="Could not copy ${TEM_DIR}/${IPMP_INI} to ${ENIQ_CONF_DIR}/${IPMP_INI}"
    abort_script "${_err_msg_}"
else
    log_msg -s "Successfully updated ${ENIQ_CONF_DIR}/${IPMP_INI} with available interfaces." -l ${LOGFILE}
fi
}

### Function: update_netmasks_file ###
#
# Updates /etc/inet/netmasks file
#
# Arguments:
#   none
# Return Values:
#   none
update_netmasks_file()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Mounting newly created NAS FS
mount_nas_shares

# Copying netmasks file from NAS backup
log_msg -q -s "Copying from ${_mountpoint_}/${HNAME}/ROOT/${ETC_INET_DIR}/netmasks to /etc/inet/" -l ${LOGFILE}
$CP -p ${_mountpoint_}/${HNAME}/ROOT/${ETC_INET_DIR}/netmasks ${ETC_INET_DIR} >> /dev/null 2>&1
if [ $? -ne 0 ]; then
    _err_msg_="Could not copy netmask from ${_mountpoint_}/${HNAME}/ROOT/${ETC_INET_DIR}/netmasks to ${ETC_INET_DIR}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi
insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: update_root_cron ###
#
#  Removes Solaris 10 OS specific cron entries  and appends Solaris 11 OS cron entries
#  if performing OS Migration / OS Migration with Blade Replacement 
#
# Arguments:
#   NONE 
# Return Values:
#   NONE 
update_root_cron()
{
# Root user cron file(Solaris 10 OS and application level cron entries)
_mig_root_cron_=${CRONTABS_DIR}/root

# Creating temporary copy of cron file for root user
_temp_root_cron_="${TEM_DIR}/root"
$RM -rf ${_temp_root_cron_} >> /dev/null
$CP -p ${_mig_root_cron_} ${_temp_root_cron_} >> /dev/null
if [ $? -ne 0 ];then
    _err_msg_="Unable to create temporary copy of ${_mig_root_cron_}."
    abort_script "${_err_msg_}"
fi

# Give write permission to temp file
$CHMOD +w ${_temp_root_cron_} >> /dev/null
if [ $? -ne 0 ];then
    _err_msg_="Unable to give write permission to ${_temp_root_cron_}."
    abort_script "${_err_msg_}"
fi

# Check if eniq_solaris_10_cronlist exist
if [ ! -f ${MIGRATION_ETC}/eniq_solaris_10_cronlist ];then
    _err_msg_="File ${MIGRATION_ETC}/eniq_solaris_10_cronlist does not exists."
    abort_script "${_err_msg_}"
fi
 
# Config file containing Solaris 10 OS level cron entries
old_os_cronlist=(`$CAT ${MIGRATION_ETC}/eniq_solaris_10_cronlist`)
for _entry_ in "${old_os_cronlist[@]}"; do
    # Check and remove all Solaris 10 OS level cron entries 
    $CAT ${_temp_root_cron_} | $GREP $_entry_ >> /dev/null 2>&1    
    if [ $? -eq 0 ]; then
        log_msg -s "Removing cron entry for $_entry_" -l ${LOGFILE} 
        $CAT ${_temp_root_cron_} | $GREP -v ${_entry_} > ${_temp_root_cron_}.tmp 
        $MV ${_temp_root_cron_}.tmp ${_temp_root_cron_} 
    fi
done

# Copy the temp file to original file (File will now contain only Solaris 10 application level cron entries)
$CP -p ${_temp_root_cron_} ${_mig_root_cron_}
if [ $? -ne 0 ]; then
    _err_msg_="Unable to save cron file ${_mig_root_cron_}."
    abort_script "${_err_msg_}"
fi

if [ ! -s ${_mig_root_cron_} ];then
    _err_msg_="File ${_mig_root_cron_} is empty."
    abort_script "${_err_msg_}"
fi

# Append Solaris 11 OS level cron entries to Solaris 10 application level cron entries
log_msg -s "Adding Solaris 11 OS level cron entries from file /${VAR_DIR}/tmp/os_11_root_cron to ${_mig_root_cron_}" -l ${LOGFILE} 
$CAT /${VAR_DIR}/tmp/os_11_root_cron >> ${_mig_root_cron_}
crontab ${_mig_root_cron_}
}

### Function: update_system_file ###
#
# Updates to /etc/system file
#
# Arguments:
#   none
# Return Values:
#   none
update_system_file()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling update_system_file stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s update_system_file ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: update_timezone_info ###
#
# Updates TIMEZONE information
#
# Arguments:
#   none
# Return Values:
#   none
update_timezone_info()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}

# Calling update_timezone_info stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s update_timezone_info ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
return 0
}

### Function: user_confirm ###
#
#   Take user confirmation
#
# Arguments:
#   $1 : User display message
# Return Values:
#   User response : YES/NO
user_confirm()
{
_usr_msg_="\n\nDo you want to proceed? (Yy/Nn)"
unset _response_

while [ 1 ]
do
    $ECHO ${_usr_msg_}
    read ans
    case $ans in
       Y|y) _response_="YES"
                        break
                        ;;
      N|n) _response_="NO"
                    break
                    ;;
     *) $ECHO "Invalid input. Enter again."
        ;;
    esac
done

}


### Function: usage_msg ###
#
#   Print out the usage message
#
# Arguments:
#   none
# Return Values:
#   none
usage_msg()
{
$CLEAR
$ECHO "
Usage: 

`$BASENAME $0` -a <migration action> 

Optional: [-R] [ -l <path_to_logfile> ] 

-a  : Mandatory parameter specifying the migration action type. Should be either of the following:
    premigration : To backup the necessary files.
    recovery     : To recover the system
    cleanup      : To clear leftovers of migration/recovery [Commit step]

-R  : Parameter specifying the type of migration on blade servers. If specified, 
    the migration will be treated as blade replacement.

-l  : Optional parameter specifying the full path to logfile. If not specified, a
    logfile will be created in /eniq/local_logs/migration

"
}

### Function: validate_SMF_contracts ###
#
# Validate the SMF scripts and
#
# Arguments:
#   none
# Return Values:
#   none
validate_SMF_contracts()
{
insert_header_footer head "Entering ${ACTION_TYPE} stage - ${NEXT_STAGE}" ${LOGFILE}


# Calling validate_SMF_contracts stage from eniq_core_install.bsh
$BASH ${ENIQ_CORE_INST_SCRIPT} -s validate_SMF_contracts ${ENIQ_CORE_INST_ARG}
if [ $? -ne 0 ]; then
    _err_msg_="Failed to execute ${ACTION_TYPE} stage - ${NEXT_STAGE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

insert_header_footer foot "Successfully completed - ${NEXT_STAGE}" ${LOGFILE}

set_next_stage `$EXPR ${ARRAY_ELEM}+1`
}

# ********************************************************************
#
#   Main body of program
#
# ********************************************************************

RUN_TIME=`$DATE '+%Y-%b-%d_%H:%M:%S'`

# Check that the effective id of the user is root
check_id ${DEFAULT_USER}


while getopts ":a:b:d:Il:NBnoC:s:R" arg; do
    case $arg in
    a) ACTION_TYPE="$OPTARG"
        ;;
    b) ENIQ_BASE_DIR="$OPTARG"
        ;;
    d) BASE_SW_DIR="$OPTARG"
        ;;
    I) INITIATE="YES"
        ;;
    l) LOGFILE="$OPTARG"
        ;;
    N) NO_CONFIRM="YES"
        ;;
    n) NO_RESET_STAGE="YES"
        ;;
    o) OM_SW_DIR="$OPTARG"
        ;;
    C) CONTINUE="YES"
        ;;
    R) REPLACEMENT="YES"
        ;;
    s) USER_STAGE="$OPTARG"
        ;;
    B) BACKUP="YES"
        ;;
    \?) _err_msg_="`$BASENAME $0` -s <stage>"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
        ;;
  esac
done
shift `$EXPR $OPTIND - 1`

# Check Input Params
check_params

# Check hardware
if [ -f /eniq/installation/config/san_details ];then
    STORAGE_TYPE=`$CAT /eniq/installation/config/san_details | $GREP STORAGE_TYPE | $CUT -f 2 -d =`
    if [ "${STORAGE_TYPE}" != "raw" -a "${STORAGE_TYPE}" != "fs" ];then
        _err_msg_="\nHardware type is not supported for Linux Migration."
        abort_script "${_err_msg_}"
    fi
else
    _err_msg_="Could not find the file to get hardware type."
    abort_script "${_err_msg_}"
fi

# Determine absolute path to software
check_absolute_path

# Set up environment variables for script.
setup_env

# Get new lun ids and store it in "migration.conf"
if [ "${ACTION_TYPE}" == "recovery" ]; then
    if [ ! -s ${STAGEFILE} ]; then
        get_new_lun_id
    fi
fi

# Get BASE SW and OM SW directory
if [ "${ACTION_TYPE}" != "premigration" -a "${ACTION_TYPE}" != "cleanup" ];then
    if [ ! ${BASE_SW_DIR} -o ! ${OM_SW_DIR} ];then
        BASE_SW_DIR=`read_value ${_base_sw_param_} ${MIGRATION_CONF}` || abort_script "${BASE_SW_DIR}"
        OM_SW_DIR=`read_value ${_om_sw_param_} ${MIGRATION_CONF}` || abort_script "${OM_SW_DIR}"
    fi
fi

# Log file
if [ ! "${LOGFILE}" ]; then
    $MKDIR -p ${ENIQ_LOG_DIR}/migration
    LOGFILE="${ENIQ_LOG_DIR}/migration/eniq_linux_${ACTION_TYPE}_`$DATE '+%Y-%b-%d'`.log"
fi

# ENIQ Core install arguments
ENIQ_CORE_INST_ARG="-n -M -l ${LOGFILE}"

# Create a temporary Directory
TEM_DIR=/tmp/linux_migration.$$.$$
$RM -rf ${TEM_DIR}
$MKDIR -p ${TEM_DIR}
if [ $? -ne 0 ]; then
    _err_msg_="Could not create directory ${TEM_DIR}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

if [[ ${BACKUP} ]] && [[ ${USER_STAGE} == "get_migration_data" ]];then
    if [ -f "${MIGRATION_CONF}" ];then
        $MV "${MIGRATION_CONF}" "${MIGRATION_CONF}"_bkp
    fi
fi

# Call function if cleanup is called
if [ "${ACTION_TYPE}" == "cleanup" ]; then
    # Check if any activity is ongoing
    _flag_list_=`$LS -1 ${VAR_DIR}/tmp/linux_*_progress 2>/dev/null`
    if [ "${_flag_list_}" ]; then
        _err_msg_="Migration Activity is in progress. Can not run \"cleanup\" now."
        log_msg -s "Progress files detected: " -l ${LOGFILE}
        for _flag_ in ${_flag_list_}; do
            log_msg -s "`basename ${_flag_}`" -l ${LOGFILE}
        done
        abort_script "${_err_msg_}"
    fi

    log_msg -h -l ${LOGFILE} -s "Starting ${ACTION_TYPE} activity."
    clear_data
    log_msg -h -l ${LOGFILE} -t -s "Successfully completed ${ACTIVITY}."
    $RM -rf ${TEM_DIR}

    exit 0
fi

# Check if migration action has been re-initialised
if [ "${INITIATE}" ]; then
    log_msg -h -l ${LOGFILE} -s "Starting over ${ACTION_TYPE} activity."
    $RM -rf ${MIGR_PROGRESS} ${MIGR_SUCCESS}
    start_eniq_services
fi

# Show system info in ACTIVITY and ask confirmation for the first time
if [ ! -f "${MIGR_PROGRESS}" -a ! -f "${MIGR_SUCCESS}" ];then
    log_msg -h -l ${LOGFILE} -s "Starting ${ACTION_TYPE} activity."
    $RM -rf ${STAGEFILE}

    # Creating progress flag file
    $TOUCH ${MIGR_PROGRESS}
    show_server_info
    if [ ! "${NO_CONFIRM}" ];then
        $ECHO "\nINFO: You are about to start ${ACTIVITY}."
        user_confirm
        if [ "${_response_}" != "YES" ];then
            $RM -rf ${TEM_DIR} >> /dev/null 2>&1
            $RM -rf ${MIGR_PROGRESS} >> /dev/null 2>&1
            log_msg -s "\nExiting from script as user selected NOT to proceed." -l ${LOGFILE}
            exit 0
        fi
    fi
fi

# Determine the server list on CO
if [ "${ACTION_TYPE}" == "premigration" ]; then
    if [[ ! ${BACKUP} ]] && [[ "${CO_SERVER}" ]]; then
        # Determine the server list to run premigration
        get_server_list
    fi
fi

log_msg -h -l ${LOGFILE} -t -s "Entering $ACTIVITY."

# Create a stage array
core_install_build_stage_array ${LOGFILE} ${TEM_DIR} eniq_linux ${ACTION_TYPE} ${MIGRATION_CORE}
if [ $? -ne 0 ]; then
    _err_msg_="Could not build a stage array for ${ACTION_TYPE}"
    abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
fi

# If stage specified, make sure it is in the stage list for this server type
if [ "$USER_STAGE" ]; then
    core_install_check_user_stage ${LOGFILE} ${TEM_DIR} ${USER_STAGE}
    if [ $? -ne 0 ]; then
        _err_msg_="The specified stage ${USER_STAGE} is not in the stage list for ${ACTION_TYPE}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi
fi

if [ "$USER_STAGE" ]; then
    NEXT_STAGE="${USER_STAGE}"
    # Get the element number so we can move along the array
    get_array_element
else
    get_next_stage
fi


# If we read last stage from the stagefile
if [ "$NEXT_STAGE" == "${STOP_STAGE}" ]; then
    # We exit unless the user specified that the stage be run again
    if [ ! "$USER_STAGE" ]; then
        _completion_date_=""
        _completion_date_=`$LS -l ${VAR_DIR}/tmp/linux_${ACTION_TYPE}_success | \
                           $AWK '{print " on " $6, $7, "at " $8}' 2> /dev/null`
        log_msg -s "\nAll Stages of ${ACTION_TYPE} are already completed${_completion_date_}." -l ${LOGFILE}
        exit 0
    fi
fi

# Check if stop stage is defined
if [ -s ${ENIQ_CONF_DIR}/extra_params/stop_stage ]; then
    _stop_stage_=`$CAT ${ENIQ_CONF_DIR}/extra_params/stop_stage`
    $ECHO ${ENIQ_CORE_STAGES[*]} | $GREP -w ${_stop_stage_} >> /dev/null 2>&1
    if [ $? -eq 0 ]; then
        log_msg -s "Linux Migration Procedure will stop at stage ${_stop_stage_}" -l ${LOGFILE}
    else
        log_msg -s "User defined stop stage - ${_stop_stage_} not valid.....Ignoring" -l ${LOGFILE}
        unset _stop_stage_
        $RM -f ${ENIQ_CONF_DIR}/extra_params/stop_stage
    fi
fi


# Loop through the stages from stage list 
while :; do
    _nxt_stage_="${NEXT_STAGE}"
    $_nxt_stage_
    if [ $? -ne 0 ]; then
        _err_msg_="Error in Stage ${NEXT_STAGE}"
        abort_script "${_err_msg_}" "${EXEC_SHELL_CMD}"
    fi

    # Exit if the user specified to run a specific stage only
    if [ "$USER_STAGE" ]; then
        break
    fi

    # If we read ${STOP_STAGE} from the stagefile
    if [ "$NEXT_STAGE" == "${STOP_STAGE}" ]; then
        break
    fi
    get_next_stage
done

log_msg -h -l ${LOGFILE} -t -s "Successfully completed $ACTIVITY. Logfile: ${LOGFILE}" 

$RM -rf ${TEM_DIR}
$RM -rf ${rep_connection_string_enc}
exit 0
